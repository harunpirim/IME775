{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Deep Architectures - Modern Building Blocks",
    "**IME775: Data Driven Modeling and Optimization**",
    "ðŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 8",
    "---",
    "## Learning Objectives",
    "- Understand the degradation problem in deep networks",
    "- Master residual connections",
    "- Learn dense connections and SE blocks",
    "- Understand modern efficient architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 The Degradation Problem",
    "**Paradox**: Deeper networks can perform **worse** than shallow ones, even on training data!",
    "This isn't overfitting - it's an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Residual Networks (ResNet)",
    "**Key Insight**: Instead of learning $H(x)$, learn the residual $F(x) = H(x) - x$",
    "$$H(x) = F(x) + x$$",
    "The skip connection adds $x$ directly to the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize residual block",
    "fig2, axes2 = plt.subplots(1, 2, figsize=(14, 6))",
    "# Plain network block",
    "ax1 = axes2[0]",
    "ax1.set_xlim(0, 10)",
    "ax1.set_ylim(0, 10)",
    "# Draw plain block",
    "ax1.add_patch(plt.Rectangle((3, 7), 4, 1.5, fill=True, facecolor='lightblue', edgecolor='black'))",
    "ax1.text(5, 7.75, 'Conv-BN-ReLU', ha='center', va='center', fontsize=10)",
    "ax1.add_patch(plt.Rectangle((3, 4), 4, 1.5, fill=True, facecolor='lightblue', edgecolor='black'))",
    "ax1.text(5, 4.75, 'Conv-BN-ReLU', ha='center', va='center', fontsize=10)",
    "# Arrows",
    "ax1.annotate('', xy=(5, 7), xytext=(5, 5.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))",
    "ax1.annotate('', xy=(5, 4), xytext=(5, 2.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))",
    "ax1.annotate('', xy=(5, 9.5), xytext=(5, 8.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))",
    "ax1.text(5, 9.7, 'x', ha='center', fontsize=12, fontweight='bold')",
    "ax1.text(5, 2.3, 'H(x)', ha='center', fontsize=12, fontweight='bold')",
    "ax1.set_title('Plain Block: Learn H(x) directly', fontsize=12)",
    "ax1.axis('off')",
    "# Residual block",
    "ax2 = axes2[1]",
    "ax2.set_xlim(0, 10)",
    "ax2.set_ylim(0, 10)",
    "# Draw residual block",
    "ax2.add_patch(plt.Rectangle((3, 7), 4, 1.5, fill=True, facecolor='lightgreen', edgecolor='black'))",
    "ax2.text(5, 7.75, 'Conv-BN-ReLU', ha='center', va='center', fontsize=10)",
    "ax2.add_patch(plt.Rectangle((3, 4), 4, 1.5, fill=True, facecolor='lightgreen', edgecolor='black'))",
    "ax2.text(5, 4.75, 'Conv-BN', ha='center', va='center', fontsize=10)",
    "# Skip connection",
    "ax2.plot([1.5, 1.5], [9, 2.5], 'r-', linewidth=2)",
    "ax2.annotate('', xy=(5, 2.5), xytext=(1.5, 2.5), arrowprops=dict(arrowstyle='->', color='red', lw=2))",
    "# Addition",
    "ax2.add_patch(plt.Circle((5, 2.5), 0.3, fill=True, facecolor='yellow', edgecolor='black'))",
    "ax2.text(5, 2.5, '+', ha='center', va='center', fontsize=14, fontweight='bold')",
    "# Arrows",
    "ax2.annotate('', xy=(5, 7), xytext=(5, 5.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))",
    "ax2.annotate('', xy=(5, 4), xytext=(5, 2.8), arrowprops=dict(arrowstyle='->', color='black', lw=2))",
    "ax2.annotate('', xy=(5, 9.5), xytext=(5, 8.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))",
    "ax2.annotate('', xy=(5, 2.2), xytext=(5, 1), arrowprops=dict(arrowstyle='->', color='black', lw=2))",
    "ax2.plot([1.5, 5], [9, 9], 'black', linewidth=1)",
    "ax2.plot([1.5, 1.5], [9, 9], 'black', linewidth=1)",
    "ax2.text(5, 9.7, 'x', ha='center', fontsize=12, fontweight='bold')",
    "ax2.text(5, 0.7, 'H(x) = F(x) + x', ha='center', fontsize=12, fontweight='bold')",
    "ax2.text(0.8, 5.5, 'Skip\\nConnection', ha='center', fontsize=10, color='red')",
    "ax2.set_title('Residual Block: Learn F(x) = H(x) - x', fontsize=12)",
    "ax2.axis('off')",
    "plt.tight_layout()",
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Why Residual Connections Work",
    "**Gradient Flow Analysis**:",
    "Without skip: $\\frac{\\partial L}{\\partial x_l} = \\prod_{i=l}^{L-1} \\frac{\\partial x_{i+1}}{\\partial x_i}$ â†’ Can vanish!",
    "With skip: $\\frac{\\partial L}{\\partial x_l} = \\frac{\\partial L}{\\partial x_L}(1 + \\text{other terms})$ â†’ Always has the \"1\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 DenseNet: Dense Connections",
    "**Key Idea**: Connect every layer to every subsequent layer",
    "$$x_l = H_l([x_0, x_1, \\ldots, x_{l-1}])$$",
    "Concatenate all previous features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize connectivity patterns",
    "fig4, axes4 = plt.subplots(1, 3, figsize=(15, 4))",
    "    n_layers = len(connections)",
    "    # Draw layers",
    "    for i in range(n_layers):",
    "        ax.add_patch(plt.Circle((i, 0), 0.15, fill=True, color='lightblue', edgecolor='black'))",
    "        ax.text(i, 0, str(i), ha='center', va='center', fontsize=10)",
    "    # Draw connections",
    "    for i, conns in enumerate(connections):",
    "        for j in conns:",
    "            if j < i:",
    "                # Draw curved arrow",
    "                y_offset = 0.3 + 0.15 * (i - j)",
    "                ax.annotate('', xy=(i-0.15, 0.1), xytext=(j+0.15, 0.1),",
    "                           arrowprops=dict(arrowstyle='->', color='red', ",
    "                                         connectionstyle=f'arc3,rad={0.3}', lw=1))",
    "    ax.set_xlim(-0.5, n_layers - 0.5)",
    "    ax.set_ylim(-0.5, 1.5)",
    "    ax.set_aspect('equal')",
    "    ax.set_title(title, fontsize=11)",
    "    ax.axis('off')",
    "# Plain network",
    "plain = [[], [0], [1], [2], [3]]",
    "draw_connectivity(axes4[0], plain, 'Plain Network\\n(Sequential)')",
    "# ResNet",
    "resnet = [[], [0], [0, 1], [1, 2], [2, 3]]  ",
    "axes4[1].text(0.5, 1.3, 'ResNet: Skip every 2 layers', ha='center', transform=axes4[1].transAxes)",
    "# Draw ResNet manually",
    "for i in range(5):",
    "    axes4[1].add_patch(plt.Circle((i, 0), 0.15, fill=True, color='lightgreen', edgecolor='black'))",
    "    axes4[1].text(i, 0, str(i), ha='center', va='center', fontsize=10)",
    "# Sequential connections",
    "for i in range(4):",
    "    axes4[1].annotate('', xy=(i+0.85, 0), xytext=(i+0.15, 0),",
    "                     arrowprops=dict(arrowstyle='->', color='black', lw=1))",
    "# Skip connections (every 2)",
    "for i in range(0, 4, 2):",
    "    axes4[1].annotate('', xy=(i+1.85, 0.1), xytext=(i+0.15, 0.1),",
    "                     arrowprops=dict(arrowstyle='->', color='red', ",
    "                                   connectionstyle='arc3,rad=0.5', lw=1.5))",
    "axes4[1].set_xlim(-0.5, 4.5)",
    "axes4[1].set_ylim(-0.5, 1.5)",
    "axes4[1].set_title('ResNet\\n(Skip connections)')",
    "axes4[1].axis('off')",
    "# DenseNet",
    "axes4[2].text(0.5, 1.3, 'DenseNet: All-to-all connections', ha='center', transform=axes4[2].transAxes)",
    "for i in range(5):",
    "    axes4[2].add_patch(plt.Circle((i, 0), 0.15, fill=True, color='lightyellow', edgecolor='black'))",
    "    axes4[2].text(i, 0, str(i), ha='center', va='center', fontsize=10)",
    "# All connections",
    "for i in range(5):",
    "    for j in range(i):",
    "        y_rad = 0.3 + 0.1 * (i - j)",
    "        axes4[2].annotate('', xy=(i-0.15, 0.1), xytext=(j+0.15, 0.1),",
    "                         arrowprops=dict(arrowstyle='->', color='orange', ",
    "                                       connectionstyle=f'arc3,rad={y_rad}', lw=0.8, alpha=0.7))",
    "axes4[2].set_xlim(-0.5, 4.5)",
    "axes4[2].set_ylim(-0.5, 1.5)",
    "axes4[2].set_title('DenseNet\\n(Dense connections)')",
    "axes4[2].axis('off')",
    "plt.tight_layout()",
    "fig4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Squeeze-and-Excitation (SE) Blocks",
    "**Channel Attention**: Not all channels are equally important.",
    "1. **Squeeze**: Global average pooling",
    "2. **Excitation**: Learn channel weights",
    "3. **Scale**: Reweight channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Efficient Architectures",
    "**Depthwise Separable Convolution** (MobileNet):",
    "- Standard: $K^2 \\times C_{in} \\times C_{out}$ parameters",
    "- Separable: $K^2 \\times C_{in} + C_{in} \\times C_{out}$ parameters",
    "- Reduction: ~$\\frac{1}{K^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary",
    "| Architecture | Key Innovation | Use Case |",
    "|--------------|----------------|----------|",
    "| **ResNet** | Skip connections | Very deep networks |",
    "| **DenseNet** | Dense connections | Feature reuse |",
    "| **SE-Net** | Channel attention | Any backbone |",
    "| **MobileNet** | Depthwise separable | Mobile/edge devices |",
    "| **EfficientNet** | Compound scaling | Best accuracy/compute |",
    "---",
    "## References",
    "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 8.",
    "- **ResNet**: He et al. (2016). \"Deep Residual Learning.\"",
    "- **DenseNet**: Huang et al. (2017). \"Densely Connected Networks.\"",
    "- **SE-Net**: Hu et al. (2018). \"Squeeze-and-Excitation Networks.\"",
    "## Connection to ML Refined Curriculum",
    "Modern architectures extend concepts from:",
    "- Week 7: Multi-class classification",
    "- Week 8: Feature learning (PCA â†’ learned features)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}