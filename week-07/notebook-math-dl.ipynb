{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-07/notebook-math-dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 7: Deep Architectures - Modern Building Blocks\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 8\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand the degradation problem in deep networks\n",
        "- Master residual connections\n",
        "- Learn dense connections and SE blocks\n",
        "- Understand modern efficient architectures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## 7.1 The Degradation Problem\n",
        "**Paradox**: Deeper networks can perform **worse** than shallow ones, even on training data!\n",
        "This isn't overfitting - it's an optimization problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Simulate degradation problem\n",
        "np.random.seed(42)\n",
        "# Simulated training curves\n",
        "epochs = 100\n",
        "# Shallow network (20 layers) - good convergence\n",
        "shallow_train = 0.5 * np.exp(-epochs * np.linspace(0, 1, epochs) * 3) + 0.04 + np.random.randn(epochs) * 0.005\n",
        "shallow_val = 0.5 * np.exp(-epochs * np.linspace(0, 1, epochs) * 2.5) + 0.06 + np.random.randn(epochs) * 0.008\n",
        "# Deep network (56 layers) without skip connections - degradation\n",
        "deep_train = 0.5 * np.exp(-epochs * np.linspace(0, 1, epochs) * 1.5) + 0.08 + np.random.randn(epochs) * 0.005\n",
        "deep_val = 0.5 * np.exp(-epochs * np.linspace(0, 1, epochs) * 1.2) + 0.12 + np.random.randn(epochs) * 0.01\n",
        "fig1, axes1 = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Training error\n",
        "axes1[0].plot(shallow_train, 'b-', linewidth=2, label='20-layer network')\n",
        "axes1[0].plot(deep_train, 'r-', linewidth=2, label='56-layer network')\n",
        "axes1[0].set_xlabel('Epoch')\n",
        "axes1[0].set_ylabel('Training Error')\n",
        "axes1[0].set_title('Training Error: Deeper â‰  Better!')\n",
        "axes1[0].legend()\n",
        "axes1[0].grid(True, alpha=0.3)\n",
        "# Validation error\n",
        "axes1[1].plot(shallow_val, 'b-', linewidth=2, label='20-layer network')\n",
        "axes1[1].plot(deep_val, 'r-', linewidth=2, label='56-layer network')\n",
        "axes1[1].set_xlabel('Epoch')\n",
        "axes1[1].set_ylabel('Validation Error')\n",
        "axes1[1].set_title('Validation Error: Degradation Problem')\n",
        "axes1[1].legend()\n",
        "axes1[1].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_4"
      },
      "source": [
        "## 7.2 Residual Networks (ResNet)\n",
        "**Key Insight**: Instead of learning $H(x)$, learn the residual $F(x) = H(x) - x$\n",
        "$$H(x) = F(x) + x$$\n",
        "The skip connection adds $x$ directly to the output!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Visualize residual block\n",
        "fig2, axes2 = plt.subplots(1, 2, figsize=(14, 6))\n",
        "# Plain network block\n",
        "ax1 = axes2[0]\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "# Draw plain block\n",
        "ax1.add_patch(plt.Rectangle((3, 7), 4, 1.5, fill=True, facecolor='lightblue', edgecolor='black'))\n",
        "ax1.text(5, 7.75, 'Conv-BN-ReLU', ha='center', va='center', fontsize=10)\n",
        "ax1.add_patch(plt.Rectangle((3, 4), 4, 1.5, fill=True, facecolor='lightblue', edgecolor='black'))\n",
        "ax1.text(5, 4.75, 'Conv-BN-ReLU', ha='center', va='center', fontsize=10)\n",
        "# Arrows\n",
        "ax1.annotate('', xy=(5, 7), xytext=(5, 5.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
        "ax1.annotate('', xy=(5, 4), xytext=(5, 2.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
        "ax1.annotate('', xy=(5, 9.5), xytext=(5, 8.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
        "ax1.text(5, 9.7, 'x', ha='center', fontsize=12, fontweight='bold')\n",
        "ax1.text(5, 2.3, 'H(x)', ha='center', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Plain Block: Learn H(x) directly', fontsize=12)\n",
        "ax1.axis('off')\n",
        "# Residual block\n",
        "ax2 = axes2[1]\n",
        "ax2.set_xlim(0, 10)\n",
        "ax2.set_ylim(0, 10)\n",
        "# Draw residual block\n",
        "ax2.add_patch(plt.Rectangle((3, 7), 4, 1.5, fill=True, facecolor='lightgreen', edgecolor='black'))\n",
        "ax2.text(5, 7.75, 'Conv-BN-ReLU', ha='center', va='center', fontsize=10)\n",
        "ax2.add_patch(plt.Rectangle((3, 4), 4, 1.5, fill=True, facecolor='lightgreen', edgecolor='black'))\n",
        "ax2.text(5, 4.75, 'Conv-BN', ha='center', va='center', fontsize=10)\n",
        "# Skip connection\n",
        "ax2.plot([1.5, 1.5], [9, 2.5], 'r-', linewidth=2)\n",
        "ax2.annotate('', xy=(5, 2.5), xytext=(1.5, 2.5), arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "# Addition\n",
        "ax2.add_patch(plt.Circle((5, 2.5), 0.3, fill=True, facecolor='yellow', edgecolor='black'))\n",
        "ax2.text(5, 2.5, '+', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "# Arrows\n",
        "ax2.annotate('', xy=(5, 7), xytext=(5, 5.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
        "ax2.annotate('', xy=(5, 4), xytext=(5, 2.8), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
        "ax2.annotate('', xy=(5, 9.5), xytext=(5, 8.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
        "ax2.annotate('', xy=(5, 2.2), xytext=(5, 1), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
        "ax2.plot([1.5, 5], [9, 9], 'black', linewidth=1)\n",
        "ax2.plot([1.5, 1.5], [9, 9], 'black', linewidth=1)\n",
        "ax2.text(5, 9.7, 'x', ha='center', fontsize=12, fontweight='bold')\n",
        "ax2.text(5, 0.7, 'H(x) = F(x) + x', ha='center', fontsize=12, fontweight='bold')\n",
        "ax2.text(0.8, 5.5, 'Skip\\nConnection', ha='center', fontsize=10, color='red')\n",
        "ax2.set_title('Residual Block: Learn F(x) = H(x) - x', fontsize=12)\n",
        "ax2.axis('off')\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_6"
      },
      "source": [
        "## 7.3 Why Residual Connections Work\n",
        "**Gradient Flow Analysis**:\n",
        "Without skip: $\\frac{\\partial L}{\\partial x_l} = \\prod_{i=l}^{L-1} \\frac{\\partial x_{i+1}}{\\partial x_i}$ â†’ Can vanish!\n",
        "With skip: $\\frac{\\partial L}{\\partial x_l} = \\frac{\\partial L}{\\partial x_L}(1 + \\text{other terms})$ â†’ Always has the \"1\"!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Demonstrate gradient flow\n",
        "def simulate_gradient_flow_resnet(n_layers, use_skip=True):\n",
        "    np.random.seed(42)\n",
        "    gradients = [1.0]\n",
        "    for _ in range(n_layers):\n",
        "        # Residual function gradient (can be small)\n",
        "        f_grad = np.random.uniform(0.1, 0.5)\n",
        "        if use_skip:\n",
        "            # With skip: gradient = 1 + f_grad\n",
        "            total_grad = gradients[-1] * (1.0 + f_grad * 0.1)\n",
        "        else:\n",
        "            # Without skip: gradient = f_grad only\n",
        "            total_grad = gradients[-1] * f_grad\n",
        "        gradients.append(total_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_8"
      },
      "source": [
        "## 7.4 DenseNet: Dense Connections\n",
        "**Key Idea**: Connect every layer to every subsequent layer\n",
        "$$x_l = H_l([x_0, x_1, \\ldots, x_{l-1}])$$\n",
        "Concatenate all previous features!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Visualize connectivity patterns\n",
        "fig4, axes4 = plt.subplots(1, 3, figsize=(15, 4))\n",
        "def draw_connectivity(ax, connections, title):\n",
        "    n_layers = len(connections)\n",
        "    # Draw layers\n",
        "    for i in range(n_layers):\n",
        "        ax.add_patch(plt.Circle((i, 0), 0.15, fill=True, color='lightblue', edgecolor='black'))\n",
        "        ax.text(i, 0, str(i), ha='center', va='center', fontsize=10)\n",
        "    # Draw connections\n",
        "    for i, conns in enumerate(connections):\n",
        "        for j in conns:\n",
        "            if j < i:\n",
        "                # Draw curved arrow\n",
        "                y_offset = 0.3 + 0.15 * (i - j)\n",
        "                ax.annotate('', xy=(i-0.15, 0.1), xytext=(j+0.15, 0.1),\n",
        "                           arrowprops=dict(arrowstyle='->', color='red', \n",
        "                                         connectionstyle=f'arc3,rad={0.3}', lw=1))\n",
        "    ax.set_xlim(-0.5, n_layers - 0.5)\n",
        "    ax.set_ylim(-0.5, 1.5)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.axis('off')\n",
        "# Plain network\n",
        "plain = [[], [0], [1], [2], [3]]\n",
        "draw_connectivity(axes4[0], plain, 'Plain Network\\n(Sequential)')\n",
        "# ResNet\n",
        "resnet = [[], [0], [0, 1], [1, 2], [2, 3]]  \n",
        "axes4[1].text(0.5, 1.3, 'ResNet: Skip every 2 layers', ha='center', transform=axes4[1].transAxes)\n",
        "# Draw ResNet manually\n",
        "for i in range(5):\n",
        "    axes4[1].add_patch(plt.Circle((i, 0), 0.15, fill=True, color='lightgreen', edgecolor='black'))\n",
        "    axes4[1].text(i, 0, str(i), ha='center', va='center', fontsize=10)\n",
        "# Sequential connections\n",
        "for i in range(4):\n",
        "    axes4[1].annotate('', xy=(i+0.85, 0), xytext=(i+0.15, 0),\n",
        "                     arrowprops=dict(arrowstyle='->', color='black', lw=1))\n",
        "# Skip connections (every 2)\n",
        "for i in range(0, 4, 2):\n",
        "    axes4[1].annotate('', xy=(i+1.85, 0.1), xytext=(i+0.15, 0.1),\n",
        "                     arrowprops=dict(arrowstyle='->', color='red', \n",
        "                                   connectionstyle='arc3,rad=0.5', lw=1.5))\n",
        "axes4[1].set_xlim(-0.5, 4.5)\n",
        "axes4[1].set_ylim(-0.5, 1.5)\n",
        "axes4[1].set_title('ResNet\\n(Skip connections)')\n",
        "axes4[1].axis('off')\n",
        "# DenseNet\n",
        "axes4[2].text(0.5, 1.3, 'DenseNet: All-to-all connections', ha='center', transform=axes4[2].transAxes)\n",
        "for i in range(5):\n",
        "    axes4[2].add_patch(plt.Circle((i, 0), 0.15, fill=True, color='lightyellow', edgecolor='black'))\n",
        "    axes4[2].text(i, 0, str(i), ha='center', va='center', fontsize=10)\n",
        "# All connections\n",
        "for i in range(5):\n",
        "    for j in range(i):\n",
        "        y_rad = 0.3 + 0.1 * (i - j)\n",
        "        axes4[2].annotate('', xy=(i-0.15, 0.1), xytext=(j+0.15, 0.1),\n",
        "                         arrowprops=dict(arrowstyle='->', color='orange', \n",
        "                                       connectionstyle=f'arc3,rad={y_rad}', lw=0.8, alpha=0.7))\n",
        "axes4[2].set_xlim(-0.5, 4.5)\n",
        "axes4[2].set_ylim(-0.5, 1.5)\n",
        "axes4[2].set_title('DenseNet\\n(Dense connections)')\n",
        "axes4[2].axis('off')\n",
        "plt.tight_layout()\n",
        "fig4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_10"
      },
      "source": [
        "## 7.5 Squeeze-and-Excitation (SE) Blocks\n",
        "**Channel Attention**: Not all channels are equally important.\n",
        "1. **Squeeze**: Global average pooling\n",
        "2. **Excitation**: Learn channel weights\n",
        "3. **Scale**: Reweight channels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_11"
      },
      "outputs": [],
      "source": [
        "# Visualize SE block operation\n",
        "np.random.seed(42)\n",
        "# Simulate feature map\n",
        "H, W, C = 4, 4, 8\n",
        "feature_map = np.random.randn(H, W, C)\n",
        "# Squeeze: Global Average Pooling\n",
        "squeezed = feature_map.mean(axis=(0, 1))  # Shape: (C,)\n",
        "# Excitation: Simple simulation (normally learned)\n",
        "# Two FC layers with reduction ratio r=4\n",
        "r = 4\n",
        "W1_se = np.random.randn(C, C // r) * 0.5\n",
        "W2_se = np.random.randn(C // r, C) * 0.5\n",
        "excitation = np.tanh(squeezed @ W1_se)\n",
        "channel_weights = 1 / (1 + np.exp(-excitation @ W2_se))  # Sigmoid\n",
        "# Scale\n",
        "scaled_feature_map = feature_map * channel_weights\n",
        "# Visualize\n",
        "fig5, axes5 = plt.subplots(1, 4, figsize=(16, 3))\n",
        "# Original channel importance (average activation)\n",
        "orig_importance = np.abs(feature_map).mean(axis=(0, 1))\n",
        "axes5[0].bar(range(C), orig_importance, color='blue', alpha=0.7)\n",
        "axes5[0].set_xlabel('Channel')\n",
        "axes5[0].set_ylabel('Avg Activation')\n",
        "axes5[0].set_title('Original Channel Activations')\n",
        "# Squeezed representation\n",
        "axes5[1].bar(range(C), squeezed, color='green', alpha=0.7)\n",
        "axes5[1].set_xlabel('Channel')\n",
        "axes5[1].set_ylabel('Value')\n",
        "axes5[1].set_title('After Squeeze (GAP)')\n",
        "# Learned weights\n",
        "axes5[2].bar(range(C), channel_weights, color='orange', alpha=0.7)\n",
        "axes5[2].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "axes5[2].set_xlabel('Channel')\n",
        "axes5[2].set_ylabel('Weight')\n",
        "axes5[2].set_title('Excitation Weights (Learned)')\n",
        "# Scaled importance\n",
        "scaled_importance = np.abs(scaled_feature_map).mean(axis=(0, 1))\n",
        "axes5[3].bar(range(C), scaled_importance, color='red', alpha=0.7)\n",
        "axes5[3].set_xlabel('Channel')\n",
        "axes5[3].set_ylabel('Avg Activation')\n",
        "axes5[3].set_title('After SE (Reweighted)')\n",
        "plt.tight_layout()\n",
        "fig5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_12"
      },
      "source": [
        "## 7.6 Efficient Architectures\n",
        "**Depthwise Separable Convolution** (MobileNet):\n",
        "- Standard: $K^2 \\times C_{in} \\times C_{out}$ parameters\n",
        "- Separable: $K^2 \\times C_{in} + C_{in} \\times C_{out}$ parameters\n",
        "- Reduction: ~$\\frac{1}{K^2}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_13"
      },
      "outputs": [],
      "source": [
        "# Compare parameter counts\n",
        "def conv_params(k, c_in, c_out):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_14"
      },
      "source": [
        "## Summary\n",
        "| Architecture | Key Innovation | Use Case |\n",
        "|--------------|----------------|----------|\n",
        "| **ResNet** | Skip connections | Very deep networks |\n",
        "| **DenseNet** | Dense connections | Feature reuse |\n",
        "| **SE-Net** | Channel attention | Any backbone |\n",
        "| **MobileNet** | Depthwise separable | Mobile/edge devices |\n",
        "| **EfficientNet** | Compound scaling | Best accuracy/compute |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 8.\n",
        "- **ResNet**: He et al. (2016). \"Deep Residual Learning.\"\n",
        "- **DenseNet**: Huang et al. (2017). \"Densely Connected Networks.\"\n",
        "- **SE-Net**: Hu et al. (2018). \"Squeeze-and-Excitation Networks.\"\n",
        "## Connection to ML Refined Curriculum\n",
        "Modern architectures extend concepts from:\n",
        "- Week 7: Multi-class classification\n",
        "- Week 8: Feature learning (PCA â†’ learned features)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook-math-dl",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}