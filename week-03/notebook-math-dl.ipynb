{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 3: Gradient-Based Optimization for Deep Learning\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 4\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand gradient descent and its variants\n",
        "- Master momentum-based acceleration\n",
        "- Learn adaptive learning rate methods (Adam)\n",
        "- Connect optimization to neural network training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## 3.1 Gradient Descent Visualization\n",
        "Basic update: $\\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t)$\n",
        "The gradient points toward steepest ascent, so we go the opposite way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Visualize gradient descent on a 2D function\n",
        "def rosenbrock(x, y, a=1, b=100):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_4"
      },
      "source": [
        "## 3.2 Momentum: Accelerating Convergence\n",
        "Momentum accumulates velocity in consistent gradient directions:\n",
        "$$v_{t+1} = \\beta v_t + \\nabla L(\\theta_t)$$\n",
        "$$\\theta_{t+1} = \\theta_t - \\alpha v_{t+1}$$\n",
        "Like a ball rolling down a hill - accelerates in consistent directions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "def gd_momentum(grad_f, x0, lr=0.001, momentum=0.9, n_iters=100):\n",
        "    path = [x0.copy()]\n",
        "    x = x0.copy()\n",
        "    v = np.zeros_like(x)\n",
        "    for _ in range(n_iters):\n",
        "        g = grad_f(x[0], x[1])\n",
        "        v = momentum * v + g\n",
        "        x = x - lr * v\n",
        "        path.append(x.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_6"
      },
      "source": [
        "## 3.3 Adam: Adaptive Moment Estimation\n",
        "Combines momentum with adaptive learning rates:\n",
        "**First moment** (momentum): $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n",
        "**Second moment** (RMSprop): $v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n",
        "**Update**: $\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "class AdamOptimizer:\n",
        "    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "    def step(self, x, grad):\n",
        "        if self.m is None:\n",
        "            self.m = np.zeros_like(x)\n",
        "            self.v = np.zeros_like(x)\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad**2\n",
        "        m_hat = self.m / (1 - self.beta1**self.t)\n",
        "        v_hat = self.v / (1 - self.beta2**self.t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_8"
      },
      "outputs": [],
      "source": [
        "# Loss curves comparison\n",
        "fig4, ax4 = plt.subplots(figsize=(10, 5))\n",
        "for name, path in paths_compare.items():\n",
        "    losses = [rosenbrock(p[0], p[1]) for p in path]\n",
        "    ax4.semilogy(losses, label=name, linewidth=2)\n",
        "ax4.set_xlabel('Iteration')\n",
        "ax4.set_ylabel('Loss (log scale)')\n",
        "ax4.set_title('Convergence Comparison on Rosenbrock Function')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "fig4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_9"
      },
      "source": [
        "## 3.4 Learning Rate Schedules\n",
        "| Schedule | Formula | Use Case |\n",
        "|----------|---------|----------|\n",
        "| Step Decay | $\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}$ | Classic CNNs |\n",
        "| Exponential | $\\alpha_t = \\alpha_0 \\cdot e^{-\\lambda t}$ | Smooth decay |\n",
        "| Cosine | $\\alpha_t = \\alpha_{min} + \\frac{1}{2}(\\alpha_{max} - \\alpha_{min})(1 + \\cos(\\frac{t}{T}\\pi))$ | Transformers |\n",
        "| Warmup | Linear increase then decay | Large batch training |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_10"
      },
      "outputs": [],
      "source": [
        "# Visualize learning rate schedules\n",
        "fig5, axes5 = plt.subplots(2, 2, figsize=(14, 8))\n",
        "epochs = np.arange(100)\n",
        "alpha_0 = 0.1\n",
        "# Step decay\n",
        "step_lr = alpha_0 * (0.1 ** (epochs // 30))\n",
        "axes5[0, 0].plot(epochs, step_lr, 'b-', linewidth=2)\n",
        "axes5[0, 0].set_title('Step Decay (Î³=0.1 every 30 epochs)')\n",
        "axes5[0, 0].set_xlabel('Epoch')\n",
        "axes5[0, 0].set_ylabel('Learning Rate')\n",
        "axes5[0, 0].grid(True, alpha=0.3)\n",
        "# Exponential decay\n",
        "exp_lr = alpha_0 * np.exp(-0.03 * epochs)\n",
        "axes5[0, 1].plot(epochs, exp_lr, 'g-', linewidth=2)\n",
        "axes5[0, 1].set_title('Exponential Decay (Î»=0.03)')\n",
        "axes5[0, 1].set_xlabel('Epoch')\n",
        "axes5[0, 1].set_ylabel('Learning Rate')\n",
        "axes5[0, 1].grid(True, alpha=0.3)\n",
        "# Cosine annealing\n",
        "alpha_min = 0.001\n",
        "cosine_lr = alpha_min + 0.5 * (alpha_0 - alpha_min) * (1 + np.cos(np.pi * epochs / 100))\n",
        "axes5[1, 0].plot(epochs, cosine_lr, 'r-', linewidth=2)\n",
        "axes5[1, 0].set_title('Cosine Annealing')\n",
        "axes5[1, 0].set_xlabel('Epoch')\n",
        "axes5[1, 0].set_ylabel('Learning Rate')\n",
        "axes5[1, 0].grid(True, alpha=0.3)\n",
        "# Warmup + Cosine\n",
        "warmup_epochs = 10\n",
        "warmup_lr = np.where(epochs < warmup_epochs,\n",
        "                     alpha_0 * epochs / warmup_epochs,\n",
        "                     alpha_min + 0.5 * (alpha_0 - alpha_min) * (1 + np.cos(np.pi * (epochs - warmup_epochs) / (100 - warmup_epochs))))\n",
        "axes5[1, 1].plot(epochs, warmup_lr, 'm-', linewidth=2)\n",
        "axes5[1, 1].axvline(warmup_epochs, color='gray', linestyle='--', alpha=0.5, label='End warmup')\n",
        "axes5[1, 1].set_title('Warmup + Cosine Annealing')\n",
        "axes5[1, 1].set_xlabel('Epoch')\n",
        "axes5[1, 1].set_ylabel('Learning Rate')\n",
        "axes5[1, 1].legend()\n",
        "axes5[1, 1].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "## 3.5 Weight Initialization\n",
        "| Method | Formula | Best For |\n",
        "|--------|---------|----------|\n",
        "| Xavier | $W \\sim \\mathcal{U}(-\\sqrt{6/(n_{in}+n_{out})}, \\sqrt{6/(n_{in}+n_{out})})$ | Sigmoid, Tanh |\n",
        "| He | $W \\sim \\mathcal{N}(0, \\sqrt{2/n_{in}})$ | ReLU |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_12"
      },
      "outputs": [],
      "source": [
        "# Demonstrate importance of initialization\n",
        "def forward_pass(W_list, x, activation='relu'):\n",
        "    activations = [x]\n",
        "    for W in W_list:\n",
        "        x = x @ W\n",
        "        if activation == 'relu':\n",
        "            x = np.maximum(0, x)\n",
        "        elif activation == 'tanh':\n",
        "            x = np.tanh(x)\n",
        "        activations.append(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_13"
      },
      "source": [
        "## Summary\n",
        "| Optimizer | Key Feature | When to Use |\n",
        "|-----------|-------------|-------------|\n",
        "| **SGD** | Simple, generalizes well | Final training |\n",
        "| **Momentum** | Accelerates convergence | Standard choice |\n",
        "| **Adam** | Adaptive + momentum | Quick prototyping |\n",
        "| Schedule | Key Feature | When to Use |\n",
        "|----------|-------------|-------------|\n",
        "| **Step Decay** | Simple, predictable | CNNs |\n",
        "| **Cosine** | Smooth, no hyperparameters | Modern networks |\n",
        "| **Warmup** | Stabilizes early training | Large models |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 4.\n",
        "- **Supplementary**: Ruder, S. \"An overview of gradient descent optimization algorithms.\"\n",
        "## Connection to ML Refined Curriculum\n",
        "These optimization techniques are used throughout:\n",
        "- Weeks 2-3: Foundation for all optimization\n",
        "- Weeks 4-13: Training any supervised learning model\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}