{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 3: First-Order Optimization - Gradient Descent\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 3**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand the first-order optimality condition\n",
        "- Derive and implement gradient descent\n",
        "- Explore learning rate selection\n",
        "- Identify weaknesses of gradient descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## The First-Order Optimality Condition (Section 3.2)\n",
        "For a differentiable function $g(w)$, a necessary condition for $w^*$ to be a local minimum:\n",
        "$$\\nabla g(w^*) = 0$$\n",
        "### Intuition\n",
        "- Gradient $\\nabla g(w)$ points in direction of steepest **ascent**\n",
        "- At a minimum, there's no direction of descent\n",
        "- Hence gradient must be zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Visualize gradient and optimality\n",
        "x = np.linspace(-3, 3, 200)\n",
        "g = x**2 + 1\n",
        "grad_g = 2*x\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Function\n",
        "ax1 = axes[0]\n",
        "ax1.plot(x, g, 'b-', linewidth=2)\n",
        "ax1.plot(0, 1, 'r*', markersize=15, label='Minimum at w=0')\n",
        "ax1.set_xlabel('w', fontsize=12)\n",
        "ax1.set_ylabel('g(w)', fontsize=12)\n",
        "ax1.set_title('Function g(w) = wÂ² + 1')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "# Gradient\n",
        "ax2 = axes[1]\n",
        "ax2.plot(x, grad_g, 'g-', linewidth=2)\n",
        "ax2.axhline(0, color='gray', linestyle='--')\n",
        "ax2.plot(0, 0, 'r*', markersize=15, label='âˆ‡g(0) = 0')\n",
        "ax2.set_xlabel('w', fontsize=12)\n",
        "ax2.set_ylabel('âˆ‡g(w)', fontsize=12)\n",
        "ax2.set_title('Gradient âˆ‡g(w) = 2w')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_4"
      },
      "source": [
        "## The Geometry of First-Order Taylor Series (Section 3.3)\n",
        "Near a point $w$, the function can be approximated:\n",
        "$$g(w + d) \\approx g(w) + \\nabla g(w)^T d$$\n",
        "### Key Insight\n",
        "The direction of **steepest descent** is:\n",
        "$$d = -\\nabla g(w)$$\n",
        "Because $\\nabla g(w)^T (-\\nabla g(w)) = -\\|\\nabla g(w)\\|^2 < 0$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_5"
      },
      "source": [
        "mo.md(r\"\"\"\n",
        "## Gradient Descent (Section 3.5)\n",
        "### The Algorithm\n",
        "$$w^{(k+1)} = w^{(k)} - \\alpha \\nabla g(w^{(k)})$$\n",
        "Where $\\alpha > 0$ is the **learning rate** (step size).\n",
        "### Pseudocode\n",
        "```python\n",
        "def gradient_descent(g, grad_g, w0, alpha, max_iter):\n",
        "    w = w0\n",
        "    for k in range(max_iter):\n",
        "        w = w - alpha * grad_g(w)\n",
        "        if converged(w):\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_6"
      },
      "outputs": [],
      "source": [
        "# Gradient descent visualization\n",
        "def f(w):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_7"
      },
      "source": [
        "## Learning Rate Selection\n",
        "The learning rate $\\alpha$ is crucial:\n",
        "| $\\alpha$ | Effect |\n",
        "|----------|--------|\n",
        "| Too small | Very slow convergence |\n",
        "| Just right | Smooth, efficient convergence |\n",
        "| Too large | Oscillation |\n",
        "| Very large | Divergence |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_8"
      },
      "outputs": [],
      "source": [
        "alpha_slider = mo.ui.slider(0.01, 0.5, value=0.1, step=0.01, label=\"Learning Rate Î±\")\n",
        "alpha_slider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Interactive learning rate demo\n",
        "def f_demo(w):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_10"
      },
      "source": [
        "## Two Natural Weaknesses of Gradient Descent (Section 3.6)\n",
        "### Weakness 1: Zigzagging\n",
        "For ill-conditioned functions (very different curvature in different directions),\n",
        "gradient descent zigzags inefficiently.\n",
        "### Weakness 2: Saddle Points\n",
        "In high dimensions, saddle points are common. Gradient is zero but it's not a minimum!\n",
        "### Solutions (Covered in Appendix A)\n",
        "- Momentum\n",
        "- Adaptive learning rates (Adam, RMSprop)\n",
        "- Second-order methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_11"
      },
      "outputs": [],
      "source": [
        "# Saddle point visualization\n",
        "x_range = np.linspace(-2, 2, 100)\n",
        "y_range = np.linspace(-2, 2, 100)\n",
        "X, Y = np.meshgrid(x_range, y_range)\n",
        "Z = X**2 - Y**2  # Saddle function\n",
        "fig4 = plt.figure(figsize=(12, 5))\n",
        "# 3D surface\n",
        "ax1 = fig4.add_subplot(121, projection='3d')\n",
        "ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
        "ax1.plot([0], [0], [0], 'r*', markersize=15)\n",
        "ax1.set_xlabel('$w_1$')\n",
        "ax1.set_ylabel('$w_2$')\n",
        "ax1.set_zlabel('g(w)')\n",
        "ax1.set_title('Saddle Point at (0, 0)')\n",
        "# Contour\n",
        "ax2 = fig4.add_subplot(122)\n",
        "ax2.contour(X, Y, Z, levels=20, cmap='viridis')\n",
        "ax2.plot(0, 0, 'r*', markersize=15, label='Saddle point: âˆ‡g = 0')\n",
        "ax2.set_xlabel('$w_1$')\n",
        "ax2.set_ylabel('$w_2$')\n",
        "ax2.set_title('g(w) = $w_1^2$ - $w_2^2$')\n",
        "ax2.legend()\n",
        "ax2.set_aspect('equal')\n",
        "plt.tight_layout()\n",
        "fig4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_12"
      },
      "source": [
        "## Summary\n",
        "| Concept | Key Points |\n",
        "|---------|------------|\n",
        "| **First-order condition** | $\\nabla g(w^*) = 0$ at stationary points |\n",
        "| **Gradient descent** | $w^{(k+1)} = w^{(k)} - \\alpha \\nabla g(w^{(k)})$ |\n",
        "| **Learning rate** | Critical for convergence |\n",
        "| **Weaknesses** | Zigzagging, saddle points |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 3.\n",
        "- **Supplementary**: Nocedal, J. & Wright, S. (2006). *Numerical Optimization*, Chapter 3.\n",
        "## Next Week\n",
        "**Second-Order Optimization: Newton's Method** (Chapter 4): Using curvature information for faster convergence.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}