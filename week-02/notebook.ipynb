{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-02/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 2: Zero-Order Optimization Techniques\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 2**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand the zero-order optimality condition\n",
        "- Apply global optimization methods\n",
        "- Apply local optimization methods\n",
        "- Implement random search and coordinate descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## Introduction (Section 2.1)\n",
        "**Zero-order methods** optimize functions using only function evaluationsâ€”no derivatives required.\n",
        "### When to Use Zero-Order Methods\n",
        "- Derivative is unavailable or expensive to compute\n",
        "- Function is non-smooth or discontinuous\n",
        "- Black-box optimization\n",
        "- Hyperparameter tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_3"
      },
      "source": [
        "## The Zero-Order Optimality Condition (Section 2.2)\n",
        "For an unconstrained minimization problem:\n",
        "$$\\min_{w} g(w)$$\n",
        "A point $w^*$ is a **global minimum** if:\n",
        "$$g(w^*) \\leq g(w) \\quad \\forall w$$\n",
        "A point $w^*$ is a **local minimum** if:\n",
        "$$g(w^*) \\leq g(w) \\quad \\forall w \\text{ in some neighborhood of } w^*$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_4"
      },
      "outputs": [],
      "source": [
        "# Visualize global vs local minima\n",
        "x = np.linspace(-2, 4, 500)\n",
        "g = lambda x: x**4 - 4*x**3 + 4*x**2 + 2\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(x, g(x), 'b-', linewidth=2)\n",
        "# Mark local minimum\n",
        "ax.plot(0, g(0), 'go', markersize=12, label='Local minimum at x=0')\n",
        "ax.plot(2, g(2), 'r*', markersize=15, label='Global minimum at x=2')\n",
        "ax.set_xlabel('w', fontsize=12)\n",
        "ax.set_ylabel('g(w)', fontsize=12)\n",
        "ax.set_title('Global vs Local Minima (ML Refined, Section 2.2)', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_5"
      },
      "source": [
        "mo.md(r\"\"\"\n",
        "## Global Optimization Methods (Section 2.3)\n",
        "### Exhaustive Grid Search\n",
        "Evaluate function at every point on a grid:\n",
        "```\n",
        "for each w in grid:\n",
        "    evaluate g(w)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_6"
      },
      "outputs": [],
      "source": [
        "# Grid search visualization\n",
        "def rastrigin(x, y):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_7"
      },
      "source": [
        "## Local Optimization Methods (Section 2.4)\n",
        "### The Descent Framework\n",
        "```\n",
        "1. Initialize: w = w_0\n",
        "2. Repeat:\n",
        "   a. Choose descent direction d\n",
        "   b. Choose step size Î±\n",
        "   c. Update: w = w + Î±Â·d\n",
        "3. Until: convergence\n",
        "```\n",
        "### Challenges\n",
        "- May converge to local minimum, not global\n",
        "- Choice of direction and step size is crucial\n",
        "- Initialization affects final result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_8"
      },
      "source": [
        "mo.md(r\"\"\"\n",
        "## Random Search (Section 2.5)\n",
        "### Algorithm\n",
        "Randomly sample points and keep the best:\n",
        "```python\n",
        "best_w = None\n",
        "best_cost = infinity\n",
        "for k = 1 to K:\n",
        "    w = sample_random_point()\n",
        "    cost = g(w)\n",
        "    if cost < best_cost:\n",
        "        best_w = w\n",
        "        best_cost = cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Random search demo\n",
        "np.random.seed(42)\n",
        "def himmelblau(x, y):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_10"
      },
      "source": [
        "## Coordinate Search and Descent (Section 2.6)\n",
        "### Coordinate Search\n",
        "Optimize one variable at a time while holding others fixed:\n",
        "```python\n",
        "while not converged:\n",
        "    for j = 1 to n:\n",
        "        w_j = argmin_{w_j} g(w_1, ..., w_j, ..., w_n)\n",
        "```\n",
        "### Coordinate Descent\n",
        "Move in coordinate directions with line search:\n",
        "```python\n",
        "while not converged:\n",
        "    for j = 1 to n:\n",
        "        direction = e_j  # j-th unit vector\n",
        "        Î± = line_search(w, direction)\n",
        "        w = w + Î± * direction\n",
        "```\n",
        "### Advantages\n",
        "- No gradient needed\n",
        "- Simple to implement\n",
        "- Works well when dimensions are separable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_11"
      },
      "outputs": [],
      "source": [
        "# Coordinate descent visualization\n",
        "def quadratic(w):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_12"
      },
      "source": [
        "## Summary\n",
        "| Method | Type | Pros | Cons |\n",
        "|--------|------|------|------|\n",
        "| **Grid Search** | Global | Guaranteed optimal (on grid) | Exponential cost |\n",
        "| **Random Search** | Global | Simple, parallel | Slow convergence |\n",
        "| **Coordinate Search** | Local | No gradients needed | Can be slow |\n",
        "| **Coordinate Descent** | Local | Simple, works well for separable | Zigzag path |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 2.\n",
        "- **Supplementary**: Nocedal, J. & Wright, S. (2006). *Numerical Optimization*, Chapter 9.\n",
        "## Next Week\n",
        "**First-Order Optimization: Gradient Descent** (Chapter 3): Using derivatives for faster optimization.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}