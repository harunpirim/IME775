{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 2: Calculus Foundations for Deep Learning\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ğŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 3\n",
        "---\n",
        "## Learning Objectives\n",
        "- Master derivatives and their role in optimization\n",
        "- Understand multivariable calculus for neural networks\n",
        "- Learn the chain rule as the foundation of backpropagation\n",
        "- Connect gradients to learning algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## 2.1 Derivatives: Measuring Change\n",
        "The derivative measures instantaneous rate of change:\n",
        "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n",
        "**Geometric interpretation**: Slope of tangent line = direction of steepest ascent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Visualize derivative as slope\n",
        "fig1, axes1 = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Left: Function and tangent\n",
        "ax1 = axes1[0]\n",
        "x = np.linspace(-2, 3, 200)\n",
        "f = lambda x: x**3 - 2*x**2 + 1\n",
        "f_prime = lambda x: 3*x**2 - 4*x\n",
        "ax1.plot(x, f(x), 'b-', linewidth=2, label='$f(x) = x^3 - 2x^2 + 1$')\n",
        "# Tangent at x=1.5\n",
        "x0 = 1.5\n",
        "slope = f_prime(x0)\n",
        "tangent = f(x0) + slope * (x - x0)\n",
        "ax1.plot(x, tangent, 'r--', linewidth=1.5, label=f'Tangent at x={x0}')\n",
        "ax1.scatter([x0], [f(x0)], color='red', s=100, zorder=5)\n",
        "ax1.axhline(0, color='gray', linewidth=0.5)\n",
        "ax1.axvline(0, color='gray', linewidth=0.5)\n",
        "ax1.set_xlim(-2, 3)\n",
        "ax1.set_ylim(-3, 5)\n",
        "ax1.legend()\n",
        "ax1.set_title(f\"Derivative: Slope = {slope:.2f}\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "# Right: Derivative function\n",
        "ax2 = axes1[1]\n",
        "ax2.plot(x, f(x), 'b-', linewidth=2, label='$f(x)$', alpha=0.5)\n",
        "ax2.plot(x, f_prime(x), 'r-', linewidth=2, label=\"$f'(x) = 3x^2 - 4x$\")\n",
        "ax2.axhline(0, color='gray', linewidth=0.5)\n",
        "ax2.axvline(0, color='gray', linewidth=0.5)\n",
        "# Mark critical points (where f'(x) = 0)\n",
        "critical_x = [0, 4/3]\n",
        "for cx in critical_x:\n",
        "    ax2.scatter([cx], [0], color='green', s=100, zorder=5, marker='o')\n",
        "    ax2.scatter([cx], [f(cx)], color='green', s=100, zorder=5, marker='s')\n",
        "    ax2.axvline(cx, color='green', linestyle=':', alpha=0.5)\n",
        "ax2.set_xlim(-2, 3)\n",
        "ax2.set_ylim(-3, 5)\n",
        "ax2.legend()\n",
        "ax2.set_title(\"Critical Points: $f'(x) = 0$\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_4"
      },
      "source": [
        "## 2.2 The Gradient: Multivariate Extension\n",
        "For scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$:\n",
        "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\end{bmatrix}$$\n",
        "**Key property**: Gradient points toward steepest ascent!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Visualize gradient as direction of steepest ascent\n",
        "fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Define function\n",
        "f_2d = lambda x, y: x**2 + 2*y**2  # Elliptical paraboloid\n",
        "grad_f = lambda x, y: np.array([2*x, 4*y])\n",
        "# Create grid\n",
        "x_2d = np.linspace(-3, 3, 50)\n",
        "y_2d = np.linspace(-3, 3, 50)\n",
        "X, Y = np.meshgrid(x_2d, y_2d)\n",
        "Z = f_2d(X, Y)\n",
        "# Left: Contour plot with gradients\n",
        "ax1_grad = axes2[0]\n",
        "contour = ax1_grad.contour(X, Y, Z, levels=15, cmap='viridis')\n",
        "ax1_grad.clabel(contour, inline=True, fontsize=8)\n",
        "# Plot gradient vectors at selected points\n",
        "points = [(-2, -1.5), (-1, 1), (1.5, -1), (0.5, 1.5), (2, 0.5)]\n",
        "for px, py in points:\n",
        "    g = grad_f(px, py)\n",
        "    g_norm = g / np.linalg.norm(g) * 0.5  # Normalize for visualization\n",
        "    ax1_grad.quiver(px, py, g_norm[0], g_norm[1], color='red', \n",
        "                   width=0.015, scale=5, label='Gradient' if (px, py) == points[0] else '')\n",
        "ax1_grad.set_xlabel('x')\n",
        "ax1_grad.set_ylabel('y')\n",
        "ax1_grad.set_title('Gradient: Direction of Steepest Ascent')\n",
        "ax1_grad.set_aspect('equal')\n",
        "# Right: 3D surface\n",
        "ax2_grad = axes2[1]\n",
        "ax2_grad = fig2.add_subplot(122, projection='3d')\n",
        "ax2_grad.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7, edgecolor='none')\n",
        "ax2_grad.set_xlabel('x')\n",
        "ax2_grad.set_ylabel('y')\n",
        "ax2_grad.set_zlabel('f(x,y)')\n",
        "ax2_grad.set_title('$f(x,y) = x^2 + 2y^2$')\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_6"
      },
      "source": [
        "## 2.3 Activation Functions and Their Derivatives\n",
        "| Function | Formula | Derivative |\n",
        "|----------|---------|------------|\n",
        "| Sigmoid | $\\sigma(x) = \\frac{1}{1+e^{-x}}$ | $\\sigma(x)(1-\\sigma(x))$ |\n",
        "| Tanh | $\\tanh(x)$ | $1 - \\tanh^2(x)$ |\n",
        "| ReLU | $\\max(0, x)$ | $\\mathbb{1}_{x>0}$ |\n",
        "| Leaky ReLU | $\\max(\\alpha x, x)$ | $\\alpha$ or $1$ |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Activation functions and their derivatives\n",
        "fig3, axes3 = plt.subplots(2, 2, figsize=(14, 10))\n",
        "x_act = np.linspace(-5, 5, 200)\n",
        "# Sigmoid\n",
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "sigmoid_grad = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
        "axes3[0, 0].plot(x_act, sigmoid(x_act), 'b-', linewidth=2, label='Ïƒ(x)')\n",
        "axes3[0, 0].plot(x_act, sigmoid_grad(x_act), 'r--', linewidth=2, label=\"Ïƒ'(x)\")\n",
        "axes3[0, 0].axhline(0, color='gray', linewidth=0.5)\n",
        "axes3[0, 0].axvline(0, color='gray', linewidth=0.5)\n",
        "axes3[0, 0].legend()\n",
        "axes3[0, 0].set_title('Sigmoid: Saturates â†’ Vanishing Gradients')\n",
        "axes3[0, 0].grid(True, alpha=0.3)\n",
        "# Tanh\n",
        "tanh_grad = lambda x: 1 - np.tanh(x)**2\n",
        "axes3[0, 1].plot(x_act, np.tanh(x_act), 'b-', linewidth=2, label='tanh(x)')\n",
        "axes3[0, 1].plot(x_act, tanh_grad(x_act), 'r--', linewidth=2, label=\"tanh'(x)\")\n",
        "axes3[0, 1].axhline(0, color='gray', linewidth=0.5)\n",
        "axes3[0, 1].axvline(0, color='gray', linewidth=0.5)\n",
        "axes3[0, 1].legend()\n",
        "axes3[0, 1].set_title('Tanh: Zero-Centered but Still Saturates')\n",
        "axes3[0, 1].grid(True, alpha=0.3)\n",
        "# ReLU\n",
        "relu = lambda x: np.maximum(0, x)\n",
        "relu_grad = lambda x: (x > 0).astype(float)\n",
        "axes3[1, 0].plot(x_act, relu(x_act), 'b-', linewidth=2, label='ReLU(x)')\n",
        "axes3[1, 0].plot(x_act, relu_grad(x_act), 'r--', linewidth=2, label=\"ReLU'(x)\")\n",
        "axes3[1, 0].axhline(0, color='gray', linewidth=0.5)\n",
        "axes3[1, 0].axvline(0, color='gray', linewidth=0.5)\n",
        "axes3[1, 0].legend()\n",
        "axes3[1, 0].set_title('ReLU: No Saturation (x > 0), Dead Neurons (x < 0)')\n",
        "axes3[1, 0].grid(True, alpha=0.3)\n",
        "# Softplus (smooth ReLU)\n",
        "softplus = lambda x: np.log(1 + np.exp(x))\n",
        "softplus_grad = sigmoid  # derivative of softplus is sigmoid!\n",
        "axes3[1, 1].plot(x_act, softplus(x_act), 'b-', linewidth=2, label='Softplus(x)')\n",
        "axes3[1, 1].plot(x_act, softplus_grad(x_act), 'r--', linewidth=2, label=\"Softplus'(x)\")\n",
        "axes3[1, 1].plot(x_act, relu(x_act), 'g:', linewidth=1, alpha=0.5, label='ReLU (reference)')\n",
        "axes3[1, 1].axhline(0, color='gray', linewidth=0.5)\n",
        "axes3[1, 1].axvline(0, color='gray', linewidth=0.5)\n",
        "axes3[1, 1].legend()\n",
        "axes3[1, 1].set_title('Softplus: Smooth Approximation to ReLU')\n",
        "axes3[1, 1].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## 2.4 The Chain Rule: Foundation of Backpropagation\n",
        "For $z = f(g(x))$:\n",
        "$$\\frac{dz}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
        "**Neural network**: Each layer is a function composition\n",
        "$$L = \\text{loss}(\\text{layer}_n(\\cdots \\text{layer}_2(\\text{layer}_1(x))))$$\n",
        "Gradients flow backward through the chain!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_9"
      },
      "source": [
        "### Example: Simple Neural Network\n",
        "Single neuron: $y = \\sigma(wx + b)$\n",
        "Loss: $L = (y - t)^2$ where $t$ is target\n",
        "**Forward pass**:\n",
        "1. $z = wx + b$\n",
        "2. $y = \\sigma(z)$\n",
        "3. $L = (y - t)^2$\n",
        "**Backward pass** (chain rule):\n",
        "- $\\frac{\\partial L}{\\partial y} = 2(y - t)$\n",
        "- $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\cdot \\sigma'(z)$\n",
        "- $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} \\cdot x$\n",
        "- $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_10"
      },
      "outputs": [],
      "source": [
        "# Demonstrate chain rule in action\n",
        "def forward_backward_demo():\n",
        "    np.random.seed(42)\n",
        "    # Simple neural network: y = sigmoid(wx + b)\n",
        "    # Loss: L = (y - target)^2\n",
        "    # Parameters\n",
        "    w = 0.5\n",
        "    b = 0.1\n",
        "    x = 2.0\n",
        "    target = 0.8\n",
        "    # Forward pass\n",
        "    z = w * x + b\n",
        "    y = 1 / (1 + np.exp(-z))\n",
        "    L = (y - target) ** 2\n",
        "    # Backward pass (analytical gradients)\n",
        "    dL_dy = 2 * (y - target)\n",
        "    dy_dz = y * (1 - y)  # sigmoid derivative\n",
        "    dL_dz = dL_dy * dy_dz\n",
        "    dL_dw = dL_dz * x\n",
        "    dL_db = dL_dz\n",
        "    # Numerical gradients for verification\n",
        "    eps = 1e-7\n",
        "    # Numerical dL/dw\n",
        "    z_plus = (w + eps) * x + b\n",
        "    y_plus = 1 / (1 + np.exp(-z_plus))\n",
        "    L_plus = (y_plus - target) ** 2\n",
        "    z_minus = (w - eps) * x + b\n",
        "    y_minus = 1 / (1 + np.exp(-z_minus))\n",
        "    L_minus = (y_minus - target) ** 2\n",
        "    numerical_dL_dw = (L_plus - L_minus) / (2 * eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "## 2.5 Gradient Checking\n",
        "**Numerical gradient** (for verification):\n",
        "$$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + \\epsilon e_i) - f(x - \\epsilon e_i)}{2\\epsilon}$$\n",
        "**Gradient check**: Compare analytical vs numerical gradients\n",
        "This is crucial for debugging custom neural networks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_12"
      },
      "outputs": [],
      "source": [
        "# Gradient checking visualization\n",
        "def numerical_gradient(f, x, eps=1e-5):\n",
        "    grad = np.zeros_like(x)\n",
        "    for i in range(len(x)):\n",
        "        x_plus = x.copy()\n",
        "        x_plus[i] += eps\n",
        "        x_minus = x.copy()\n",
        "        x_minus[i] -= eps\n",
        "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_13"
      },
      "source": [
        "## 2.6 The Hessian: Second-Order Information\n",
        "The Hessian matrix of second derivatives:\n",
        "$$\\mathbf{H} = \\begin{bmatrix}\n",
        "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n",
        "\\end{bmatrix}$$\n",
        "**Eigenvalues** tell us about curvature:\n",
        "- All positive â†’ local minimum\n",
        "- All negative â†’ local maximum\n",
        "- Mixed signs â†’ saddle point\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_14"
      },
      "outputs": [],
      "source": [
        "# Visualize different curvature scenarios\n",
        "fig5 = plt.figure(figsize=(15, 4))\n",
        "x_hess = np.linspace(-2, 2, 50)\n",
        "y_hess = np.linspace(-2, 2, 50)\n",
        "X_hess, Y_hess = np.meshgrid(x_hess, y_hess)\n",
        "# Local minimum: f = x^2 + y^2\n",
        "ax1_hess = fig5.add_subplot(131, projection='3d')\n",
        "Z1 = X_hess**2 + Y_hess**2\n",
        "ax1_hess.plot_surface(X_hess, Y_hess, Z1, cmap='viridis', alpha=0.8)\n",
        "ax1_hess.set_title('Local Min: H positive definite\\nEigenvalues: 2, 2')\n",
        "ax1_hess.set_xlabel('x')\n",
        "ax1_hess.set_ylabel('y')\n",
        "# Saddle point: f = x^2 - y^2\n",
        "ax2_hess = fig5.add_subplot(132, projection='3d')\n",
        "Z2 = X_hess**2 - Y_hess**2\n",
        "ax2_hess.plot_surface(X_hess, Y_hess, Z2, cmap='RdYlGn', alpha=0.8)\n",
        "ax2_hess.set_title('Saddle Point: H indefinite\\nEigenvalues: 2, -2')\n",
        "ax2_hess.set_xlabel('x')\n",
        "ax2_hess.set_ylabel('y')\n",
        "# Ill-conditioned: f = x^2 + 100*y^2\n",
        "ax3_hess = fig5.add_subplot(133, projection='3d')\n",
        "Z3 = X_hess**2 + 10*Y_hess**2\n",
        "ax3_hess.plot_surface(X_hess, Y_hess, Z3, cmap='plasma', alpha=0.8)\n",
        "ax3_hess.set_title('Ill-conditioned: Îº = 10\\nEigenvalues: 2, 20')\n",
        "ax3_hess.set_xlabel('x')\n",
        "ax3_hess.set_ylabel('y')\n",
        "plt.tight_layout()\n",
        "fig5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_15"
      },
      "source": [
        "## Summary\n",
        "| Concept | Definition | Deep Learning Role |\n",
        "|---------|------------|-------------------|\n",
        "| **Derivative** | Rate of change | Sensitivity of loss to parameters |\n",
        "| **Gradient** | Vector of partials | Direction for parameter updates |\n",
        "| **Chain Rule** | Composition derivative | Backpropagation algorithm |\n",
        "| **Hessian** | Second derivatives | Curvature, adaptive learning rates |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 3.\n",
        "- **Supplementary**: Goodfellow, I., et al. (2016). *Deep Learning*, Chapter 4.\n",
        "## Connection to ML Refined Curriculum\n",
        "This calculus foundation supports:\n",
        "- Week 2-3: Optimization algorithms (gradient descent variants)\n",
        "- Weeks 4-8: Computing gradients for regression and classification\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}