{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 12: Kernel Methods & Neural Networks\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapters 12-13**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand the kernel trick\n",
        "- Apply kernel methods for nonlinear classification\n",
        "- Understand neural network architecture\n",
        "- Implement forward propagation and backpropagation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## Kernel Methods (Chapter 12)\n",
        "### The Key Insight (Section 12.2-12.3)\n",
        "Many algorithms depend on data only through **inner products** $x_i^T x_j$.\n",
        "The **kernel trick**: Replace inner product with kernel function:\n",
        "$$K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$$\n",
        "Without explicitly computing $\\phi(x)$!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_3"
      },
      "source": [
        "## Common Kernels (Section 12.4)\n",
        "| Kernel | Formula | Use Case |\n",
        "|--------|---------|----------|\n",
        "| **Linear** | $x^T x'$ | Linear relationships |\n",
        "| **Polynomial** | $(1 + x^T x')^d$ | Polynomial features |\n",
        "| **RBF (Gaussian)** | $\\exp(-\\gamma \\|x - x'\\|^2)$ | Most common, general |\n",
        "| **Sigmoid** | $\\tanh(\\alpha x^T x' + c)$ | Neural network-like |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_4"
      },
      "outputs": [],
      "source": [
        "# Kernel SVM comparison\n",
        "np.random.seed(42)\n",
        "n = 200\n",
        "# Generate non-linearly separable data (circles)\n",
        "theta = np.random.uniform(0, 2*np.pi, n)\n",
        "r_inner = 1 + 0.3 * np.random.randn(n//2)\n",
        "r_outer = 3 + 0.3 * np.random.randn(n//2)\n",
        "X_inner = np.column_stack([r_inner * np.cos(theta[:n//2]), r_inner * np.sin(theta[:n//2])])\n",
        "X_outer = np.column_stack([r_outer * np.cos(theta[n//2:]), r_outer * np.sin(theta[n//2:])])\n",
        "X = np.vstack([X_inner, X_outer])\n",
        "y = np.array([0]*(n//2) + [1]*(n//2))\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for ax, kernel in zip(axes, kernels):\n",
        "    svm = SVC(kernel=kernel, gamma='auto')\n",
        "    svm.fit(X, y)\n",
        "    # Decision boundary\n",
        "    xx, yy = np.meshgrid(np.linspace(-5, 5, 200), np.linspace(-5, 5, 200))\n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "    ax.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=20, alpha=0.7)\n",
        "    ax.scatter(X[y==1, 0], X[y==1, 1], c='red', s=20, alpha=0.7)\n",
        "    ax.set_title(f'{kernel.upper()} Kernel')\n",
        "    ax.set_xlim(-5, 5)\n",
        "    ax.set_ylim(-5, 5)\n",
        "fig.suptitle('Kernel SVM Comparison (ML Refined, Chapter 12)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_5"
      },
      "source": [
        "## Neural Networks (Chapter 13)\n",
        "### Fully Connected Neural Networks (Section 13.2)\n",
        "A neural network composes linear transformations with nonlinear activations:\n",
        "$$h^{(1)} = \\sigma(W^{(1)} x + b^{(1)})$$\n",
        "$$h^{(2)} = \\sigma(W^{(2)} h^{(1)} + b^{(2)})$$\n",
        "$$\\vdots$$\n",
        "$$f(x) = W^{(L)} h^{(L-1)} + b^{(L)}$$\n",
        "Where $\\sigma$ is a nonlinear activation function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_6"
      },
      "source": [
        "## Activation Functions (Section 13.3)\n",
        "| Function | Formula | Properties |\n",
        "|----------|---------|------------|\n",
        "| **Sigmoid** | $\\frac{1}{1+e^{-z}}$ | Output in (0,1), vanishing gradient |\n",
        "| **Tanh** | $\\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | Output in (-1,1), zero-centered |\n",
        "| **ReLU** | $\\max(0, z)$ | Simple, no vanishing gradient |\n",
        "| **Leaky ReLU** | $\\max(\\alpha z, z)$ | No dying neurons |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Activation functions\n",
        "z = np.linspace(-5, 5, 100)\n",
        "sigmoid = 1 / (1 + np.exp(-z))\n",
        "tanh = np.tanh(z)\n",
        "relu = np.maximum(0, z)\n",
        "leaky_relu = np.where(z > 0, z, 0.1 * z)\n",
        "fig2, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "funcs = [(sigmoid, 'Sigmoid'), (tanh, 'Tanh'), \n",
        "         (relu, 'ReLU'), (leaky_relu, 'Leaky ReLU')]\n",
        "for ax, (func, name) in zip(axes.flat, funcs):\n",
        "    ax.plot(z, func, 'b-', linewidth=2)\n",
        "    ax.axhline(0, color='gray', linewidth=0.5)\n",
        "    ax.axvline(0, color='gray', linewidth=0.5)\n",
        "    ax.set_xlabel('z')\n",
        "    ax.set_ylabel(f'{name}(z)')\n",
        "    ax.set_title(name)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "fig2.suptitle('Activation Functions (ML Refined, Section 13.3)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## The Backpropagation Algorithm (Section 13.4)\n",
        "### Forward Pass\n",
        "Compute all layer outputs from input to output.\n",
        "### Backward Pass (Chain Rule)\n",
        "$$\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial h^{(l)}} \\cdot \\frac{\\partial h^{(l)}}{\\partial W^{(l)}}$$\n",
        "Propagate gradients from output back to input.\n",
        "### Key Insight\n",
        "Chain rule enables efficient gradient computation in $O(n)$ operations, where $n$ is the number of parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Simple neural network visualization\n",
        "def draw_neural_network(ax, layer_sizes):\n",
        "    v_spacing = 1.0\n",
        "    h_spacing = 2.0\n",
        "    # Draw nodes\n",
        "    for i, size in enumerate(layer_sizes):\n",
        "        x = i * h_spacing\n",
        "        y_offset = (max(layer_sizes) - size) / 2\n",
        "        for j in range(size):\n",
        "            y = j * v_spacing + y_offset\n",
        "            circle = plt.Circle((x, y), 0.3, color='steelblue', ec='black')\n",
        "            ax.add_patch(circle)\n",
        "            # Draw connections to next layer\n",
        "            if i < len(layer_sizes) - 1:\n",
        "                next_size = layer_sizes[i + 1]\n",
        "                next_y_offset = (max(layer_sizes) - next_size) / 2\n",
        "                for k in range(next_size):\n",
        "                    next_y = k * v_spacing + next_y_offset\n",
        "                    ax.plot([x + 0.3, (i + 1) * h_spacing - 0.3], \n",
        "                           [y, next_y], 'gray', linewidth=0.5, alpha=0.5)\n",
        "    ax.set_xlim(-1, (len(layer_sizes) - 1) * h_spacing + 1)\n",
        "    ax.set_ylim(-1, max(layer_sizes) * v_spacing)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "fig3, ax3 = plt.subplots(figsize=(12, 6))\n",
        "draw_neural_network(ax3, [4, 6, 4, 2])\n",
        "ax3.set_title('Fully Connected Neural Network: [4, 6, 4, 2]', fontsize=14)\n",
        "# Add labels\n",
        "labels = ['Input\\nLayer', 'Hidden\\nLayer 1', 'Hidden\\nLayer 2', 'Output\\nLayer']\n",
        "for i, label in enumerate(labels):\n",
        "    ax3.text(i * 2, -1.5, label, ha='center', fontsize=10)\n",
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_10"
      },
      "source": [
        "## Batch Normalization (Section 13.6)\n",
        "Normalize activations within each layer:\n",
        "$$\\hat{h} = \\frac{h - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
        "$$\\tilde{h} = \\gamma \\hat{h} + \\beta$$\n",
        "### Benefits\n",
        "- Faster training\n",
        "- Higher learning rates\n",
        "- Some regularization effect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "## Summary\n",
        "| Method | Key Idea | Use Case |\n",
        "|--------|----------|----------|\n",
        "| **Kernel Methods** | Implicit feature mapping | Small-medium data |\n",
        "| **Neural Networks** | Learned feature hierarchy | Large data, complex patterns |\n",
        "| **Backpropagation** | Efficient gradient computation | Training NNs |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapters 12-13.\n",
        "- **Supplementary**: Goodfellow, I. et al. (2016). *Deep Learning*, Chapters 5-6.\n",
        "## Next Week\n",
        "**Tree-Based Learners & Advanced Topics** (Chapter 14): Decision trees and ensemble methods.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}