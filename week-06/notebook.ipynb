{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 6: Linear Two-Class Classification\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "üìñ **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 6**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand logistic regression and cross-entropy loss\n",
        "- Implement the perceptron algorithm\n",
        "- Formulate and apply support vector machines\n",
        "- Evaluate classification quality metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## Introduction (Section 6.1)\n",
        "**Binary Classification**: Predict a discrete label $y \\in \\{-1, +1\\}$ (or $\\{0, 1\\}$).\n",
        "### The Linear Classifier\n",
        "$$\\hat{y} = \\text{sign}(w^T \\tilde{x}) = \\text{sign}(w_0 + w_1 x_1 + \\cdots + w_n x_n)$$\n",
        "The decision boundary is a **hyperplane**: $w^T \\tilde{x} = 0$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Visualize linear classification\n",
        "np.random.seed(42)\n",
        "n = 50\n",
        "# Generate two classes\n",
        "X_pos = np.random.randn(n, 2) + np.array([2, 2])\n",
        "X_neg = np.random.randn(n, 2) + np.array([-2, -2])\n",
        "X = np.vstack([X_pos, X_neg])\n",
        "y = np.array([1]*n + [-1]*n)\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.scatter(X_pos[:, 0], X_pos[:, 1], c='blue', s=60, label='Class +1', edgecolors='black')\n",
        "ax.scatter(X_neg[:, 0], X_neg[:, 1], c='red', s=60, label='Class -1', edgecolors='black')\n",
        "# Decision boundary\n",
        "x_line = np.linspace(-5, 5, 100)\n",
        "ax.plot(x_line, x_line, 'k-', linewidth=2, label='Decision boundary')\n",
        "ax.fill_between(x_line, x_line, 5, alpha=0.1, color='blue')\n",
        "ax.fill_between(x_line, x_line, -5, alpha=0.1, color='red')\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_title('Linear Binary Classification (ML Refined, Chapter 6)')\n",
        "ax.legend()\n",
        "ax.set_xlim(-5, 5)\n",
        "ax.set_ylim(-5, 5)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_aspect('equal')\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_4"
      },
      "source": [
        "## Logistic Regression (Section 6.2)\n",
        "### The Sigmoid Function\n",
        "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$\n",
        "Maps any real number to $(0, 1)$ ‚Äî interpretable as probability.\n",
        "### The Model\n",
        "$$P(y = +1 | x) = \\sigma(w^T \\tilde{x}) = \\frac{1}{1 + e^{-w^T \\tilde{x}}}$$\n",
        "### Cross-Entropy Cost\n",
        "$$g(w) = \\frac{1}{P} \\sum_{p=1}^{P} \\log(1 + e^{-y_p w^T \\tilde{x}_p})$$\n",
        "This is the **softmax cost** for binary classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Sigmoid function\n",
        "t = np.linspace(-6, 6, 100)\n",
        "sigmoid = 1 / (1 + np.exp(-t))\n",
        "fig2, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Sigmoid\n",
        "ax1 = axes[0]\n",
        "ax1.plot(t, sigmoid, 'b-', linewidth=2)\n",
        "ax1.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "ax1.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "ax1.set_xlabel('t')\n",
        "ax1.set_ylabel('œÉ(t)')\n",
        "ax1.set_title('Sigmoid Function œÉ(t) = 1/(1 + e‚Åª·µó)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "# Cross-entropy loss\n",
        "ax2 = axes[1]\n",
        "margin = np.linspace(-3, 3, 100)\n",
        "ce_loss = np.log(1 + np.exp(-margin))\n",
        "ax2.plot(margin, ce_loss, 'b-', linewidth=2, label='Cross-entropy')\n",
        "ax2.plot(margin, np.maximum(0, 1 - margin), 'r-', linewidth=2, label='Hinge (SVM)')\n",
        "ax2.set_xlabel('y ¬∑ f(x) (margin)')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Classification Loss Functions')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_6"
      },
      "source": [
        "## The Perceptron (Section 6.4)\n",
        "### The Perceptron Cost\n",
        "$$g(w) = \\frac{1}{P} \\sum_{p=1}^{P} \\max(0, -y_p w^T \\tilde{x}_p)$$\n",
        "Only penalizes misclassified points.\n",
        "### The Perceptron Algorithm\n",
        "For each misclassified point:\n",
        "$$w \\leftarrow w + y_p \\tilde{x}_p$$\n",
        "### Properties\n",
        "- Converges for linearly separable data\n",
        "- Simple and fast\n",
        "- No probability output\n",
        "- Not unique solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Perceptron algorithm\n",
        "np.random.seed(42)\n",
        "n_perc = 30\n",
        "X_pos_p = np.random.randn(n_perc, 2) + np.array([2, 2])\n",
        "X_neg_p = np.random.randn(n_perc, 2) + np.array([-2, -2])\n",
        "X_p = np.vstack([X_pos_p, X_neg_p])\n",
        "y_p = np.array([1]*n_perc + [-1]*n_perc)\n",
        "# Add bias\n",
        "X_aug = np.column_stack([np.ones(2*n_perc), X_p])\n",
        "# Perceptron\n",
        "w = np.zeros(3)\n",
        "history = [w.copy()]\n",
        "for epoch in range(10):\n",
        "    for i in range(len(y_p)):\n",
        "        if y_p[i] * (X_aug[i] @ w) <= 0:\n",
        "            w = w + y_p[i] * X_aug[i]\n",
        "            history.append(w.copy())\n",
        "# Visualization\n",
        "fig3, ax3 = plt.subplots(figsize=(10, 8))\n",
        "ax3.scatter(X_pos_p[:, 0], X_pos_p[:, 1], c='blue', s=60, label='Class +1', edgecolors='black')\n",
        "ax3.scatter(X_neg_p[:, 0], X_neg_p[:, 1], c='red', s=60, label='Class -1', edgecolors='black')\n",
        "# Decision boundaries over time\n",
        "x_line = np.linspace(-5, 5, 100)\n",
        "for i, w_hist in enumerate(history[::max(1, len(history)//5)]):\n",
        "    if abs(w_hist[2]) > 0.01:\n",
        "        y_line = -(w_hist[0] + w_hist[1]*x_line) / w_hist[2]\n",
        "        alpha = 0.3 + 0.7 * (i / len(history[::max(1, len(history)//5)]))\n",
        "        ax3.plot(x_line, y_line, 'g-', alpha=alpha, linewidth=1)\n",
        "# Final boundary\n",
        "if abs(w[2]) > 0.01:\n",
        "    y_line_final = -(w[0] + w[1]*x_line) / w[2]\n",
        "    ax3.plot(x_line, y_line_final, 'k-', linewidth=2, label='Final boundary')\n",
        "ax3.set_xlabel('$x_1$')\n",
        "ax3.set_ylabel('$x_2$')\n",
        "ax3.set_title(f'Perceptron Learning ({len(history)} updates)')\n",
        "ax3.legend()\n",
        "ax3.set_xlim(-5, 5)\n",
        "ax3.set_ylim(-5, 5)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## Support Vector Machines (Section 6.5)\n",
        "### Maximum Margin Classifier\n",
        "Find the hyperplane that maximizes the **margin** ‚Äî the distance to the nearest point.\n",
        "### Optimization Problem\n",
        "$$\\min_{w} \\frac{1}{2}\\|w\\|^2$$\n",
        "Subject to:\n",
        "$$y_p(w^T \\tilde{x}_p) \\geq 1, \\quad p = 1, \\ldots, P$$\n",
        "### Soft-Margin SVM\n",
        "For non-separable data:\n",
        "$$\\min_{w,\\xi} \\frac{1}{2}\\|w\\|^2 + C \\sum_{p=1}^{P} \\xi_p$$\n",
        "Subject to:\n",
        "$$y_p(w^T \\tilde{x}_p) \\geq 1 - \\xi_p, \\quad \\xi_p \\geq 0$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# SVM visualization\n",
        "np.random.seed(42)\n",
        "n_svm = 50\n",
        "X_pos_s = np.random.randn(n_svm, 2) + np.array([1.5, 1.5])\n",
        "X_neg_s = np.random.randn(n_svm, 2) + np.array([-1.5, -1.5])\n",
        "X_s = np.vstack([X_pos_s, X_neg_s])\n",
        "y_s = np.array([1]*n_svm + [-1]*n_svm)\n",
        "# Fit SVM\n",
        "svm = SVC(kernel='linear', C=1)\n",
        "svm.fit(X_s, y_s)\n",
        "# Create mesh\n",
        "xx, yy = np.meshgrid(np.linspace(-5, 5, 200), np.linspace(-5, 5, 200))\n",
        "Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig4, ax4 = plt.subplots(figsize=(10, 8))\n",
        "# Decision boundary and margins\n",
        "ax4.contour(xx, yy, Z, levels=[-1, 0, 1], colors=['red', 'black', 'blue'],\n",
        "            linestyles=['--', '-', '--'], linewidths=2)\n",
        "ax4.contourf(xx, yy, Z, levels=[-1, 1], alpha=0.2, colors=['gray'])\n",
        "ax4.scatter(X_pos_s[:, 0], X_pos_s[:, 1], c='blue', s=60, edgecolors='black')\n",
        "ax4.scatter(X_neg_s[:, 0], X_neg_s[:, 1], c='red', s=60, edgecolors='black')\n",
        "# Support vectors\n",
        "ax4.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
        "            s=200, facecolors='none', edgecolors='green', linewidths=2,\n",
        "            label=f'Support vectors ({len(svm.support_)})')\n",
        "ax4.set_xlabel('$x_1$')\n",
        "ax4.set_ylabel('$x_2$')\n",
        "ax4.set_title('Support Vector Machine (ML Refined, Section 6.5)')\n",
        "ax4.legend()\n",
        "ax4.set_xlim(-5, 5)\n",
        "ax4.set_ylim(-5, 5)\n",
        "fig4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_10"
      },
      "source": [
        "## Classification Quality Metrics (Section 6.8)\n",
        "### Confusion Matrix\n",
        "|  | Predicted + | Predicted - |\n",
        "|--|------------|------------|\n",
        "| **Actual +** | True Positive (TP) | False Negative (FN) |\n",
        "| **Actual -** | False Positive (FP) | True Negative (TN) |\n",
        "### Key Metrics\n",
        "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "$$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "## Summary\n",
        "| Method | Cost Function | Properties |\n",
        "|--------|--------------|------------|\n",
        "| **Logistic Regression** | Cross-entropy | Probabilistic, smooth |\n",
        "| **Perceptron** | Hinge-like | Simple, fast |\n",
        "| **SVM** | Hinge + margin | Maximum margin, support vectors |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 6.\n",
        "- **Supplementary**: James, G. et al. (2023). *An Introduction to Statistical Learning*, Chapter 4.\n",
        "## Next Week\n",
        "**Linear Multi-Class Classification** (Chapter 7): Extending to more than two classes.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}