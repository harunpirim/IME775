{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 6: Regularization and Generalization\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 7\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand the bias-variance tradeoff\n",
        "- Master L2 regularization and dropout\n",
        "- Learn batch normalization\n",
        "- Implement early stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## 6.1 The Overfitting Problem\n",
        "**Overfitting**: Model memorizes training data but fails on new data.\n",
        "- Training loss â†“ but validation loss â†‘\n",
        "- High variance, low bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Demonstrate overfitting\n",
        "np.random.seed(42)\n",
        "# Generate noisy data\n",
        "n_samples = 50\n",
        "X_demo = np.linspace(0, 1, n_samples).reshape(-1, 1)\n",
        "y_true = np.sin(2 * np.pi * X_demo).ravel()\n",
        "y_demo = y_true + np.random.randn(n_samples) * 0.3\n",
        "X_train_demo, X_val_demo, y_train_demo, y_val_demo = train_test_split(\n",
        "    X_demo, y_demo, test_size=0.3, random_state=42)\n",
        "# Fit polynomials of different degrees\n",
        "from numpy.polynomial import polynomial as P\n",
        "fig1, axes1 = plt.subplots(1, 3, figsize=(15, 4))\n",
        "degrees = [1, 4, 15]\n",
        "titles = ['Underfitting (degree=1)', 'Good Fit (degree=4)', 'Overfitting (degree=15)']\n",
        "X_plot = np.linspace(0, 1, 100).reshape(-1, 1)\n",
        "for ax, degree, title in zip(axes1, degrees, titles):\n",
        "    # Fit polynomial\n",
        "    coeffs = np.polyfit(X_train_demo.ravel(), y_train_demo, degree)\n",
        "    poly = np.poly1d(coeffs)\n",
        "    # Compute errors\n",
        "    train_error = np.mean((poly(X_train_demo.ravel()) - y_train_demo) ** 2)\n",
        "    val_error = np.mean((poly(X_val_demo.ravel()) - y_val_demo) ** 2)\n",
        "    # Plot\n",
        "    ax.scatter(X_train_demo, y_train_demo, c='blue', s=30, label='Train', alpha=0.7)\n",
        "    ax.scatter(X_val_demo, y_val_demo, c='red', s=30, label='Validation', alpha=0.7)\n",
        "    ax.plot(X_plot, poly(X_plot), 'g-', linewidth=2, label='Model')\n",
        "    ax.plot(X_plot, np.sin(2 * np.pi * X_plot), 'k--', alpha=0.5, label='True')\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(f'{title}\\nTrain MSE: {train_error:.3f}, Val MSE: {val_error:.3f}')\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.set_ylim(-2, 2)\n",
        "plt.tight_layout()\n",
        "fig1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_4"
      },
      "source": [
        "## 6.2 L2 Regularization (Weight Decay)\n",
        "Add penalty on weight magnitudes:\n",
        "$$L_{total} = L_{data} + \\frac{\\lambda}{2} \\|W\\|^2$$\n",
        "**Effect**: Shrinks weights, reduces model complexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Visualize L2 regularization effect\n",
        "np.random.seed(42)\n",
        "# Generate polynomial features\n",
        "n_train = 20\n",
        "X_train_l2 = np.random.uniform(0, 1, n_train)\n",
        "y_train_l2 = np.sin(2 * np.pi * X_train_l2) + np.random.randn(n_train) * 0.3\n",
        "# Create polynomial features (degree 10)\n",
        "degree_l2 = 10\n",
        "X_poly = np.column_stack([X_train_l2 ** i for i in range(degree_l2 + 1)])\n",
        "# Ridge regression with different lambda\n",
        "def ridge_regression(X, y, lambda_reg):\n",
        "    n_features = X.shape[1]\n",
        "    I = np.eye(n_features)\n",
        "    I[0, 0] = 0  # Don't regularize bias\n",
        "    w = np.linalg.solve(X.T @ X + lambda_reg * I, X.T @ y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_6"
      },
      "source": [
        "## 6.3 Dropout\n",
        "During training, randomly zero out neurons with probability $p$:\n",
        "$$h = \\frac{1}{1-p} \\cdot \\text{mask} \\odot \\sigma(z)$$\n",
        "**Effect**: Creates implicit ensemble of sub-networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Demonstrate dropout\n",
        "class DropoutDemo:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        self.training = True\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            mask = (np.random.rand(*x.shape) > self.p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## 6.4 Batch Normalization\n",
        "Normalize activations within each mini-batch:\n",
        "$$\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
        "$$y = \\gamma \\hat{x} + \\beta$$\n",
        "**Benefits**: Faster training, regularization, higher learning rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Demonstrate batch normalization effect\n",
        "np.random.seed(42)\n",
        "# Simulate activations before BatchNorm\n",
        "n_samples_bn = 1000\n",
        "mean_shift = 5.0\n",
        "scale = 3.0\n",
        "activations_before = np.random.randn(n_samples_bn) * scale + mean_shift\n",
        "# Apply BatchNorm\n",
        "mu = activations_before.mean()\n",
        "sigma = activations_before.std()\n",
        "activations_after = (activations_before - mu) / (sigma + 1e-5)\n",
        "# Apply learned scale and shift (example)\n",
        "gamma, beta = 1.5, 0.5\n",
        "activations_final = gamma * activations_after + beta\n",
        "fig4, axes4 = plt.subplots(1, 3, figsize=(15, 4))\n",
        "axes4[0].hist(activations_before, bins=30, density=True, alpha=0.7, color='blue')\n",
        "axes4[0].axvline(mu, color='red', linestyle='--', label=f'mean={mu:.2f}')\n",
        "axes4[0].set_xlabel('Activation')\n",
        "axes4[0].set_ylabel('Density')\n",
        "axes4[0].set_title(f'Before BatchNorm\\nÎ¼={mu:.2f}, Ïƒ={sigma:.2f}')\n",
        "axes4[0].legend()\n",
        "axes4[1].hist(activations_after, bins=30, density=True, alpha=0.7, color='green')\n",
        "axes4[1].axvline(0, color='red', linestyle='--', label='mean=0')\n",
        "axes4[1].set_xlabel('Activation')\n",
        "axes4[1].set_ylabel('Density')\n",
        "axes4[1].set_title('After Normalization\\nÎ¼=0, Ïƒ=1')\n",
        "axes4[1].legend()\n",
        "axes4[2].hist(activations_final, bins=30, density=True, alpha=0.7, color='purple')\n",
        "axes4[2].axvline(activations_final.mean(), color='red', linestyle='--', \n",
        "                label=f'mean={activations_final.mean():.2f}')\n",
        "axes4[2].set_xlabel('Activation')\n",
        "axes4[2].set_ylabel('Density')\n",
        "axes4[2].set_title(f'After Scale/Shift (Î³={gamma}, Î²={beta})\\nLearnable parameters')\n",
        "axes4[2].legend()\n",
        "plt.tight_layout()\n",
        "fig4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_10"
      },
      "source": [
        "## 6.5 Early Stopping\n",
        "Stop training when validation loss starts increasing:\n",
        "1. Monitor validation loss\n",
        "2. Save best model\n",
        "3. Stop if no improvement for $k$ epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_11"
      },
      "outputs": [],
      "source": [
        "# Simulate early stopping\n",
        "np.random.seed(42)\n",
        "epochs_es = 100\n",
        "train_loss = 1.0 / (1 + np.arange(epochs_es) * 0.1) + np.random.randn(epochs_es) * 0.02\n",
        "val_loss = 1.0 / (1 + np.arange(epochs_es) * 0.08) + 0.02 * np.arange(epochs_es) / epochs_es + np.random.randn(epochs_es) * 0.02\n",
        "# Find early stopping point\n",
        "patience = 10\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "patience_counter = 0\n",
        "stop_epoch = epochs_es\n",
        "for epoch in range(epochs_es):\n",
        "    if val_loss[epoch] < best_val_loss:\n",
        "        best_val_loss = val_loss[epoch]\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "        stop_epoch = epoch\n",
        "        break\n",
        "fig5, ax5 = plt.subplots(figsize=(10, 5))\n",
        "ax5.plot(train_loss, 'b-', linewidth=2, label='Training Loss')\n",
        "ax5.plot(val_loss, 'r-', linewidth=2, label='Validation Loss')\n",
        "ax5.axvline(best_epoch, color='green', linestyle='--', linewidth=2, label=f'Best Model (epoch {best_epoch})')\n",
        "ax5.axvline(stop_epoch, color='orange', linestyle='--', linewidth=2, label=f'Early Stop (epoch {stop_epoch})')\n",
        "ax5.fill_between(range(stop_epoch, epochs_es), 0, max(val_loss), alpha=0.2, color='red', label='Overfitting region')\n",
        "ax5.set_xlabel('Epoch')\n",
        "ax5.set_ylabel('Loss')\n",
        "ax5.set_title(f'Early Stopping (patience={patience})')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "fig5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_12"
      },
      "source": [
        "## 6.6 Data Augmentation\n",
        "Artificially expand training data by applying transformations.\n",
        "| Type | Transformations |\n",
        "|------|-----------------|\n",
        "| Geometric | Rotation, flip, crop, scale |\n",
        "| Color | Brightness, contrast, hue |\n",
        "| Noise | Gaussian noise, blur |\n",
        "| Mixing | Mixup, CutMix |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_13"
      },
      "outputs": [],
      "source": [
        "# Demonstrate data augmentation on a simple 2D example\n",
        "np.random.seed(42)\n",
        "# Original point\n",
        "original_point = np.array([0.5, 0.5])\n",
        "# Augmentation functions\n",
        "def rotate(point, angle):\n",
        "    c, s = np.cos(angle), np.sin(angle)\n",
        "    R = np.array([[c, -s], [s, c]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_14"
      },
      "source": [
        "## 6.7 Mixup: Advanced Augmentation\n",
        "Create virtual training examples by mixing:\n",
        "$$\\tilde{x} = \\lambda x_i + (1-\\lambda) x_j$$\n",
        "$$\\tilde{y} = \\lambda y_i + (1-\\lambda) y_j$$\n",
        "Where $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_15"
      },
      "outputs": [],
      "source": [
        "# Demonstrate Mixup\n",
        "def visualize_mixup():\n",
        "    np.random.seed(42)\n",
        "    # Two sample \"images\" (simplified as 1D signals)\n",
        "    x1 = np.sin(np.linspace(0, 2*np.pi, 50))\n",
        "    x2 = np.cos(np.linspace(0, 2*np.pi, 50))\n",
        "    y1, y2 = np.array([1, 0]), np.array([0, 1])  # One-hot labels\n",
        "    # Different lambda values\n",
        "    lambdas_mix = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "    for ax, lam in zip(axes, lambdas_mix):\n",
        "        x_mixed = lam * x1 + (1 - lam) * x2\n",
        "        y_mixed = lam * y1 + (1 - lam) * y2\n",
        "        ax.plot(x_mixed, 'purple', linewidth=2)\n",
        "        ax.set_title(f'Î»={lam}\\ny=[{y_mixed[0]:.1f}, {y_mixed[1]:.1f}]')\n",
        "        ax.set_ylim(-1.5, 1.5)\n",
        "    plt.suptitle('Mixup: Interpolating Between Two Samples', fontsize=12, y=1.05)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_16"
      },
      "source": [
        "## Summary\n",
        "| Technique | Mechanism | When to Use |\n",
        "|-----------|-----------|-------------|\n",
        "| **L2 Regularization** | Penalize large weights | Always (default) |\n",
        "| **Dropout** | Random neuron dropping | Fully connected layers |\n",
        "| **BatchNorm** | Normalize activations | CNNs |\n",
        "| **Early Stopping** | Stop at best validation | Always |\n",
        "| **Data Augmentation** | Transform training data | Limited data |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 7.\n",
        "- **Dropout**: Srivastava et al. (2014). \"Dropout: A simple way to prevent overfitting.\"\n",
        "- **BatchNorm**: Ioffe & Szegedy (2015). \"Batch Normalization.\"\n",
        "## Connection to ML Refined Curriculum\n",
        "Regularization prevents overfitting for:\n",
        "- All supervised learning (Weeks 4-8)\n",
        "- Feature selection (Week 9)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}