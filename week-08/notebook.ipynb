{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-08/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 8: Linear Unsupervised Learning & PCA\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "üìñ **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 8**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand unsupervised learning concepts\n",
        "- Implement Principal Component Analysis (PCA)\n",
        "- Apply K-Means clustering\n",
        "- Introduction to matrix factorization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## Introduction (Section 8.1)\n",
        "**Unsupervised Learning**: Learn patterns from data without labels.\n",
        "### Key Problems\n",
        "| Problem | Goal |\n",
        "|---------|------|\n",
        "| **Dimensionality Reduction** | Compress data to fewer dimensions |\n",
        "| **Clustering** | Group similar data points |\n",
        "| **Anomaly Detection** | Find unusual points |\n",
        "| **Matrix Factorization** | Decompose data matrix |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_3"
      },
      "source": [
        "## Principal Component Analysis (Section 8.3)\n",
        "### Goal\n",
        "Find a low-dimensional representation that captures most of the variance.\n",
        "### The Linear Autoencoder View\n",
        "Encode data $x$ into low-dimensional $z$, then decode back:\n",
        "- **Encode**: $z = C^T x$ where $C \\in \\mathbb{R}^{n \\times k}$\n",
        "- **Decode**: $\\hat{x} = C z$\n",
        "### Optimization Problem\n",
        "$$\\min_C \\frac{1}{P} \\sum_{p=1}^{P} \\|x_p - CC^T x_p\\|^2$$\n",
        "Subject to: $C^T C = I$ (orthonormal columns)\n",
        "### Solution\n",
        "The optimal $C$ consists of the top $k$ eigenvectors of the covariance matrix:\n",
        "$$\\Sigma = \\frac{1}{P} \\sum_{p=1}^{P} (x_p - \\bar{x})(x_p - \\bar{x})^T$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_4"
      },
      "outputs": [],
      "source": [
        "# PCA on Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# Standardize\n",
        "X_centered = X - X.mean(axis=0)\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_centered)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Explained variance\n",
        "ax1 = axes[0]\n",
        "ax1.bar(range(1, 5), pca.explained_variance_ratio_, alpha=0.7, label='Individual')\n",
        "ax1.plot(range(1, 5), np.cumsum(pca.explained_variance_ratio_), 'ro-', label='Cumulative')\n",
        "ax1.axhline(0.95, color='g', linestyle='--', label='95% threshold')\n",
        "ax1.set_xlabel('Principal Component')\n",
        "ax1.set_ylabel('Explained Variance Ratio')\n",
        "ax1.set_title('Scree Plot (ML Refined, Section 8.3)')\n",
        "ax1.legend()\n",
        "ax1.set_xticks(range(1, 5))\n",
        "# 2D projection\n",
        "ax2 = axes[1]\n",
        "colors = ['blue', 'orange', 'green']\n",
        "for c in range(3):\n",
        "    mask = y == c\n",
        "    ax2.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[c], \n",
        "               label=iris.target_names[c], s=50, alpha=0.7)\n",
        "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
        "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
        "ax2.set_title('Iris Dataset: 2D PCA Projection')\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_5"
      },
      "source": [
        "## K-Means Clustering (Section 8.5)\n",
        "### Goal\n",
        "Partition data into $K$ clusters to minimize within-cluster variance.\n",
        "### Cost Function\n",
        "$$g(c, \\mu) = \\sum_{k=1}^{K} \\sum_{p: c_p = k} \\|x_p - \\mu_k\\|^2$$\n",
        "Where:\n",
        "- $c_p \\in \\{1, \\ldots, K\\}$: Cluster assignment for point $p$\n",
        "- $\\mu_k$: Centroid of cluster $k$\n",
        "### Algorithm\n",
        "```\n",
        "1. Initialize: Random centroids Œº‚ÇÅ, ..., Œº‚Çñ\n",
        "2. Repeat until convergence:\n",
        "   a. Assign: c‚Çö = argmin‚Çñ ||x‚Çö - Œº‚Çñ||¬≤\n",
        "   b. Update: Œº‚Çñ = mean of points in cluster k\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_6"
      },
      "outputs": [],
      "source": [
        "# K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# True labels\n",
        "ax1 = axes2[0]\n",
        "colors = ['blue', 'orange', 'green']\n",
        "for c in range(3):\n",
        "    mask = y == c\n",
        "    ax1.scatter(X[mask, 0], X[mask, 1], c=colors[c], s=50, alpha=0.7)\n",
        "ax1.set_xlabel('Sepal Length')\n",
        "ax1.set_ylabel('Sepal Width')\n",
        "ax1.set_title('True Labels')\n",
        "# K-Means clusters\n",
        "ax2 = axes2[1]\n",
        "for c in range(3):\n",
        "    mask = clusters == c\n",
        "    ax2.scatter(X[mask, 0], X[mask, 1], c=colors[c], s=50, alpha=0.7)\n",
        "ax2.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "ax2.set_xlabel('Sepal Length')\n",
        "ax2.set_ylabel('Sepal Width')\n",
        "ax2.set_title('K-Means Clusters')\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_7"
      },
      "source": [
        "## Recommender Systems (Section 8.4)\n",
        "### Matrix Factorization\n",
        "Approximate rating matrix $R \\in \\mathbb{R}^{M \\times N}$ as:\n",
        "$$R \\approx UV^T$$\n",
        "Where:\n",
        "- $U \\in \\mathbb{R}^{M \\times k}$: User features\n",
        "- $V \\in \\mathbb{R}^{N \\times k}$: Item features\n",
        "- $k$: Number of latent factors\n",
        "### Cost Function\n",
        "$$g(U, V) = \\sum_{(i,j) \\in \\Omega} (R_{ij} - u_i^T v_j)^2 + \\lambda(\\|U\\|^2 + \\|V\\|^2)$$\n",
        "Where $\\Omega$ is the set of observed ratings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_8"
      },
      "source": [
        "## Summary\n",
        "| Method | Goal | Key Idea |\n",
        "|--------|------|----------|\n",
        "| **PCA** | Dimension reduction | Find directions of maximum variance |\n",
        "| **K-Means** | Clustering | Minimize within-cluster variance |\n",
        "| **Matrix Factorization** | Recommendations | Decompose into latent factors |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 8.\n",
        "- **Supplementary**: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*, Chapter 12.\n",
        "## Next Week\n",
        "**Feature Engineering and Selection** (Chapter 9): Preparing data for ML models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}