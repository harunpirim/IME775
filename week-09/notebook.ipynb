{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-09/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 9: Feature Engineering and Selection\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 9**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand the importance of feature engineering\n",
        "- Apply feature scaling techniques\n",
        "- Handle missing values appropriately\n",
        "- Implement feature selection via boosting and regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## Introduction (Section 9.1)\n",
        "**Feature Engineering**: The process of creating, transforming, and selecting features to improve model performance.\n",
        "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering.\" â€” Andrew Ng\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_3"
      },
      "source": [
        "## Histogram Features (Section 9.2)\n",
        "For categorical or binned data:\n",
        "### One-Hot Encoding\n",
        "Convert categorical variable to binary vectors:\n",
        "- Category A â†’ [1, 0, 0]\n",
        "- Category B â†’ [0, 1, 0]\n",
        "- Category C â†’ [0, 0, 1]\n",
        "### Binning Continuous Features\n",
        "Convert continuous to discrete:\n",
        "- Age 0-18 â†’ \"child\"\n",
        "- Age 19-65 â†’ \"adult\"\n",
        "- Age 65+ â†’ \"senior\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_4"
      },
      "source": [
        "## Feature Scaling via Standard Normalization (Section 9.3)\n",
        "### Why Scale Features?\n",
        "- Many algorithms sensitive to feature scales\n",
        "- Gradient descent converges faster\n",
        "- Regularization works properly\n",
        "### Standard Normalization (Z-score)\n",
        "$$\\tilde{x}_j = \\frac{x_j - \\mu_j}{\\sigma_j}$$\n",
        "Result: Zero mean, unit variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Feature scaling example\n",
        "np.random.seed(42)\n",
        "n = 100\n",
        "# Features with very different scales\n",
        "X_raw = np.column_stack([\n",
        "    np.random.randn(n) * 100 + 500,  # Feature 1: large scale\n",
        "    np.random.randn(n) * 0.1 + 2      # Feature 2: small scale\n",
        "])\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_raw)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Before scaling\n",
        "ax1 = axes[0]\n",
        "ax1.scatter(X_raw[:, 0], X_raw[:, 1], alpha=0.7)\n",
        "ax1.set_xlabel('Feature 1 (large scale)')\n",
        "ax1.set_ylabel('Feature 2 (small scale)')\n",
        "ax1.set_title('Before Scaling')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "# After scaling\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.7)\n",
        "ax2.set_xlabel('Feature 1 (standardized)')\n",
        "ax2.set_ylabel('Feature 2 (standardized)')\n",
        "ax2.set_title('After Standard Normalization')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xlim(-4, 4)\n",
        "ax2.set_ylim(-4, 4)\n",
        "plt.tight_layout()\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_6"
      },
      "source": [
        "## Imputing Missing Values (Section 9.4)\n",
        "### Strategies\n",
        "| Strategy | Description | When to Use |\n",
        "|----------|-------------|-------------|\n",
        "| **Mean** | Replace with feature mean | Numerical, few missing |\n",
        "| **Median** | Replace with median | Numerical, outliers present |\n",
        "| **Mode** | Replace with most frequent | Categorical |\n",
        "| **Model-based** | Predict missing values | Complex patterns |\n",
        "### Implementation\n",
        "```python\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_7"
      },
      "source": [
        "## Feature Scaling via PCA-Sphering (Section 9.5)\n",
        "### Whitening/Sphering\n",
        "Transform data to have:\n",
        "- Zero mean\n",
        "- Identity covariance matrix\n",
        "$$\\tilde{X} = (X - \\mu) \\Sigma^{-1/2}$$\n",
        "### Effect\n",
        "- Removes correlations between features\n",
        "- Equalizes variances in all directions\n",
        "- Can improve some algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_8"
      },
      "source": [
        "## Feature Selection via Boosting (Section 9.6)\n",
        "### Forward Selection\n",
        "Iteratively add the most useful feature:\n",
        "```\n",
        "1. Start with empty feature set S\n",
        "2. Repeat:\n",
        "   a. For each feature j not in S:\n",
        "      - Evaluate model with S âˆª {j}\n",
        "   b. Add feature with best performance to S\n",
        "3. Until: stopping criterion met\n",
        "```\n",
        "### Backward Elimination\n",
        "Start with all features, iteratively remove least useful.\n",
        "### Advantages\n",
        "- Wrapper method: Uses actual model performance\n",
        "- Can capture feature interactions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_9"
      },
      "source": [
        "## Feature Selection via Regularization (Section 9.7)\n",
        "### L1 Regularization (Lasso)\n",
        "$$\\min_w \\frac{1}{P}\\|y - Xw\\|^2 + \\lambda \\|w\\|_1$$\n",
        "The L1 penalty encourages **sparsity** â€” many coefficients become exactly zero.\n",
        "### Why Sparsity?\n",
        "- Automatic feature selection\n",
        "- Interpretability\n",
        "- Reduced overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_10"
      },
      "outputs": [],
      "source": [
        "# Lasso for feature selection\n",
        "np.random.seed(42)\n",
        "n, p = 100, 20\n",
        "# Only 5 features are truly relevant\n",
        "X = np.random.randn(n, p)\n",
        "true_coef = np.zeros(p)\n",
        "true_coef[:5] = [3, -2, 1.5, -1, 0.5]\n",
        "y = X @ true_coef + 0.5 * np.random.randn(n)\n",
        "# Fit Lasso with different alpha values\n",
        "alphas = [0.001, 0.01, 0.1, 0.5]\n",
        "fig2, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "for ax, alpha in zip(axes.flat, alphas):\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X, y)\n",
        "    colors = ['green' if i < 5 else 'gray' for i in range(p)]\n",
        "    ax.bar(range(p), lasso.coef_, color=colors, alpha=0.7)\n",
        "    ax.axhline(0, color='black', linewidth=0.5)\n",
        "    ax.set_xlabel('Feature Index')\n",
        "    ax.set_ylabel('Coefficient')\n",
        "    n_nonzero = np.sum(lasso.coef_ != 0)\n",
        "    ax.set_title(f'Î± = {alpha}, Non-zero: {n_nonzero}')\n",
        "fig2.suptitle('Lasso Feature Selection (ML Refined, Section 9.7)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_11"
      },
      "source": [
        "## Summary\n",
        "| Technique | Purpose | Key Idea |\n",
        "|-----------|---------|----------|\n",
        "| **Scaling** | Normalize features | Same scale for all |\n",
        "| **Imputation** | Handle missing data | Replace with statistics |\n",
        "| **Boosting** | Feature selection | Greedy search |\n",
        "| **Regularization** | Feature selection | L1 sparsity |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 9.\n",
        "- **Supplementary**: Hastie, T. et al. (2009). *The Elements of Statistical Learning*, Chapter 3.\n",
        "## Next Week\n",
        "**Principles of Nonlinear Feature Engineering** (Chapter 10): Beyond linear models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}