{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-13/notebook-math-dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 13: Attention Mechanisms and Transformers\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 11\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand the attention mechanism mathematically\n",
        "- Master self-attention and multi-head attention\n",
        "- Learn the complete Transformer architecture\n",
        "- Connect to modern applications (BERT, GPT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## 13.1 The Attention Mechanism\n",
        "**Core Idea**: Given a query, compute relevance scores over keys to retrieve values.\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Implement and visualize attention\n",
        "def softmax(x, axis=-1):\n",
        "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_4"
      },
      "source": [
        "## 13.2 Multi-Head Attention\n",
        "Multiple attention heads can capture different relationship types:\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\n",
        "Each head: $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Multi-head attention visualization\n",
        "def multi_head_attention(X, n_heads=4):\n",
        "    \"\"\"Simplified multi-head attention.\"\"\"\n",
        "    d_k = d_model // n_heads\n",
        "    all_weights = []\n",
        "    for h in range(n_heads):\n",
        "        # Random projections (in practice, learned)\n",
        "        np.random.seed(42 + h)\n",
        "        W_q = np.random.randn(d_model, d_k) * 0.1\n",
        "        W_k = np.random.randn(d_model, d_k) * 0.1\n",
        "        W_v = np.random.randn(d_model, d_k) * 0.1\n",
        "        Q_h = X @ W_q\n",
        "        K_h = X @ W_k\n",
        "        V_h = X @ W_v\n",
        "        _, weights = attention(Q_h, K_h, V_h)\n",
        "        all_weights.append(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_6"
      },
      "source": [
        "## 13.3 Positional Encoding\n",
        "Self-attention is permutation-invariant - it doesn't know position!\n",
        "**Solution**: Add positional information:\n",
        "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Visualize positional encoding\n",
        "def positional_encoding(max_len, d_model):\n",
        "    pe = np.zeros((max_len, d_model))\n",
        "    position = np.arange(max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_8"
      },
      "source": [
        "## 13.4 The Transformer Architecture\n",
        "```\n",
        "Input â†’ Embedding + Position â†’ [Encoder Ã— N] â†’ [Decoder Ã— N] â†’ Output\n",
        "Encoder:                    Decoder:\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Multi-Head Attn â”‚        â”‚ Masked Self-Attnâ”‚\n",
        "â”‚   + Add & Norm  â”‚        â”‚   + Add & Norm  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                 â”‚        â”‚ Cross-Attention â”‚\n",
        "â”‚                 â”‚        â”‚   + Add & Norm  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ Feed-Forward    â”‚        â”‚ Feed-Forward    â”‚\n",
        "â”‚   + Add & Norm  â”‚        â”‚   + Add & Norm  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Visualize Transformer architecture\n",
        "fig4, ax4 = plt.subplots(figsize=(14, 10))\n",
        "# Encoder stack\n",
        "enc_x = 3\n",
        "for i in range(3):\n",
        "    y_base = 2 + i * 2.5\n",
        "    # Multi-head attention\n",
        "    ax4.add_patch(plt.Rectangle((enc_x - 1, y_base), 2, 0.8, \n",
        "                                 facecolor='lightblue', edgecolor='black'))\n",
        "    ax4.text(enc_x, y_base + 0.4, 'Multi-Head\\nSelf-Attention', \n",
        "            ha='center', va='center', fontsize=8)\n",
        "    # Add & Norm\n",
        "    ax4.add_patch(plt.Rectangle((enc_x - 1, y_base + 0.9), 2, 0.3,\n",
        "                                 facecolor='lightgray', edgecolor='black'))\n",
        "    ax4.text(enc_x, y_base + 1.05, 'Add & Norm', ha='center', va='center', fontsize=7)\n",
        "    # FFN\n",
        "    ax4.add_patch(plt.Rectangle((enc_x - 1, y_base + 1.3), 2, 0.6,\n",
        "                                 facecolor='lightyellow', edgecolor='black'))\n",
        "    ax4.text(enc_x, y_base + 1.6, 'Feed Forward', ha='center', va='center', fontsize=8)\n",
        "    # Add & Norm\n",
        "    ax4.add_patch(plt.Rectangle((enc_x - 1, y_base + 2.0), 2, 0.3,\n",
        "                                 facecolor='lightgray', edgecolor='black'))\n",
        "    ax4.text(enc_x, y_base + 2.15, 'Add & Norm', ha='center', va='center', fontsize=7)\n",
        "    # Residual connections\n",
        "    ax4.plot([enc_x - 1.3, enc_x - 1.3], [y_base + 0.4, y_base + 1.05], 'r-', lw=1)\n",
        "    ax4.plot([enc_x - 1.3, enc_x - 1], [y_base + 1.05, y_base + 1.05], 'r-', lw=1)\n",
        "# Decoder stack\n",
        "dec_x = 9\n",
        "for i in range(3):\n",
        "    y_base = 2 + i * 2.5\n",
        "    # Masked self-attention\n",
        "    ax4.add_patch(plt.Rectangle((dec_x - 1, y_base), 2, 0.6,\n",
        "                                 facecolor='lightgreen', edgecolor='black'))\n",
        "    ax4.text(dec_x, y_base + 0.3, 'Masked\\nSelf-Attn', ha='center', va='center', fontsize=7)\n",
        "    # Cross-attention\n",
        "    ax4.add_patch(plt.Rectangle((dec_x - 1, y_base + 0.7), 2, 0.6,\n",
        "                                 facecolor='lightcoral', edgecolor='black'))\n",
        "    ax4.text(dec_x, y_base + 1.0, 'Cross\\nAttention', ha='center', va='center', fontsize=7)\n",
        "    # FFN\n",
        "    ax4.add_patch(plt.Rectangle((dec_x - 1, y_base + 1.4), 2, 0.5,\n",
        "                                 facecolor='lightyellow', edgecolor='black'))\n",
        "    ax4.text(dec_x, y_base + 1.65, 'FFN', ha='center', va='center', fontsize=8)\n",
        "    # Arrow from encoder to cross-attention\n",
        "    if i == 2:\n",
        "        ax4.annotate('', xy=(dec_x - 1, y_base + 1.0), xytext=(enc_x + 1, y_base + 1.0),\n",
        "                    arrowprops=dict(arrowstyle='->', color='purple', lw=2))\n",
        "# Input\n",
        "ax4.add_patch(plt.Rectangle((enc_x - 1, 0.5), 2, 0.8, facecolor='white', edgecolor='black'))\n",
        "ax4.text(enc_x, 0.9, 'Input\\nEmbedding\\n+ Position', ha='center', va='center', fontsize=8)\n",
        "ax4.add_patch(plt.Rectangle((dec_x - 1, 0.5), 2, 0.8, facecolor='white', edgecolor='black'))\n",
        "ax4.text(dec_x, 0.9, 'Output\\nEmbedding\\n+ Position', ha='center', va='center', fontsize=8)\n",
        "# Output\n",
        "ax4.add_patch(plt.Rectangle((dec_x - 1, 10), 2, 0.8, facecolor='white', edgecolor='black'))\n",
        "ax4.text(dec_x, 10.4, 'Linear +\\nSoftmax', ha='center', va='center', fontsize=8)\n",
        "# Labels\n",
        "ax4.text(enc_x, 11, 'ENCODER', ha='center', fontsize=14, fontweight='bold', color='blue')\n",
        "ax4.text(dec_x, 11, 'DECODER', ha='center', fontsize=14, fontweight='bold', color='green')\n",
        "# Arrows\n",
        "ax4.annotate('', xy=(enc_x, 2), xytext=(enc_x, 1.3), arrowprops=dict(arrowstyle='->', color='black'))\n",
        "ax4.annotate('', xy=(dec_x, 2), xytext=(dec_x, 1.3), arrowprops=dict(arrowstyle='->', color='black'))\n",
        "ax4.annotate('', xy=(dec_x, 10), xytext=(dec_x, 9.5), arrowprops=dict(arrowstyle='->', color='black'))\n",
        "ax4.set_xlim(0, 12)\n",
        "ax4.set_ylim(0, 12)\n",
        "ax4.axis('off')\n",
        "ax4.set_title('Transformer Architecture (Simplified)', fontsize=14, y=0.98)\n",
        "fig4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_10"
      },
      "source": [
        "## 13.5 Masked Attention (Decoder)\n",
        "Prevent attending to future positions during training:\n",
        "$$\\text{Mask}_{ij} = \\begin{cases} 0 & i \\geq j \\\\ -\\infty & i < j \\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_11"
      },
      "outputs": [],
      "source": [
        "# Demonstrate causal masking\n",
        "def masked_attention(Q, K, V, mask=None):\n",
        "    d_k = K.shape[-1]\n",
        "    scores = Q @ K.T / np.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores + mask\n",
        "    weights = softmax(scores, axis=-1)\n",
        "    output = weights @ V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_12"
      },
      "source": [
        "## 13.6 Modern Transformer Variants\n",
        "| Model | Type | Pre-training Task | Use Case |\n",
        "|-------|------|-------------------|----------|\n",
        "| **BERT** | Encoder | Masked LM + Next Sentence | Classification, NER |\n",
        "| **GPT** | Decoder | Next Token Prediction | Generation, Chat |\n",
        "| **T5** | Encoder-Decoder | Text-to-Text | Translation, Summarization |\n",
        "| **ViT** | Encoder | Image Classification | Vision tasks |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_13"
      },
      "outputs": [],
      "source": [
        "# Compare architectures\n",
        "fig6, ax6 = plt.subplots(figsize=(12, 6))\n",
        "models = ['BERT-base', 'BERT-large', 'GPT-2', 'GPT-3', 'T5-base', 'ViT-base']\n",
        "params = [110, 340, 1500, 175000, 220, 86]  # In millions\n",
        "colors = ['blue', 'blue', 'green', 'green', 'orange', 'purple']\n",
        "bars = ax6.bar(models, params, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax6.set_ylabel('Parameters (Millions)')\n",
        "ax6.set_title('Transformer Model Sizes')\n",
        "ax6.set_yscale('log')\n",
        "# Add parameter counts\n",
        "for bar, param in zip(bars, params):\n",
        "    height = bar.get_height()\n",
        "    ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{param:,}M', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "# Legend\n",
        "ax6.text(0.02, 0.95, 'â–  BERT (Encoder)', color='blue', transform=ax6.transAxes, fontsize=10)\n",
        "ax6.text(0.02, 0.90, 'â–  GPT (Decoder)', color='green', transform=ax6.transAxes, fontsize=10)\n",
        "ax6.text(0.02, 0.85, 'â–  T5 (Enc-Dec)', color='orange', transform=ax6.transAxes, fontsize=10)\n",
        "ax6.text(0.02, 0.80, 'â–  ViT (Vision)', color='purple', transform=ax6.transAxes, fontsize=10)\n",
        "ax6.grid(True, alpha=0.3, axis='y')\n",
        "fig6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_14"
      },
      "source": [
        "## Summary\n",
        "| Component | Purpose |\n",
        "|-----------|---------|\n",
        "| **Attention** | Compute relevance-weighted combinations |\n",
        "| **Self-Attention** | Relate positions within sequence |\n",
        "| **Multi-Head** | Capture multiple relationship types |\n",
        "| **Positional Encoding** | Add position information |\n",
        "| **Masking** | Prevent attending to future |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 11.\n",
        "- **Transformer**: Vaswani et al. (2017). \"Attention Is All You Need.\"\n",
        "- **BERT**: Devlin et al. (2019)\n",
        "- **GPT-3**: Brown et al. (2020)\n",
        "## Connection to ML Refined Curriculum\n",
        "Transformers are the current state-of-the-art for:\n",
        "- Sequence modeling (beyond RNNs from Week 11)\n",
        "- Automatic feature learning\n",
        "- Foundation for modern AI systems\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook-math-dl",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}