{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-13/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 13: Tree-Based Learners & Advanced Topics\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 14**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand decision tree construction\n",
        "- Apply gradient boosting for improved performance\n",
        "- Implement random forests\n",
        "- Compare tree-based methods with other approaches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## From Stumps to Deep Trees (Section 14.2)\n",
        "### Decision Stump\n",
        "A single split:\n",
        "$$f(x) = \\begin{cases} c_1 & \\text{if } x_j \\leq t \\\\ c_2 & \\text{if } x_j > t \\end{cases}$$\n",
        "### Deep Trees\n",
        "Recursively partition the feature space with more splits.\n",
        "### Tree Building Algorithm\n",
        "```\n",
        "1. If stopping criterion met, return leaf\n",
        "2. Find best split (feature j, threshold t)\n",
        "3. Split data into left/right\n",
        "4. Recursively build left and right subtrees\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Decision tree visualization\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, \n",
        "                            n_informative=2, n_clusters_per_class=1, random_state=42)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "# Decision boundary\n",
        "ax1 = axes[0]\n",
        "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree.fit(X, y)\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 200),\n",
        "                     np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 200))\n",
        "Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "ax1.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "ax1.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=50, edgecolors='black')\n",
        "ax1.scatter(X[y==1, 0], X[y==1, 1], c='red', s=50, edgecolors='black')\n",
        "ax1.set_xlabel('Feature 1')\n",
        "ax1.set_ylabel('Feature 2')\n",
        "ax1.set_title('Decision Tree Boundaries (max_depth=3)')\n",
        "# Tree structure\n",
        "ax2 = axes[1]\n",
        "plot_tree(tree, ax=ax2, feature_names=['$x_1$', '$x_2$'], \n",
        "          class_names=['0', '1'], filled=True, rounded=True)\n",
        "ax2.set_title('Tree Structure')\n",
        "fig.suptitle('Decision Tree Classifier (ML Refined, Section 14.2)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_4"
      },
      "source": [
        "## Regression Trees (Section 14.3)\n",
        "### Split Criterion\n",
        "Minimize squared error:\n",
        "$$\\sum_{x_i \\in R_L} (y_i - c_L)^2 + \\sum_{x_i \\in R_R} (y_i - c_R)^2$$\n",
        "Where $c_L = \\text{mean}(y_i : x_i \\in R_L)$ and $c_R = \\text{mean}(y_i : x_i \\in R_R)$.\n",
        "### Prediction\n",
        "For a new point, traverse tree and return leaf value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Regression tree\n",
        "np.random.seed(42)\n",
        "X_reg = np.sort(np.random.uniform(0, 10, 100)).reshape(-1, 1)\n",
        "y_reg = np.sin(X_reg.ravel()) + 0.3 * np.random.randn(100)\n",
        "fig2, axes2 = plt.subplots(1, 3, figsize=(15, 4))\n",
        "depths = [1, 3, 10]\n",
        "for ax, depth in zip(axes2, depths):\n",
        "    tree_reg = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
        "    tree_reg.fit(X_reg, y_reg)\n",
        "    X_test = np.linspace(0, 10, 200).reshape(-1, 1)\n",
        "    y_pred = tree_reg.predict(X_test)\n",
        "    ax.scatter(X_reg, y_reg, alpha=0.7, s=20)\n",
        "    ax.plot(X_test, y_pred, 'r-', linewidth=2)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(f'max_depth = {depth}')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "fig2.suptitle('Regression Trees with Different Depths', fontsize=14)\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_6"
      },
      "source": [
        "## Classification Trees (Section 14.4)\n",
        "### Split Criteria\n",
        "| Criterion | Formula |\n",
        "|-----------|---------|\n",
        "| **Gini Impurity** | $\\sum_c p_c(1 - p_c)$ |\n",
        "| **Entropy** | $-\\sum_c p_c \\log p_c$ |\n",
        "| **Misclassification** | $1 - \\max_c p_c$ |\n",
        "Where $p_c$ is the proportion of class $c$ in the node.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_7"
      },
      "source": [
        "## Gradient Boosting (Section 14.5)\n",
        "### The Idea\n",
        "Build trees sequentially, each correcting errors of the ensemble:\n",
        "$$f_m(x) = f_{m-1}(x) + \\gamma h_m(x)$$\n",
        "Where $h_m$ is fit to the **residuals** of $f_{m-1}$.\n",
        "### Algorithm\n",
        "```\n",
        "1. Initialize fâ‚€(x) = mean(y)\n",
        "2. For m = 1 to M:\n",
        "   a. Compute residuals: ráµ¢ = yáµ¢ - f_{m-1}(xáµ¢)\n",
        "   b. Fit tree hâ‚˜ to residuals\n",
        "   c. Update: fâ‚˜ = f_{m-1} + Î³Â·hâ‚˜\n",
        "3. Return fâ‚˜\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_8"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting learning curve\n",
        "from sklearn.model_selection import learning_curve\n",
        "gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    gb, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5\n",
        ")\n",
        "fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
        "ax3.plot(train_sizes, train_scores.mean(axis=1), 'b-o', label='Training Score')\n",
        "ax3.plot(train_sizes, val_scores.mean(axis=1), 'r-o', label='Validation Score')\n",
        "ax3.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
        "                 train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.1)\n",
        "ax3.fill_between(train_sizes, val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
        "                 val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.1)\n",
        "ax3.set_xlabel('Training Set Size')\n",
        "ax3.set_ylabel('Score')\n",
        "ax3.set_title('Gradient Boosting Learning Curve (ML Refined, Section 14.5)')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_9"
      },
      "source": [
        "## Random Forests (Section 14.6)\n",
        "### The Idea\n",
        "Combine many trees trained on **random subsets** of data and features.\n",
        "### Algorithm\n",
        "```\n",
        "1. For b = 1 to B:\n",
        "   a. Draw bootstrap sample\n",
        "   b. Grow tree with random feature subset at each split\n",
        "2. Average predictions (regression) or vote (classification)\n",
        "```\n",
        "### Key Hyperparameters\n",
        "| Parameter | Effect |\n",
        "|-----------|--------|\n",
        "| `n_estimators` | More trees = less variance |\n",
        "| `max_features` | Controls correlation between trees |\n",
        "| `max_depth` | Controls individual tree complexity |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_10"
      },
      "outputs": [],
      "source": [
        "GradientBoostingClassifier,\n",
        "RandomForestClassifier,\n",
        "X,\n",
        "make_classification,\n",
        "np,\n",
        "plt,\n",
        "y,\n",
        "):\n",
        "# Compare methods\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "X_compare, y_compare = make_classification(n_samples=500, n_features=20, \n",
        "                                            n_informative=10, random_state=42)\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'SVM (RBF)': SVC(kernel='rbf'),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X_compare, y_compare, cv=5)\n",
        "    results[name] = (scores.mean(), scores.std())\n",
        "fig4, ax4 = plt.subplots(figsize=(10, 6))\n",
        "names = list(results.keys())\n",
        "means = [r[0] for r in results.values()]\n",
        "stds = [r[1] for r in results.values()]\n",
        "bars = ax4.bar(names, means, yerr=stds, capsize=5, alpha=0.7, \n",
        "               color=['steelblue', 'coral', 'green', 'purple'])\n",
        "ax4.set_ylabel('Accuracy')\n",
        "ax4.set_title('Model Comparison (5-Fold CV)')\n",
        "ax4.set_ylim(0.7, 1.0)\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "for bar, mean in zip(bars, means):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "             f'{mean:.3f}', ha='center', fontsize=10)\n",
        "fig4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_11"
      },
      "source": [
        "## Summary\n",
        "| Method | Pros | Cons |\n",
        "|--------|------|------|\n",
        "| **Decision Trees** | Interpretable, fast | Overfits easily |\n",
        "| **Random Forests** | Robust, parallel | Many trees needed |\n",
        "| **Gradient Boosting** | Often best accuracy | Sequential, slower |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 14.\n",
        "- **Supplementary**: Hastie, T. et al. (2009). *The Elements of Statistical Learning*, Chapters 9-10.\n",
        "## Course Conclusion\n",
        "This completes the theoretical foundations of the course. Weeks 14-15 will focus on student presentations.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}