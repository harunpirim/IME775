{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 5: Linear Regression\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 5**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Formulate and solve least squares linear regression\n",
        "- Understand least absolute deviations regression\n",
        "- Apply regression quality metrics\n",
        "- Implement weighted regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## Introduction (Section 5.1)\n",
        "**Linear Regression**: Learn a linear relationship between input features and a continuous output.\n",
        "### The Model\n",
        "$$f(x) = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n = w^T \\tilde{x}$$\n",
        "Where $\\tilde{x} = [1, x_1, \\ldots, x_n]^T$ includes the bias term.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_3"
      },
      "source": [
        "## Least Squares Linear Regression (Section 5.2)\n",
        "### The Cost Function\n",
        "$$g(w) = \\frac{1}{P} \\sum_{p=1}^{P} (y_p - f(x_p))^2 = \\frac{1}{P} \\|y - Xw\\|^2$$\n",
        "Where:\n",
        "- $P$: Number of training points\n",
        "- $y$: Vector of outputs\n",
        "- $X$: Design matrix (rows are $\\tilde{x}_p^T$)\n",
        "- $w$: Weight vector\n",
        "### The Normal Equations\n",
        "Setting $\\nabla g = 0$:\n",
        "$$X^T X w = X^T y$$\n",
        "Solution:\n",
        "$$w^* = (X^T X)^{-1} X^T y$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_4"
      },
      "outputs": [],
      "source": [
        "# Generate data\n",
        "np.random.seed(42)\n",
        "n = 50\n",
        "x = np.linspace(0, 10, n)\n",
        "y_true = 2.5 * x + 3\n",
        "y = y_true + np.random.randn(n) * 2\n",
        "# Build design matrix\n",
        "X = np.column_stack([np.ones(n), x])\n",
        "# Solve normal equations\n",
        "w = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "y_pred = X @ w\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(x, y, alpha=0.7, s=50, label='Data')\n",
        "ax.plot(x, y_pred, 'r-', linewidth=2, \n",
        "        label=f'Fit: y = {w[1]:.2f}x + {w[0]:.2f}')\n",
        "ax.plot(x, y_true, 'g--', linewidth=1, alpha=0.7, label='True: y = 2.5x + 3')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_title('Least Squares Linear Regression (ML Refined, Section 5.2)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_5"
      },
      "source": [
        "## Gradient Descent for Linear Regression\n",
        "The gradient of the least squares cost:\n",
        "$$\\nabla g(w) = \\frac{2}{P} X^T(Xw - y)$$\n",
        "### Gradient Descent Update\n",
        "$$w^{(k+1)} = w^{(k)} - \\alpha \\cdot \\frac{2}{P} X^T(Xw^{(k)} - y)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_6"
      },
      "outputs": [],
      "source": [
        "# Gradient descent for linear regression\n",
        "np.random.seed(42)\n",
        "n_gd = 100\n",
        "x_gd = np.random.randn(n_gd, 2)\n",
        "true_w = np.array([3, -2, 1])  # bias, w1, w2\n",
        "X_gd = np.column_stack([np.ones(n_gd), x_gd])\n",
        "y_gd = X_gd @ true_w + 0.5 * np.random.randn(n_gd)\n",
        "# Gradient descent\n",
        "def gradient(w, X, y):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_7"
      },
      "source": [
        "## Least Absolute Deviations (Section 5.3)\n",
        "An alternative to least squares:\n",
        "$$g(w) = \\frac{1}{P} \\sum_{p=1}^{P} |y_p - f(x_p)|$$\n",
        "### Comparison\n",
        "| Aspect | Least Squares | Least Absolute Deviations |\n",
        "|--------|--------------|--------------------------|\n",
        "| Cost | $(y - \\hat{y})^2$ | $|y - \\hat{y}|$ |\n",
        "| Outlier sensitivity | High | Low |\n",
        "| Closed-form | Yes | No |\n",
        "| Optimization | Easy | Requires iterative methods |\n",
        "### When to Use LAD\n",
        "- Data contains outliers\n",
        "- Heavy-tailed error distribution\n",
        "- Robust estimation required\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## Regression Quality Metrics (Section 5.4)\n",
        "### Mean Squared Error (MSE)\n",
        "$$\\text{MSE} = \\frac{1}{P} \\sum_{p=1}^{P} (y_p - \\hat{y}_p)^2$$\n",
        "### Root Mean Squared Error (RMSE)\n",
        "$$\\text{RMSE} = \\sqrt{\\text{MSE}}$$\n",
        "### Mean Absolute Error (MAE)\n",
        "$$\\text{MAE} = \\frac{1}{P} \\sum_{p=1}^{P} |y_p - \\hat{y}_p|$$\n",
        "### Coefficient of Determination ($R^2$)\n",
        "$$R^2 = 1 - \\frac{\\sum_p (y_p - \\hat{y}_p)^2}{\\sum_p (y_p - \\bar{y})^2}$$\n",
        "- $R^2 = 1$: Perfect fit\n",
        "- $R^2 = 0$: Model is no better than predicting the mean\n",
        "- $R^2 < 0$: Model is worse than predicting the mean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "y_pred_metrics = X @ w\n",
        "residuals = y - y_pred_metrics\n",
        "mse = np.mean(residuals**2)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = np.mean(np.abs(residuals))\n",
        "ss_res = np.sum(residuals**2)\n",
        "ss_tot = np.sum((y - np.mean(y))**2)\n",
        "r2 = 1 - ss_res / ss_tot\n",
        "print(\"Regression Quality Metrics:\")\n",
        "print(f\"  MSE:  {mse:.4f}\")\n",
        "print(f\"  RMSE: {rmse:.4f}\")\n",
        "print(f\"  MAE:  {mae:.4f}\")\n",
        "print(f\"  RÂ²:   {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_10"
      },
      "source": [
        "## Weighted Regression (Section 5.5)\n",
        "When data points have different importance or reliability:\n",
        "$$g(w) = \\frac{1}{P} \\sum_{p=1}^{P} \\beta_p (y_p - f(x_p))^2$$\n",
        "Where $\\beta_p > 0$ is the weight for point $p$.\n",
        "### Normal Equations for Weighted Regression\n",
        "$$X^T B X w = X^T B y$$\n",
        "Where $B = \\text{diag}(\\beta_1, \\ldots, \\beta_P)$.\n",
        "### Applications\n",
        "- Heteroscedastic data (varying error variance)\n",
        "- Importance weighting\n",
        "- Sample weighting for imbalanced data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "## Summary\n",
        "| Concept | Key Points |\n",
        "|---------|------------|\n",
        "| **Least Squares** | Minimize squared errors, closed-form solution |\n",
        "| **LAD** | Minimize absolute errors, robust to outliers |\n",
        "| **Metrics** | MSE, RMSE, MAE, $R^2$ |\n",
        "| **Weighted** | Different importance for different points |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 5.\n",
        "- **Supplementary**: James, G. et al. (2023). *An Introduction to Statistical Learning*, Chapter 3.\n",
        "## Next Week\n",
        "**Linear Two-Class Classification** (Chapter 6): Logistic regression, perceptron, and SVM.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}