{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 5: Backpropagation - The Engine of Deep Learning\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 6\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand backpropagation as reverse-mode automatic differentiation\n",
        "- Derive gradients for common layers\n",
        "- Implement backpropagation from scratch\n",
        "- Verify gradients numerically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## 5.1 Why Backpropagation?\n",
        "**Problem**: Computing gradients for millions of parameters\n",
        "**Naive approach**: Numerical gradients require $2n$ forward passes for $n$ parameters\n",
        "**Solution**: Backpropagation computes ALL gradients in ONE backward pass!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Demonstrate computational cost\n",
        "def numerical_gradient_cost(n_params, n_samples):\n",
        "    forward_passes = 2 * n_params  # Central difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_4"
      },
      "source": [
        "## 5.2 Computational Graphs\n",
        "Neural network computation can be represented as a **directed acyclic graph (DAG)**:\n",
        "- Nodes: Operations or variables\n",
        "- Edges: Data flow\n",
        "**Forward pass**: Compute outputs (left â†’ right)\n",
        "**Backward pass**: Compute gradients (right â†’ left)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_5"
      },
      "source": [
        "## 5.3 The Chain Rule in Action\n",
        "For $L = f(g(h(x)))$:\n",
        "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$$\n",
        "Each node:\n",
        "1. Computes **local gradient** (derivative of its operation)\n",
        "2. Multiplies by **upstream gradient** (from output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_6"
      },
      "outputs": [],
      "source": [
        "# Visualize backpropagation on a simple network\n",
        "# y = sigmoid(w2 * relu(w1 * x + b1) + b2)\n",
        "class ComputeNode:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.output = None\n",
        "        self.grad = None\n",
        "    def __repr__(self):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Print the gradients\n",
        "print(\"Gradients computed via backpropagation:\")\n",
        "print(\"-\" * 40)\n",
        "for name, grad in grads.items():\n",
        "    print(f\"âˆ‚L/âˆ‚{name} = {grad:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## 5.4 Layer-by-Layer Gradients\n",
        "### Linear Layer: $z = Wx + b$\n",
        "- $\\frac{\\partial L}{\\partial W} = x^T \\cdot \\frac{\\partial L}{\\partial z}$\n",
        "- $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z}$\n",
        "- $\\frac{\\partial L}{\\partial x} = W^T \\cdot \\frac{\\partial L}{\\partial z}$\n",
        "### ReLU: $h = \\max(0, z)$\n",
        "- $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial h} \\cdot \\mathbb{1}[z > 0]$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Full backprop implementation\n",
        "class Layer:\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "    def backward(self, grad_output):\n",
        "        raise NotImplementedError\n",
        "class Linear(Layer):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        # He initialization\n",
        "        self.W = np.random.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
        "        self.b = np.zeros(out_features)\n",
        "        self.grad_W = None\n",
        "        self.grad_b = None\n",
        "    def forward(self, x):\n",
        "        self.x = x  # Cache for backward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_10"
      },
      "outputs": [],
      "source": [
        "# Test the implementation\n",
        "class SimpleNet:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "            if i < len(layer_sizes) - 2:  # No activation after last layer\n",
        "                self.layers.append(ReLU())\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "## 5.5 Gradient Checking\n",
        "**Always verify analytical gradients numerically!**\n",
        "$$\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta - \\epsilon)}{2\\epsilon}$$\n",
        "Relative error should be $< 10^{-5}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_12"
      },
      "outputs": [],
      "source": [
        "# Gradient checking\n",
        "def gradient_check(layer, x, upstream_grad, eps=1e-5):\n",
        "    \"\"\"Check if analytical gradient matches numerical gradient.\"\"\"\n",
        "    # Analytical gradient\n",
        "    output = layer.forward(x)\n",
        "    layer.backward(upstream_grad)\n",
        "    results = []\n",
        "    if hasattr(layer, 'W'):\n",
        "        # Check W gradient\n",
        "        analytical_W = layer.grad_W.copy()\n",
        "        numerical_W = np.zeros_like(layer.W)\n",
        "        for i in range(layer.W.shape[0]):\n",
        "            for j in range(layer.W.shape[1]):\n",
        "                original = layer.W[i, j]\n",
        "                layer.W[i, j] = original + eps\n",
        "                out_plus = layer.forward(x)\n",
        "                loss_plus = np.sum(out_plus * upstream_grad)\n",
        "                layer.W[i, j] = original - eps\n",
        "                out_minus = layer.forward(x)\n",
        "                loss_minus = np.sum(out_minus * upstream_grad)\n",
        "                layer.W[i, j] = original\n",
        "                numerical_W[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
        "        rel_error_W = np.linalg.norm(analytical_W - numerical_W) / (\n",
        "            np.linalg.norm(analytical_W) + np.linalg.norm(numerical_W) + 1e-8)\n",
        "        results.append(('W', rel_error_W))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_13"
      },
      "source": [
        "## 5.6 Vanishing and Exploding Gradients\n",
        "In deep networks, gradients can become very small (vanishing) or very large (exploding):\n",
        "$$\\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial h^{(L)}} \\prod_{l=1}^{L} \\frac{\\partial h^{(l)}}{\\partial h^{(l-1)}}$$\n",
        "**Solutions**: ReLU, proper initialization, skip connections, normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_14"
      },
      "outputs": [],
      "source": [
        "# Demonstrate vanishing gradients with sigmoid vs ReLU\n",
        "def simulate_gradient_flow(n_layers, activation='sigmoid'):\n",
        "    np.random.seed(42)\n",
        "    gradients = [1.0]  # Start with upstream gradient of 1\n",
        "    for l in range(n_layers):\n",
        "        # Random weights\n",
        "        W = np.random.randn(100, 100) * 0.1\n",
        "        # Random activations (for derivative computation)\n",
        "        if activation == 'sigmoid':\n",
        "            h = 1 / (1 + np.exp(-np.random.randn(100)))\n",
        "            derivative = np.mean(h * (1 - h))  # sigmoid derivative\n",
        "        else:  # relu\n",
        "            z = np.random.randn(100)\n",
        "            derivative = np.mean(z > 0)  # relu derivative\n",
        "        # Gradient magnitude after this layer\n",
        "        grad_magnitude = gradients[-1] * np.abs(W).mean() * derivative\n",
        "        gradients.append(grad_magnitude)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_15"
      },
      "source": [
        "## Summary\n",
        "| Concept | Key Point |\n",
        "|---------|-----------|\n",
        "| **Backpropagation** | Compute all gradients in one backward pass |\n",
        "| **Chain Rule** | Multiply local gradient by upstream gradient |\n",
        "| **Gradient Checking** | Verify analytical vs numerical gradients |\n",
        "| **Vanishing Gradients** | Use ReLU, proper init, skip connections |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 6.\n",
        "- **Classic**: Rumelhart, Hinton & Williams (1986). \"Learning representations by back-propagating errors.\"\n",
        "## Connection to ML Refined Curriculum\n",
        "Backpropagation enables training for:\n",
        "- All gradient descent methods (Weeks 2-3)\n",
        "- Any supervised learning model (Weeks 4-8)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}