{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-10/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 10: Principles of Nonlinear Feature Engineering\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 10**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand limitations of linear models\n",
        "- Apply polynomial feature transformation\n",
        "- Implement nonlinear regression and classification\n",
        "- Connect feature engineering to model capacity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## Introduction (Section 10.1)\n",
        "### The Limitation of Linear Models\n",
        "Linear models: $f(x) = w^T \\tilde{x}$\n",
        "Can only represent **linear** relationships.\n",
        "### The Solution: Feature Engineering\n",
        "Transform input features nonlinearly, then apply linear model:\n",
        "$$f(x) = w^T \\phi(x)$$\n",
        "Where $\\phi(x)$ is a nonlinear feature transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Nonlinear data that linear model can't fit\n",
        "np.random.seed(42)\n",
        "x = np.linspace(-3, 3, 100)\n",
        "y = x**2 + 0.5 * np.random.randn(100)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Linear fit\n",
        "ax1 = axes[0]\n",
        "coeffs_linear = np.polyfit(x, y, 1)\n",
        "y_linear = np.polyval(coeffs_linear, x)\n",
        "ax1.scatter(x, y, alpha=0.7, s=30)\n",
        "ax1.plot(x, y_linear, 'r-', linewidth=2, label='Linear fit')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title('Linear Model: Poor Fit')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "# Polynomial fit\n",
        "ax2 = axes[1]\n",
        "coeffs_poly = np.polyfit(x, y, 2)\n",
        "y_poly = np.polyval(coeffs_poly, x)\n",
        "ax2.scatter(x, y, alpha=0.7, s=30)\n",
        "ax2.plot(x, y_poly, 'g-', linewidth=2, label='Polynomial (degree 2)')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "ax2.set_title('Polynomial Features: Good Fit')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_4"
      },
      "source": [
        "## Nonlinear Regression (Section 10.2)\n",
        "### Polynomial Features\n",
        "Transform $x$ into $\\phi(x) = [1, x, x^2, \\ldots, x^D]$\n",
        "The model becomes:\n",
        "$$f(x) = w_0 + w_1 x + w_2 x^2 + \\cdots + w_D x^D$$\n",
        "**Still linear in parameters** $w$, so we can use least squares!\n",
        "### Multi-Dimensional Polynomials\n",
        "For $x \\in \\mathbb{R}^n$, include all monomials up to degree $D$:\n",
        "- Degree 1: $x_1, x_2$\n",
        "- Degree 2: $x_1^2, x_1 x_2, x_2^2$\n",
        "- Etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Polynomial regression example\n",
        "np.random.seed(42)\n",
        "n = 50\n",
        "X_train = np.sort(np.random.uniform(-3, 3, n)).reshape(-1, 1)\n",
        "y_train = np.sin(X_train.ravel()) + 0.3 * np.random.randn(n)\n",
        "X_test = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
        "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "ax2.scatter(X_train, y_train, alpha=0.7, label='Training data')\n",
        "for degree in [1, 3, 9]:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly_train = poly.fit_transform(X_train)\n",
        "    X_poly_test = poly.transform(X_test)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly_train, y_train)\n",
        "    y_pred = model.predict(X_poly_test)\n",
        "    ax2.plot(X_test, y_pred, linewidth=2, label=f'Degree {degree}')\n",
        "ax2.plot(X_test, np.sin(X_test), 'k--', linewidth=1, alpha=0.5, label='True function')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "ax2.set_title('Polynomial Regression with Different Degrees (ML Refined, Section 10.2)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(-2, 2)\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_6"
      },
      "source": [
        "## Nonlinear Two-Class Classification (Section 10.4)\n",
        "### The Idea\n",
        "Apply same feature transformation, then linear classifier:\n",
        "$$f(x) = \\text{sign}(w^T \\phi(x))$$\n",
        "### Example: XOR Problem\n",
        "XOR is not linearly separable in $\\mathbb{R}^2$, but is linearly separable with polynomial features!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# XOR problem\n",
        "np.random.seed(42)\n",
        "n_per_class = 50\n",
        "# Generate XOR data\n",
        "X_xor = np.vstack([\n",
        "    np.random.randn(n_per_class, 2) + [1, 1],\n",
        "    np.random.randn(n_per_class, 2) + [-1, -1],\n",
        "    np.random.randn(n_per_class, 2) + [1, -1],\n",
        "    np.random.randn(n_per_class, 2) + [-1, 1]\n",
        "])\n",
        "y_xor = np.array([0]*2*n_per_class + [1]*2*n_per_class)\n",
        "fig3, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Original space\n",
        "ax1 = axes[0]\n",
        "ax1.scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=30, alpha=0.7)\n",
        "ax1.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=30, alpha=0.7)\n",
        "ax1.set_xlabel('$x_1$')\n",
        "ax1.set_ylabel('$x_2$')\n",
        "ax1.set_title('XOR Problem: Not Linearly Separable')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "# Add feature x1*x2\n",
        "X_xor_poly = np.column_stack([X_xor, X_xor[:, 0] * X_xor[:, 1]])\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(X_xor_poly[y_xor==0, 0], X_xor_poly[y_xor==0, 2], c='blue', s=30, alpha=0.7)\n",
        "ax2.scatter(X_xor_poly[y_xor==1, 0], X_xor_poly[y_xor==1, 2], c='red', s=30, alpha=0.7)\n",
        "ax2.axhline(0, color='black', linewidth=2, linestyle='--')\n",
        "ax2.set_xlabel('$x_1$')\n",
        "ax2.set_ylabel('$x_1 \\\\cdot x_2$')\n",
        "ax2.set_title('With Feature $x_1 x_2$: Linearly Separable!')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_8"
      },
      "source": [
        "## The Bias-Variance Trade-off\n",
        "### Model Complexity\n",
        "| Aspect | Low Complexity | High Complexity |\n",
        "|--------|----------------|-----------------|\n",
        "| Features | Few | Many |\n",
        "| Degree | Low | High |\n",
        "| Bias | High | Low |\n",
        "| Variance | Low | High |\n",
        "| Training error | High | Low |\n",
        "| Test error | Can be high | Can be high |\n",
        "### The Goal\n",
        "Find the **sweet spot** where total error (biasÂ² + variance) is minimized.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_9"
      },
      "source": [
        "## Summary\n",
        "| Concept | Key Idea |\n",
        "|---------|----------|\n",
        "| **Nonlinear features** | Transform inputs, keep linear model |\n",
        "| **Polynomial features** | $\\phi(x) = [1, x, x^2, \\ldots]$ |\n",
        "| **Model capacity** | Controlled by feature complexity |\n",
        "| **Trade-off** | Balance bias and variance |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 10.\n",
        "- **Supplementary**: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*, Chapter 3.\n",
        "## Next Week\n",
        "**Principles of Feature Learning & Cross-Validation** (Chapter 11): Automatic feature learning and model selection.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}