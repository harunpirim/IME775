{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-04/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 4: Second-Order Optimization - Newton's Method\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 4**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand second-order optimality conditions\n",
        "- Derive and implement Newton's method\n",
        "- Compare Newton's method with gradient descent\n",
        "- Identify weaknesses of Newton's method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## The Second-Order Optimality Condition (Section 4.1)\n",
        "For a twice-differentiable function $g(w)$:\n",
        "### Necessary Conditions\n",
        "If $w^*$ is a local minimum:\n",
        "1. $\\nabla g(w^*) = 0$ (first-order condition)\n",
        "2. $\\nabla^2 g(w^*) \\succeq 0$ (Hessian is positive semi-definite)\n",
        "### Sufficient Conditions\n",
        "If at $w^*$:\n",
        "1. $\\nabla g(w^*) = 0$\n",
        "2. $\\nabla^2 g(w^*) \\succ 0$ (Hessian is positive definite)\n",
        "Then $w^*$ is a **strict local minimum**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_3"
      },
      "source": [
        "## The Hessian Matrix\n",
        "The Hessian is the matrix of second partial derivatives:\n",
        "$$\\nabla^2 g(w) = H = \\begin{bmatrix} \n",
        "\\frac{\\partial^2 g}{\\partial w_1^2} & \\frac{\\partial^2 g}{\\partial w_1 \\partial w_2} & \\cdots \\\\\n",
        "\\frac{\\partial^2 g}{\\partial w_2 \\partial w_1} & \\frac{\\partial^2 g}{\\partial w_2^2} & \\cdots \\\\\n",
        "\\vdots & \\vdots & \\ddots\n",
        "\\end{bmatrix}$$\n",
        "### Properties\n",
        "- Symmetric (for smooth functions)\n",
        "- Eigenvalues indicate curvature\n",
        "- Positive definite $\\Rightarrow$ bowl-shaped (minimum)\n",
        "- Negative definite $\\Rightarrow$ hilltop (maximum)\n",
        "- Indefinite $\\Rightarrow$ saddle point\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_4"
      },
      "source": [
        "## The Geometry of Second-Order Taylor Series (Section 4.2)\n",
        "Near a point $w$:\n",
        "$$g(w + d) \\approx g(w) + \\nabla g(w)^T d + \\frac{1}{2} d^T \\nabla^2 g(w) d$$\n",
        "### The Quadratic Approximation\n",
        "This is a quadratic function in $d$. To minimize:\n",
        "$$\\frac{\\partial}{\\partial d}\\left[g(w) + \\nabla g(w)^T d + \\frac{1}{2} d^T H d\\right] = 0$$\n",
        "$$\\nabla g(w) + H d = 0$$\n",
        "$$d = -H^{-1} \\nabla g(w)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_5"
      },
      "source": [
        "mo.md(r\"\"\"\n",
        "## Newton's Method (Section 4.3)\n",
        "### The Newton Update\n",
        "$$w^{(k+1)} = w^{(k)} - [\\nabla^2 g(w^{(k)})]^{-1} \\nabla g(w^{(k)})$$\n",
        "Or equivalently:\n",
        "$$w^{(k+1)} = w^{(k)} + d^{(k)}$$\n",
        "where $d^{(k)}$ solves: $\\nabla^2 g(w^{(k)}) d^{(k)} = -\\nabla g(w^{(k)})$\n",
        "### Algorithm\n",
        "```python\n",
        "def newtons_method(g, grad_g, hess_g, w0, max_iter, tol):\n",
        "    w = w0\n",
        "    for k in range(max_iter):\n",
        "        gradient = grad_g(w)\n",
        "        hessian = hess_g(w)\n",
        "        if np.linalg.norm(gradient) < tol:\n",
        "            break\n",
        "        d = np.linalg.solve(hessian, -gradient)\n",
        "        w = w + d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_6"
      },
      "outputs": [],
      "source": [
        "# Newton's method visualization\n",
        "def f(w):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_7"
      },
      "source": [
        "## Convergence Properties\n",
        "### Newton's Method Convergence\n",
        "For functions with Lipschitz continuous Hessian, near a local minimum:\n",
        "$$\\|w^{(k+1)} - w^*\\| \\leq C \\|w^{(k)} - w^*\\|^2$$\n",
        "This is **quadratic convergence** â€” the number of correct digits roughly doubles each iteration!\n",
        "### Comparison\n",
        "| Method | Convergence Rate | Per-iteration Cost |\n",
        "|--------|-----------------|-------------------|\n",
        "| Gradient Descent | Linear: $O((1-1/\\kappa)^k)$ | $O(n)$ gradient |\n",
        "| Newton's Method | Quadratic | $O(n^3)$ Hessian inverse |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_8"
      },
      "source": [
        "## Two Natural Weaknesses of Newton's Method (Section 4.4)\n",
        "### Weakness 1: Computational Cost\n",
        "- Computing Hessian: $O(n^2)$ storage, $O(n^2)$ computation\n",
        "- Inverting/solving: $O(n^3)$\n",
        "- Impractical for large-scale ML ($n$ = millions)\n",
        "### Weakness 2: Non-Convex Functions\n",
        "- Newton step may go uphill (ascent direction)\n",
        "- May converge to saddle point or maximum\n",
        "- Hessian may be singular or indefinite\n",
        "### Solutions\n",
        "- **Damped Newton**: Use step size $w^{(k+1)} = w^{(k)} - \\alpha H^{-1} \\nabla g$\n",
        "- **Hessian modification**: Ensure positive definiteness\n",
        "- **Quasi-Newton methods**: Approximate Hessian (BFGS, L-BFGS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Convergence comparison\n",
        "def f_1d(w):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_10"
      },
      "source": [
        "## Summary\n",
        "| Concept | Key Points |\n",
        "|---------|------------|\n",
        "| **Second-order condition** | $\\nabla g = 0$ and $H \\succ 0$ |\n",
        "| **Newton's method** | $w \\leftarrow w - H^{-1} \\nabla g$ |\n",
        "| **Convergence** | Quadratic (very fast near optimum) |\n",
        "| **Weaknesses** | Expensive, issues with non-convex functions |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 4.\n",
        "- **Supplementary**: Nocedal, J. & Wright, S. (2006). *Numerical Optimization*, Chapters 3, 6.\n",
        "## Next Week\n",
        "**Linear Regression** (Chapter 5): Applying optimization to regression problems.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}