{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-04/notebook-math-dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 4: Neural Network Foundations - Perceptrons to MLPs\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 5\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand the perceptron as a linear classifier\n",
        "- Master multi-layer network mathematics\n",
        "- Learn forward propagation computation\n",
        "- Visualize decision boundaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## 4.1 The Perceptron: A Single Neuron\n",
        "$$y = \\sigma(w^T x + b) = \\sigma\\left(\\sum_{i=1}^n w_i x_i + b\\right)$$\n",
        "The perceptron defines a **hyperplane** that separates classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Visualize single perceptron decision boundary\n",
        "fig1, axes1 = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Generate linearly separable data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X_class0 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])\n",
        "X_class1 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])\n",
        "X_linear = np.vstack([X_class0, X_class1])\n",
        "y_linear = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
        "# Define perceptron weights (learned or set manually)\n",
        "w = np.array([1.0, 1.0])\n",
        "b = 0.0\n",
        "# Decision boundary: w^T x + b = 0  =>  x2 = -(w1*x1 + b)/w2\n",
        "x1_line = np.linspace(-5, 5, 100)\n",
        "x2_line = -(w[0] * x1_line + b) / w[1]\n",
        "# Plot\n",
        "ax1 = axes1[0]\n",
        "ax1.scatter(X_class0[:, 0], X_class0[:, 1], c='blue', label='Class 0', alpha=0.6)\n",
        "ax1.scatter(X_class1[:, 0], X_class1[:, 1], c='red', label='Class 1', alpha=0.6)\n",
        "ax1.plot(x1_line, x2_line, 'k-', linewidth=2, label='Decision boundary')\n",
        "# Add weight vector\n",
        "ax1.quiver(0, 0, w[0], w[1], angles='xy', scale_units='xy', scale=1, \n",
        "          color='green', width=0.03, label='Weight vector')\n",
        "ax1.axhline(0, color='gray', linewidth=0.5)\n",
        "ax1.axvline(0, color='gray', linewidth=0.5)\n",
        "ax1.set_xlim(-5, 5)\n",
        "ax1.set_ylim(-5, 5)\n",
        "ax1.set_xlabel('xâ‚')\n",
        "ax1.set_ylabel('xâ‚‚')\n",
        "ax1.set_title('Perceptron: Linear Decision Boundary')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "# XOR problem - not linearly separable\n",
        "ax2 = axes1[1]\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "colors_xor = ['blue' if y == 0 else 'red' for y in y_xor]\n",
        "ax2.scatter(X_xor[:, 0], X_xor[:, 1], c=colors_xor, s=200, edgecolors='black')\n",
        "for i, (x, y) in enumerate(zip(X_xor, y_xor)):\n",
        "    ax2.annotate(f'XOR={y}', xy=(x[0], x[1]), xytext=(x[0]+0.1, x[1]+0.1), fontsize=12)\n",
        "ax2.set_xlim(-0.5, 1.5)\n",
        "ax2.set_ylim(-0.5, 1.5)\n",
        "ax2.set_xlabel('xâ‚')\n",
        "ax2.set_ylabel('xâ‚‚')\n",
        "ax2.set_title('XOR Problem: NOT Linearly Separable\\n(Need multiple layers!)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_4"
      },
      "source": [
        "## 4.2 Activation Functions\n",
        "| Function | Formula | Use Case |\n",
        "|----------|---------|----------|\n",
        "| Sigmoid | $\\frac{1}{1+e^{-x}}$ | Output for binary classification |\n",
        "| ReLU | $\\max(0, x)$ | Hidden layers (most common) |\n",
        "| Tanh | $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | Hidden layers (zero-centered) |\n",
        "| Softmax | $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | Multi-class output |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Visualize activation functions\n",
        "fig2, axes2 = plt.subplots(2, 2, figsize=(14, 10))\n",
        "x_act = np.linspace(-5, 5, 200)\n",
        "# Sigmoid\n",
        "sigmoid = 1 / (1 + np.exp(-x_act))\n",
        "axes2[0, 0].plot(x_act, sigmoid, 'b-', linewidth=2, label='Ïƒ(x)')\n",
        "axes2[0, 0].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "axes2[0, 0].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes2[0, 0].fill_between(x_act, sigmoid, alpha=0.2)\n",
        "axes2[0, 0].set_title('Sigmoid: Output âˆˆ (0, 1)')\n",
        "axes2[0, 0].set_xlabel('x')\n",
        "axes2[0, 0].set_ylabel('Ïƒ(x)')\n",
        "axes2[0, 0].legend()\n",
        "axes2[0, 0].grid(True, alpha=0.3)\n",
        "# Tanh\n",
        "tanh = np.tanh(x_act)\n",
        "axes2[0, 1].plot(x_act, tanh, 'g-', linewidth=2, label='tanh(x)')\n",
        "axes2[0, 1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes2[0, 1].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes2[0, 1].fill_between(x_act, tanh, alpha=0.2, color='green')\n",
        "axes2[0, 1].set_title('Tanh: Output âˆˆ (-1, 1), Zero-Centered')\n",
        "axes2[0, 1].set_xlabel('x')\n",
        "axes2[0, 1].set_ylabel('tanh(x)')\n",
        "axes2[0, 1].legend()\n",
        "axes2[0, 1].grid(True, alpha=0.3)\n",
        "# ReLU\n",
        "relu = np.maximum(0, x_act)\n",
        "axes2[1, 0].plot(x_act, relu, 'r-', linewidth=2, label='ReLU(x)')\n",
        "axes2[1, 0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes2[1, 0].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes2[1, 0].fill_between(x_act, relu, alpha=0.2, color='red')\n",
        "axes2[1, 0].set_title('ReLU: Non-Saturating for x > 0')\n",
        "axes2[1, 0].set_xlabel('x')\n",
        "axes2[1, 0].set_ylabel('ReLU(x)')\n",
        "axes2[1, 0].legend()\n",
        "axes2[1, 0].grid(True, alpha=0.3)\n",
        "# Leaky ReLU and variants\n",
        "leaky_relu = np.where(x_act > 0, x_act, 0.1 * x_act)\n",
        "elu = np.where(x_act > 0, x_act, np.exp(x_act) - 1)\n",
        "axes2[1, 1].plot(x_act, relu, 'r-', linewidth=2, label='ReLU', alpha=0.5)\n",
        "axes2[1, 1].plot(x_act, leaky_relu, 'm-', linewidth=2, label='Leaky ReLU (Î±=0.1)')\n",
        "axes2[1, 1].plot(x_act, elu, 'c-', linewidth=2, label='ELU')\n",
        "axes2[1, 1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes2[1, 1].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes2[1, 1].set_title('ReLU Variants: Avoid Dead Neurons')\n",
        "axes2[1, 1].set_xlabel('x')\n",
        "axes2[1, 1].set_ylabel('f(x)')\n",
        "axes2[1, 1].legend()\n",
        "axes2[1, 1].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_6"
      },
      "source": [
        "## 4.3 Multi-Layer Perceptron (MLP)\n",
        "**Forward Propagation**:\n",
        "For layer $l$:\n",
        "$$z^{(l)} = W^{(l)} h^{(l-1)} + b^{(l)}$$\n",
        "$$h^{(l)} = \\sigma(z^{(l)})$$\n",
        "Stacking layers enables learning complex non-linear functions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Implement and visualize MLP solving XOR\n",
        "class SimpleMLP:\n",
        "    def __init__(self, layer_sizes):\n",
        "        np.random.seed(42)\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.5\n",
        "            b = np.zeros(layer_sizes[i+1])\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(b)\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        h = X\n",
        "        for i, (W, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            z = h @ W + b\n",
        "            if i < len(self.weights) - 1:\n",
        "                h = np.maximum(0, z)  # ReLU\n",
        "            else:\n",
        "                h = 1 / (1 + np.exp(-z))  # Sigmoid output\n",
        "            self.activations.append(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_8"
      },
      "source": [
        "## 4.4 Universal Approximation\n",
        "**Theorem**: A single hidden layer network with sufficient neurons can approximate any continuous function.\n",
        "Let's visualize how more neurons improve approximation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Demonstrate universal approximation\n",
        "def target_function(x):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_10"
      },
      "source": [
        "## 4.5 Network Architecture Visualization\n",
        "Understanding how dimensions flow through a network is crucial!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_11"
      },
      "outputs": [],
      "source": [
        "# Visualize network architecture\n",
        "def draw_network(ax, layer_sizes, title):\n",
        "    n_layers = len(layer_sizes)\n",
        "    max_neurons = max(layer_sizes)\n",
        "    layer_positions = np.linspace(0, 1, n_layers)\n",
        "    for i, (pos, n_neurons) in enumerate(zip(layer_positions, layer_sizes)):\n",
        "        # Calculate vertical positions for neurons\n",
        "        neuron_positions = np.linspace(0.1, 0.9, min(n_neurons, 10))\n",
        "        for j, y_pos in enumerate(neuron_positions):\n",
        "            circle = plt.Circle((pos, y_pos), 0.03, fill=True, \n",
        "                               color='lightblue' if i == 0 else ('lightgreen' if i == n_layers-1 else 'lightyellow'),\n",
        "                               edgecolor='black', linewidth=1)\n",
        "            ax.add_patch(circle)\n",
        "        # Draw connections to next layer\n",
        "        if i < n_layers - 1:\n",
        "            next_positions = np.linspace(0.1, 0.9, min(layer_sizes[i+1], 10))\n",
        "            for y1 in neuron_positions[:min(5, len(neuron_positions))]:\n",
        "                for y2 in next_positions[:min(5, len(next_positions))]:\n",
        "                    ax.plot([pos, layer_positions[i+1]], [y1, y2], \n",
        "                           'gray', linewidth=0.3, alpha=0.3)\n",
        "        # Add label\n",
        "        ax.text(pos, -0.05, f'Layer {i}\\n({layer_sizes[i]})', ha='center', fontsize=10)\n",
        "    ax.set_xlim(-0.1, 1.1)\n",
        "    ax.set_ylim(-0.15, 1.0)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(title, fontsize=12, pad=10)\n",
        "fig5, axes5 = plt.subplots(1, 2, figsize=(14, 6))\n",
        "# Architecture 1: Wide and shallow\n",
        "draw_network(axes5[0], [784, 512, 10], 'Shallow: 784 â†’ 512 â†’ 10\\nParams: ~400K')\n",
        "# Architecture 2: Deep and narrow  \n",
        "draw_network(axes5[1], [784, 256, 128, 64, 10], 'Deep: 784 â†’ 256 â†’ 128 â†’ 64 â†’ 10\\nParams: ~240K')\n",
        "plt.tight_layout()\n",
        "fig5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_12"
      },
      "outputs": [],
      "source": [
        "# Parameter counting\n",
        "def count_params(layer_sizes):\n",
        "    total = 0\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        weights = layer_sizes[i] * layer_sizes[i+1]\n",
        "        biases = layer_sizes[i+1]\n",
        "        total += weights + biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_13"
      },
      "source": [
        "## Summary\n",
        "| Concept | Key Insight |\n",
        "|---------|-------------|\n",
        "| **Perceptron** | Linear classifier, limited to linearly separable data |\n",
        "| **Activation** | Non-linearity enables complex function learning |\n",
        "| **MLP** | Stacked layers learn hierarchical representations |\n",
        "| **Universal Approximation** | Sufficient neurons can approximate any function |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 5.\n",
        "- **Supplementary**: Goodfellow, I., et al. (2016). *Deep Learning*, Chapter 6.\n",
        "## Connection to ML Refined Curriculum\n",
        "Neural networks extend the linear models from Weeks 4-7 to handle non-linear patterns:\n",
        "- Linear regression â†’ Neural network regression\n",
        "- Logistic regression â†’ Neural network classification\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook-math-dl",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}