{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 11: Recurrent Neural Networks (RNNs)\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 10\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand sequence modeling challenges\n",
        "- Master RNN architecture and mathematics\n",
        "- Learn LSTM and GRU mechanisms\n",
        "- Implement basic sequence models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## 11.1 Why Recurrent Networks?\n",
        "**Sequential data challenges:**\n",
        "- Variable length sequences\n",
        "- Order matters\n",
        "- Long-range dependencies\n",
        "**Solution**: Process one element at a time, maintain **hidden state**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_3"
      },
      "outputs": [],
      "source": [
        "# Visualize RNN unrolling\n",
        "fig1, ax1 = plt.subplots(figsize=(14, 5))\n",
        "# Draw unrolled RNN\n",
        "n_steps = 5\n",
        "for t in range(n_steps):\n",
        "    x_pos = t * 2.5\n",
        "    # Input\n",
        "    ax1.add_patch(plt.Circle((x_pos, 0), 0.3, fill=True, facecolor='lightblue', edgecolor='black'))\n",
        "    ax1.text(x_pos, 0, f'x{t}', ha='center', va='center', fontsize=10)\n",
        "    # RNN cell\n",
        "    ax1.add_patch(plt.Rectangle((x_pos - 0.4, 1.2), 0.8, 0.8, fill=True, \n",
        "                                  facecolor='lightgreen', edgecolor='black'))\n",
        "    ax1.text(x_pos, 1.6, 'RNN', ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "    # Output\n",
        "    ax1.add_patch(plt.Circle((x_pos, 3), 0.3, fill=True, facecolor='lightyellow', edgecolor='black'))\n",
        "    ax1.text(x_pos, 3, f'y{t}', ha='center', va='center', fontsize=10)\n",
        "    # Input to cell arrow\n",
        "    ax1.annotate('', xy=(x_pos, 1.2), xytext=(x_pos, 0.3),\n",
        "                arrowprops=dict(arrowstyle='->', color='black'))\n",
        "    # Cell to output arrow\n",
        "    ax1.annotate('', xy=(x_pos, 2.7), xytext=(x_pos, 2),\n",
        "                arrowprops=dict(arrowstyle='->', color='black'))\n",
        "    # Recurrent connection\n",
        "    if t < n_steps - 1:\n",
        "        ax1.annotate('', xy=(x_pos + 2.1, 1.6), xytext=(x_pos + 0.4, 1.6),\n",
        "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "        ax1.text(x_pos + 1.25, 1.85, f'h{t}', ha='center', fontsize=9, color='red')\n",
        "# Initial hidden state\n",
        "ax1.annotate('', xy=(-0.4, 1.6), xytext=(-1.2, 1.6),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "ax1.text(-1.5, 1.6, 'hâ‚€', ha='center', fontsize=10, color='red')\n",
        "ax1.set_xlim(-2, 12)\n",
        "ax1.set_ylim(-0.5, 3.5)\n",
        "ax1.set_aspect('equal')\n",
        "ax1.axis('off')\n",
        "ax1.set_title('RNN Unrolled Through Time\\n(Same weights at each step)', fontsize=14)\n",
        "fig1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_4"
      },
      "source": [
        "## 11.2 RNN Equations\n",
        "At each time step $t$:\n",
        "$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)$$\n",
        "$$y_t = W_y h_t + b_y$$\n",
        "**Key**: Same weights $W_h, W_x, W_y$ at every time step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Implement and visualize simple RNN\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        np.random.seed(42)\n",
        "        self.Wx = np.random.randn(input_size, hidden_size) * 0.1\n",
        "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.1\n",
        "        self.bh = np.zeros(hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"inputs: (seq_len, input_size)\"\"\"\n",
        "        h = np.zeros(self.hidden_size)\n",
        "        hidden_states = [h]\n",
        "        for x in inputs:\n",
        "            h = np.tanh(x @ self.Wx + h @ self.Wh + self.bh)\n",
        "            hidden_states.append(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_6"
      },
      "source": [
        "## 11.3 The Vanishing Gradient Problem\n",
        "For long sequences, gradients must flow through many time steps:\n",
        "$$\\frac{\\partial L_T}{\\partial h_1} = \\prod_{t=2}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
        "If each factor < 1 â†’ **Vanishing gradients**\n",
        "If each factor > 1 â†’ **Exploding gradients**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Demonstrate vanishing gradients\n",
        "def simulate_gradient_flow_rnn(seq_len, weight_scale=0.9):\n",
        "    gradients = [1.0]\n",
        "    for t in range(seq_len):\n",
        "        # Approximate gradient factor (simplified)\n",
        "        # In reality: W_h^T * tanh'(z)\n",
        "        # tanh' is between 0 and 1, so gradient shrinks\n",
        "        grad_factor = weight_scale * np.random.uniform(0.8, 1.0)\n",
        "        gradients.append(gradients[-1] * grad_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## 11.4 Long Short-Term Memory (LSTM)\n",
        "**Key Innovation**: Explicit memory cell with gating\n",
        "| Gate | Purpose |\n",
        "|------|---------|\n",
        "| Forget ($f_t$) | What to erase from memory |\n",
        "| Input ($i_t$) | What to write to memory |\n",
        "| Output ($o_t$) | What to read from memory |\n",
        "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
        "$$h_t = o_t \\odot \\tanh(c_t)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# Visualize LSTM gates\n",
        "fig4, axes4 = plt.subplots(1, 4, figsize=(16, 4))\n",
        "# Simulate gate activations over time\n",
        "np.random.seed(42)\n",
        "T = 30\n",
        "# Create a scenario: remember information at t=5, forget at t=20\n",
        "t_range = np.arange(T)\n",
        "# Forget gate (starts high, drops at t=20)\n",
        "forget_gate = np.ones(T) * 0.95\n",
        "forget_gate[20:25] = np.linspace(0.95, 0.1, 5)\n",
        "forget_gate[25:] = 0.1\n",
        "forget_gate += np.random.randn(T) * 0.03\n",
        "# Input gate (spikes at t=5)\n",
        "input_gate = np.ones(T) * 0.1\n",
        "input_gate[5:10] = np.linspace(0.1, 0.9, 5)\n",
        "input_gate[10:15] = np.linspace(0.9, 0.1, 5)\n",
        "input_gate += np.random.randn(T) * 0.03\n",
        "# Output gate\n",
        "output_gate = np.ones(T) * 0.5 + np.random.randn(T) * 0.1\n",
        "# Cell state (accumulates based on gates)\n",
        "cell_state = np.zeros(T)\n",
        "c = 0\n",
        "for t in range(T):\n",
        "    c = np.clip(forget_gate[t], 0, 1) * c + np.clip(input_gate[t], 0, 1) * 0.5\n",
        "    cell_state[t] = c\n",
        "axes4[0].plot(t_range, np.clip(forget_gate, 0, 1), 'r-', linewidth=2)\n",
        "axes4[0].fill_between(t_range, np.clip(forget_gate, 0, 1), alpha=0.3, color='red')\n",
        "axes4[0].set_title('Forget Gate $f_t$\\n(What to keep)')\n",
        "axes4[0].set_xlabel('Time')\n",
        "axes4[0].set_ylim(0, 1)\n",
        "axes4[0].axvline(20, color='gray', linestyle='--', alpha=0.5)\n",
        "axes4[0].grid(True, alpha=0.3)\n",
        "axes4[1].plot(t_range, np.clip(input_gate, 0, 1), 'g-', linewidth=2)\n",
        "axes4[1].fill_between(t_range, np.clip(input_gate, 0, 1), alpha=0.3, color='green')\n",
        "axes4[1].set_title('Input Gate $i_t$\\n(What to write)')\n",
        "axes4[1].set_xlabel('Time')\n",
        "axes4[1].set_ylim(0, 1)\n",
        "axes4[1].axvline(5, color='gray', linestyle='--', alpha=0.5)\n",
        "axes4[1].grid(True, alpha=0.3)\n",
        "axes4[2].plot(t_range, np.clip(output_gate, 0, 1), 'b-', linewidth=2)\n",
        "axes4[2].fill_between(t_range, np.clip(output_gate, 0, 1), alpha=0.3, color='blue')\n",
        "axes4[2].set_title('Output Gate $o_t$\\n(What to read)')\n",
        "axes4[2].set_xlabel('Time')\n",
        "axes4[2].set_ylim(0, 1)\n",
        "axes4[2].grid(True, alpha=0.3)\n",
        "axes4[3].plot(t_range, cell_state, 'm-', linewidth=2)\n",
        "axes4[3].fill_between(t_range, cell_state, alpha=0.3, color='purple')\n",
        "axes4[3].set_title('Cell State $c_t$\\n(Memory)')\n",
        "axes4[3].set_xlabel('Time')\n",
        "axes4[3].axvline(5, color='green', linestyle='--', alpha=0.5, label='Write')\n",
        "axes4[3].axvline(20, color='red', linestyle='--', alpha=0.5, label='Forget')\n",
        "axes4[3].legend()\n",
        "axes4[3].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_10"
      },
      "source": [
        "## 11.5 GRU: Simplified Gating\n",
        "**Gated Recurrent Unit**: Fewer parameters, similar performance\n",
        "| Gate | Purpose |\n",
        "|------|---------|\n",
        "| Reset ($r_t$) | How much past to forget |\n",
        "| Update ($z_t$) | Interpolation factor |\n",
        "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_11"
      },
      "outputs": [],
      "source": [
        "# Compare architectures\n",
        "fig5, ax5 = plt.subplots(figsize=(12, 6))\n",
        "architectures = {\n",
        "    'Vanilla RNN': {'params': 1, 'states': 1, 'gates': 0},\n",
        "    'GRU': {'params': 3, 'states': 1, 'gates': 2},\n",
        "    'LSTM': {'params': 4, 'states': 2, 'gates': 3},\n",
        "}\n",
        "x_arch = np.arange(len(architectures))\n",
        "width = 0.25\n",
        "params = [v['params'] for v in architectures.values()]\n",
        "states = [v['states'] for v in architectures.values()]\n",
        "gates = [v['gates'] for v in architectures.values()]\n",
        "ax5.bar(x_arch - width, params, width, label='Weight Matrices (Ã—hiddenÂ²)', color='blue', alpha=0.7)\n",
        "ax5.bar(x_arch, states, width, label='State Vectors', color='green', alpha=0.7)\n",
        "ax5.bar(x_arch + width, gates, width, label='Gates', color='red', alpha=0.7)\n",
        "ax5.set_xticks(x_arch)\n",
        "ax5.set_xticklabels(architectures.keys())\n",
        "ax5.set_ylabel('Count')\n",
        "ax5.set_title('Architecture Comparison: Complexity vs Capability')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3, axis='y')\n",
        "# Add annotations\n",
        "ax5.annotate('Simple but\\nvanishing gradients', xy=(0, 1), xytext=(0, 1.5),\n",
        "            fontsize=10, ha='center')\n",
        "ax5.annotate('Good tradeoff\\nfewer params', xy=(1, 3), xytext=(1, 3.5),\n",
        "            fontsize=10, ha='center')\n",
        "ax5.annotate('Most powerful\\nmore params', xy=(2, 4), xytext=(2, 4.5),\n",
        "            fontsize=10, ha='center')\n",
        "fig5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_12"
      },
      "source": [
        "## 11.6 Sequence-to-Sequence (Seq2Seq)\n",
        "**Encoder**: Process input â†’ context vector\n",
        "**Decoder**: Generate output from context\n",
        "```\n",
        "[Hello] [World] â†’ Encoder â†’ [context] â†’ Decoder â†’ [Bonjour] [Monde]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_13"
      },
      "outputs": [],
      "source": [
        "# Visualize seq2seq\n",
        "fig6, ax6 = plt.subplots(figsize=(14, 6))\n",
        "# Encoder\n",
        "encoder_words = ['Hello', 'World', '<EOS>']\n",
        "for i, word in enumerate(encoder_words):\n",
        "    x_enc = i * 1.5\n",
        "    ax6.add_patch(plt.Circle((x_enc, 0), 0.3, fill=True, facecolor='lightblue', edgecolor='black'))\n",
        "    ax6.text(x_enc, 0, word, ha='center', va='center', fontsize=8)\n",
        "    ax6.add_patch(plt.Rectangle((x_enc - 0.35, 1), 0.7, 0.7, fill=True, \n",
        "                                 facecolor='lightgreen', edgecolor='black'))\n",
        "    ax6.text(x_enc, 1.35, 'Enc', ha='center', va='center', fontsize=9)\n",
        "    ax6.annotate('', xy=(x_enc, 1), xytext=(x_enc, 0.3),\n",
        "                arrowprops=dict(arrowstyle='->', color='black'))\n",
        "    if i < len(encoder_words) - 1:\n",
        "        ax6.annotate('', xy=(x_enc + 1.15, 1.35), xytext=(x_enc + 0.35, 1.35),\n",
        "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "# Context vector\n",
        "ax6.add_patch(plt.Circle((4.5, 1.35), 0.4, fill=True, facecolor='yellow', edgecolor='black'))\n",
        "ax6.text(4.5, 1.35, 'ctx', ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "ax6.annotate('', xy=(4.1, 1.35), xytext=(3.35, 1.35),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "# Decoder\n",
        "decoder_words = ['<SOS>', 'Bonjour', 'Monde']\n",
        "for i, word in enumerate(decoder_words):\n",
        "    x_dec = 6 + i * 1.5\n",
        "    ax6.add_patch(plt.Rectangle((x_dec - 0.35, 1), 0.7, 0.7, fill=True, \n",
        "                                 facecolor='lightyellow', edgecolor='black'))\n",
        "    ax6.text(x_dec, 1.35, 'Dec', ha='center', va='center', fontsize=9)\n",
        "    if i < len(decoder_words) - 1:\n",
        "        ax6.add_patch(plt.Circle((x_dec, 2.5), 0.3, fill=True, facecolor='lightcoral', edgecolor='black'))\n",
        "        ax6.text(x_dec, 2.5, decoder_words[i+1], ha='center', va='center', fontsize=8)\n",
        "        ax6.annotate('', xy=(x_dec, 2.2), xytext=(x_dec, 1.7),\n",
        "                    arrowprops=dict(arrowstyle='->', color='black'))\n",
        "    if i < len(decoder_words) - 1:\n",
        "        ax6.annotate('', xy=(x_dec + 1.15, 1.35), xytext=(x_dec + 0.35, 1.35),\n",
        "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "# Context to decoder\n",
        "ax6.annotate('', xy=(5.65, 1.35), xytext=(4.9, 1.35),\n",
        "            arrowprops=dict(arrowstyle='->', color='purple', lw=2))\n",
        "# Labels\n",
        "ax6.text(1.5, -0.8, 'ENCODER', ha='center', fontsize=12, fontweight='bold', color='green')\n",
        "ax6.text(7.5, -0.8, 'DECODER', ha='center', fontsize=12, fontweight='bold', color='orange')\n",
        "ax6.set_xlim(-1, 11)\n",
        "ax6.set_ylim(-1.2, 3)\n",
        "ax6.axis('off')\n",
        "ax6.set_title('Sequence-to-Sequence Architecture\\n(Machine Translation Example)', fontsize=14)\n",
        "fig6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_14"
      },
      "source": [
        "## Summary\n",
        "| Architecture | Key Feature | Use Case |\n",
        "|--------------|-------------|----------|\n",
        "| **Vanilla RNN** | Simple hidden state | Short sequences |\n",
        "| **LSTM** | Cell state + 3 gates | Long-term dependencies |\n",
        "| **GRU** | 2 gates, simpler | Similar to LSTM |\n",
        "| **Bidirectional** | Forward + backward | Full context needed |\n",
        "| **Seq2Seq** | Encoder-decoder | Translation, summarization |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapter 10.\n",
        "- **LSTM**: Hochreiter & Schmidhuber (1997)\n",
        "- **GRU**: Cho et al. (2014)\n",
        "## Connection to ML Refined Curriculum\n",
        "RNNs extend time series from Week 10:\n",
        "- Automatic feature learning from sequences\n",
        "- Handle variable-length data\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}