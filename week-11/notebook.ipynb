{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harunpirim/IME775/blob/main/week-11/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
        "",
        "---",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_0"
      },
      "source": [
        "# Week 11: Principles of Feature Learning & Cross-Validation\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 11**\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand universal approximation\n",
        "- Apply cross-validation for model selection\n",
        "- Implement regularization strategies\n",
        "- Distinguish training, validation, and test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_2"
      },
      "source": [
        "## Universal Approximators (Section 11.2)\n",
        "### The Goal\n",
        "Find functions that can approximate **any** continuous function arbitrarily well.\n",
        "### Examples of Universal Approximators\n",
        "| Type | Form |\n",
        "|------|------|\n",
        "| Polynomials | $\\sum_j w_j x^j$ |\n",
        "| Neural Networks | Multi-layer perceptrons |\n",
        "| Kernel Methods | $\\sum_j w_j K(x, x_j)$ |\n",
        "| Trees | Ensemble of decision trees |\n",
        "### The Catch\n",
        "Universal approximation â‰  learning from finite data!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_3"
      },
      "source": [
        "## Naive Cross-Validation (Section 11.4)\n",
        "### The Problem\n",
        "How do we choose model complexity (e.g., polynomial degree)?\n",
        "### Train-Validation Split\n",
        "1. Split data: Training set + Validation set\n",
        "2. Train model on training set\n",
        "3. Evaluate on validation set\n",
        "4. Choose complexity that minimizes validation error\n",
        "### The Danger\n",
        "Using test data for model selection leads to **overfitting** to the test set!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_4"
      },
      "outputs": [],
      "source": [
        "# Cross-validation example\n",
        "np.random.seed(42)\n",
        "n = 50\n",
        "X = np.sort(np.random.uniform(0, 1, n)).reshape(-1, 1)\n",
        "y = np.sin(2 * np.pi * X.ravel()) + 0.3 * np.random.randn(n)\n",
        "# Split into train/validation\n",
        "train_idx = np.random.choice(n, int(0.7*n), replace=False)\n",
        "val_idx = np.setdiff1d(np.arange(n), train_idx)\n",
        "X_train, y_train = X[train_idx], y[train_idx]\n",
        "X_val, y_val = X[val_idx], y[val_idx]\n",
        "degrees = range(1, 15)\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "for d in degrees:\n",
        "    poly = PolynomialFeatures(degree=d)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    X_val_poly = poly.transform(X_val)\n",
        "    model = Ridge(alpha=0.0001)\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    train_errors.append(np.mean((y_train - model.predict(X_train_poly))**2))\n",
        "    val_errors.append(np.mean((y_val - model.predict(X_val_poly))**2))\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(degrees, train_errors, 'b-o', label='Training Error')\n",
        "ax.plot(degrees, val_errors, 'r-o', label='Validation Error')\n",
        "ax.axvline(degrees[np.argmin(val_errors)], color='g', linestyle='--', \n",
        "           label=f'Best degree: {degrees[np.argmin(val_errors)]}')\n",
        "ax.set_xlabel('Polynomial Degree')\n",
        "ax.set_ylabel('Mean Squared Error')\n",
        "ax.set_title('Train-Validation Split (ML Refined, Section 11.4)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_5"
      },
      "source": [
        "## K-Fold Cross-Validation (Section 11.10)\n",
        "### Why K-Fold?\n",
        "- More robust than single split\n",
        "- Uses all data for both training and validation\n",
        "- Less variance in estimate\n",
        "### Algorithm\n",
        "```\n",
        "1. Split data into K folds\n",
        "2. For k = 1 to K:\n",
        "   a. Train on all folds except k\n",
        "   b. Validate on fold k\n",
        "3. Average the K validation errors\n",
        "```\n",
        "### Common Choices\n",
        "- K = 5 or K = 10 (standard)\n",
        "- K = n (Leave-One-Out, high variance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_6"
      },
      "outputs": [],
      "source": [
        "# K-Fold Cross-Validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "degrees_kfold = range(1, 12)\n",
        "cv_mean_errors = []\n",
        "cv_std_errors = []\n",
        "for d in degrees_kfold:\n",
        "    poly = PolynomialFeatures(degree=d)\n",
        "    fold_errors = []\n",
        "    for train_idx_kf, val_idx_kf in kfold.split(X):\n",
        "        X_train_kf = poly.fit_transform(X[train_idx_kf])\n",
        "        X_val_kf = poly.transform(X[val_idx_kf])\n",
        "        model = Ridge(alpha=0.0001)\n",
        "        model.fit(X_train_kf, y[train_idx_kf])\n",
        "        error = np.mean((y[val_idx_kf] - model.predict(X_val_kf))**2)\n",
        "        fold_errors.append(error)\n",
        "    cv_mean_errors.append(np.mean(fold_errors))\n",
        "    cv_std_errors.append(np.std(fold_errors))\n",
        "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "ax2.errorbar(degrees_kfold, cv_mean_errors, yerr=cv_std_errors, \n",
        "             fmt='b-o', capsize=3, label='5-Fold CV Error')\n",
        "ax2.axvline(list(degrees_kfold)[np.argmin(cv_mean_errors)], color='g', linestyle='--',\n",
        "            label=f'Best degree: {list(degrees_kfold)[np.argmin(cv_mean_errors)]}')\n",
        "ax2.set_xlabel('Polynomial Degree')\n",
        "ax2.set_ylabel('Mean Squared Error')\n",
        "ax2.set_title('K-Fold Cross-Validation (ML Refined, Section 11.10)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_7"
      },
      "source": [
        "## Efficient Cross-Validation via Regularization (Section 11.6)\n",
        "### The Idea\n",
        "Instead of varying model complexity, fix complexity and vary regularization.\n",
        "$$\\min_w \\|y - X_\\phi w\\|^2 + \\lambda \\|w\\|^2$$\n",
        "- Small $\\lambda$: Complex model (low bias, high variance)\n",
        "- Large $\\lambda$: Simple model (high bias, low variance)\n",
        "### Advantages\n",
        "- Continuous hyperparameter\n",
        "- Often faster to tune\n",
        "- Built-in with many libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_8"
      },
      "outputs": [],
      "source": [
        "# Regularization path\n",
        "poly_reg = PolynomialFeatures(degree=10)\n",
        "X_poly = poly_reg.fit_transform(X)\n",
        "alphas = np.logspace(-6, 2, 50)\n",
        "# Store coefficients\n",
        "coefs = []\n",
        "for alpha in alphas:\n",
        "    model = Ridge(alpha=alpha)\n",
        "    model.fit(X_poly, y)\n",
        "    coefs.append(model.coef_)\n",
        "coefs = np.array(coefs)\n",
        "fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
        "for i in range(1, min(10, coefs.shape[1])):\n",
        "    ax3.semilogx(alphas, coefs[:, i], linewidth=2, label=f'$w_{i}$')\n",
        "ax3.axhline(0, color='gray', linewidth=0.5)\n",
        "ax3.set_xlabel('Regularization parameter Î»')\n",
        "ax3.set_ylabel('Coefficient value')\n",
        "ax3.set_title('Regularization Path (ML Refined, Section 11.6)')\n",
        "ax3.legend(loc='upper right')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_9"
      },
      "source": [
        "## Testing Data (Section 11.7)\n",
        "### The Three-Way Split\n",
        "| Set | Purpose | Used For |\n",
        "|-----|---------|----------|\n",
        "| **Training** | Fit model | Learning parameters |\n",
        "| **Validation** | Model selection | Hyperparameter tuning |\n",
        "| **Test** | Final evaluation | Report performance |\n",
        "### Critical Rule\n",
        "> **Never use test data for any decision-making!**\n",
        "Test data should only be touched once, at the very end.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown_10"
      },
      "source": [
        "## Summary\n",
        "| Concept | Key Point |\n",
        "|---------|-----------|\n",
        "| **Universal approximators** | Can fit any function (with enough capacity) |\n",
        "| **Cross-validation** | Estimate generalization error |\n",
        "| **K-Fold CV** | More robust than single split |\n",
        "| **Regularization** | Control complexity continuously |\n",
        "| **Test set** | Only for final evaluation |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 11.\n",
        "- **Supplementary**: Hastie, T. et al. (2009). *The Elements of Statistical Learning*, Chapter 7.\n",
        "## Next Week\n",
        "**Kernel Methods & Neural Networks** (Chapters 12-13): Advanced nonlinear models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}