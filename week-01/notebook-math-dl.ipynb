{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 1: Mathematical Foundations - Linear Algebra for Deep Learning\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "üìñ **Reference**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapters 1-2\n",
        "---\n",
        "## Learning Objectives\n",
        "- Understand vectors, matrices, and tensors as data representations\n",
        "- Master matrix operations essential for neural networks\n",
        "- Connect linear algebra to neural network computations\n",
        "- Visualize transformations geometrically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## 1.1 Vectors: The Building Blocks\n",
        "### The Dot Product: Heart of Neural Networks\n",
        "The dot product is the fundamental operation in neural networks:\n",
        "$$\\mathbf{w}^T \\mathbf{x} = \\sum_{i=1}^{n} w_i x_i$$\n",
        "**Geometric interpretation**: Measures alignment between vectors\n",
        "$$\\mathbf{w}^T \\mathbf{x} = \\|\\mathbf{w}\\| \\|\\mathbf{x}\\| \\cos\\theta$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_3"
      },
      "source": [
        "# Visualize dot product as projection\n",
        "fig1, axes1 = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Left: Vector alignment\n",
        "ax1 = axes1[0]\n",
        "w = np.array([3, 1])\n",
        "x1 = np.array([2, 2])\n",
        "x2 = np.array([1, -2])\n",
        "ax1.quiver(0, 0, w[0], w[1], angles='xy', scale_units='xy', scale=1, \n",
        "           color='blue', label=f'w = {w}', width=0.02)\n",
        "ax1.quiver(0, 0, x1[0], x1[1], angles='xy', scale_units='xy', scale=1, \n",
        "           color='green', label=f'x‚ÇÅ = {x1}, w¬∑x‚ÇÅ = {np.dot(w, x1)}', width=0.02)\n",
        "ax1.quiver(0, 0, x2[0], x2[1], angles='xy', scale_units='xy', scale=1, \n",
        "           color='red', label=f'x‚ÇÇ = {x2}, w¬∑x‚ÇÇ = {np.dot(w, x2)}', width=0.02)\n",
        "ax1.set_xlim(-3, 4)\n",
        "ax1.set_ylim(-3, 3)\n",
        "ax1.axhline(0, color='gray', linewidth=0.5)\n",
        "ax1.axvline(0, color='gray', linewidth=0.5)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_aspect('equal')\n",
        "ax1.legend()\n",
        "ax1.set_title('Dot Product: Measures Alignment')\n",
        "# Right: Neuron as dot product\n",
        "ax2 = axes1[1]\n",
        "angles = np.linspace(0, 2*np.pi, 100)\n",
        "ax2.plot(np.cos(angles), np.sin(angles), 'b-', alpha=0.3)\n",
        "# Sample points and their dot products with w\n",
        "np.random.seed(42)\n",
        "points = np.random.randn(50, 2) * 0.8\n",
        "w_normalized = w / np.linalg.norm(w)\n",
        "dots = points @ w_normalized\n",
        "scatter = ax2.scatter(points[:, 0], points[:, 1], c=dots, cmap='RdYlGn', \n",
        "                      s=60, edgecolors='black', linewidth=0.5)\n",
        "ax2.quiver(0, 0, w_normalized[0], w_normalized[1], angles='xy', \n",
        "           scale_units='xy', scale=1, color='blue', width=0.02, label='Weight direction')\n",
        "plt.colorbar(scatter, ax=ax2, label='Dot product value')\n",
        "ax2.set_xlim(-2, 2)\n",
        "ax2.set_ylim(-2, 2)\n",
        "ax2.set_aspect('equal')\n",
        "ax2.set_title('Neuron Response: w¬∑x for Different Inputs')\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "fig1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_4"
      },
      "source": [
        "## 1.2 Matrix Operations in Neural Networks\n",
        "### Matrix-Vector Multiplication: Layer Transformation\n",
        "$$\\mathbf{y} = \\mathbf{W}\\mathbf{x}$$\n",
        "Each output is a dot product of a weight row with input:\n",
        "$$y_i = \\mathbf{w}_i^T \\mathbf{x}$$\n",
        "**Neural network interpretation**:\n",
        "- $\\mathbf{x} \\in \\mathbb{R}^n$: Input activations\n",
        "- $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$: Weight matrix\n",
        "- $\\mathbf{y} \\in \\mathbb{R}^m$: Output (before activation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_5"
      },
      "outputs": [],
      "source": [
        "# Visualize matrix transformation\n",
        "fig2, axes2 = plt.subplots(1, 3, figsize=(15, 4))\n",
        "# Original points (circle)\n",
        "theta = np.linspace(0, 2*np.pi, 100)\n",
        "circle = np.vstack([np.cos(theta), np.sin(theta)])\n",
        "# Different transformations\n",
        "transformations = [\n",
        "    (np.array([[2, 0], [0, 1]]), 'Scaling: diag(2, 1)'),\n",
        "    (np.array([[np.cos(np.pi/4), -np.sin(np.pi/4)], \n",
        "               [np.sin(np.pi/4), np.cos(np.pi/4)]]), 'Rotation: 45¬∞'),\n",
        "    (np.array([[1, 0.5], [0.5, 1]]), 'Shear + Scale')\n",
        "]\n",
        "for ax, (W, title) in zip(axes2, transformations):\n",
        "    # Original circle\n",
        "    ax.plot(circle[0], circle[1], 'b-', alpha=0.3, linewidth=2, label='Original')\n",
        "    # Transformed circle\n",
        "    transformed = W @ circle\n",
        "    ax.plot(transformed[0], transformed[1], 'r-', linewidth=2, label='Transformed')\n",
        "    # Show basis vectors\n",
        "    ax.quiver(0, 0, 1, 0, angles='xy', scale_units='xy', scale=1, \n",
        "              color='blue', alpha=0.5, width=0.02)\n",
        "    ax.quiver(0, 0, 0, 1, angles='xy', scale_units='xy', scale=1, \n",
        "              color='blue', alpha=0.5, width=0.02)\n",
        "    ax.quiver(0, 0, W[0, 0], W[1, 0], angles='xy', scale_units='xy', scale=1, \n",
        "              color='red', alpha=0.5, width=0.02)\n",
        "    ax.quiver(0, 0, W[0, 1], W[1, 1], angles='xy', scale_units='xy', scale=1, \n",
        "              color='red', alpha=0.5, width=0.02)\n",
        "    ax.set_xlim(-3, 3)\n",
        "    ax.set_ylim(-3, 3)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_title(title)\n",
        "    ax.legend(fontsize=8)\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_6"
      },
      "source": [
        "## 1.3 Eigenvalues and Eigenvectors\n",
        "### Definition\n",
        "For a square matrix $\\mathbf{A}$:\n",
        "$$\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$$\n",
        "Eigenvectors define directions that are only **scaled** (not rotated) by the transformation.\n",
        "### Eigendecomposition\n",
        "For symmetric $\\mathbf{A}$:\n",
        "$$\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_7"
      },
      "outputs": [],
      "source": [
        "# Visualize eigenvectors\n",
        "fig3, axes3 = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Symmetric matrix\n",
        "A = np.array([[3, 1], [1, 2]])\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(A)\n",
        "# Left: Eigenvalue/eigenvector visualization\n",
        "ax1_eigen = axes3[0]\n",
        "# Draw transformed circle\n",
        "theta_e = np.linspace(0, 2*np.pi, 100)\n",
        "circle_e = np.vstack([np.cos(theta_e), np.sin(theta_e)])\n",
        "transformed_e = A @ circle_e\n",
        "ax1_eigen.plot(circle_e[0], circle_e[1], 'b-', alpha=0.3, linewidth=2, label='Original circle')\n",
        "ax1_eigen.plot(transformed_e[0], transformed_e[1], 'r-', linewidth=2, label='Transformed')\n",
        "# Draw eigenvectors\n",
        "for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
        "    ax1_eigen.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1,\n",
        "                    color=['green', 'purple'][i], width=0.03, \n",
        "                    label=f'v{i+1}: Œª={val:.2f}')\n",
        "    ax1_eigen.quiver(0, 0, val*vec[0], val*vec[1], angles='xy', scale_units='xy', scale=1,\n",
        "                    color=['green', 'purple'][i], alpha=0.3, width=0.02)\n",
        "ax1_eigen.set_xlim(-4, 4)\n",
        "ax1_eigen.set_ylim(-4, 4)\n",
        "ax1_eigen.set_aspect('equal')\n",
        "ax1_eigen.grid(True, alpha=0.3)\n",
        "ax1_eigen.legend()\n",
        "ax1_eigen.set_title(f'Eigenvectors of A = [[3,1],[1,2]]')\n",
        "# Right: PCA interpretation\n",
        "ax2_eigen = axes3[1]\n",
        "np.random.seed(42)\n",
        "# Generate correlated data\n",
        "cov = np.array([[2, 1.5], [1.5, 2]])\n",
        "data = np.random.multivariate_normal([0, 0], cov, 200)\n",
        "# Compute PCA via eigendecomposition\n",
        "data_centered = data - data.mean(axis=0)\n",
        "cov_matrix = np.cov(data_centered.T)\n",
        "pca_eigenvalues, pca_eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "# Sort by eigenvalue (descending)\n",
        "idx = np.argsort(pca_eigenvalues)[::-1]\n",
        "pca_eigenvalues = pca_eigenvalues[idx]\n",
        "pca_eigenvectors = pca_eigenvectors[:, idx]\n",
        "ax2_eigen.scatter(data[:, 0], data[:, 1], alpha=0.4, s=20)\n",
        "for i, (val, vec) in enumerate(zip(pca_eigenvalues, pca_eigenvectors.T)):\n",
        "    scale = np.sqrt(val) * 2\n",
        "    ax2_eigen.quiver(0, 0, vec[0]*scale, vec[1]*scale, angles='xy', \n",
        "                    scale_units='xy', scale=1, color=['red', 'blue'][i],\n",
        "                    width=0.03, label=f'PC{i+1}: Œª={val:.2f}')\n",
        "ax2_eigen.set_xlim(-5, 5)\n",
        "ax2_eigen.set_ylim(-5, 5)\n",
        "ax2_eigen.set_aspect('equal')\n",
        "ax2_eigen.grid(True, alpha=0.3)\n",
        "ax2_eigen.legend()\n",
        "ax2_eigen.set_title('PCA: Eigenvectors of Covariance Matrix')\n",
        "plt.tight_layout()\n",
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## 1.4 Singular Value Decomposition (SVD)\n",
        "### Definition\n",
        "Any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ can be decomposed:\n",
        "$$\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$$\n",
        "### Low-Rank Approximation\n",
        "Truncated SVD with top $k$ singular values provides the best rank-$k$ approximation.\n",
        "**Applications**: Model compression, dimensionality reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "# SVD for image compression\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "import io\n",
        "# Create a simple test image (gradient pattern)\n",
        "def create_test_image(size=100):\n",
        "    x, y = np.meshgrid(np.linspace(0, 1, size), np.linspace(0, 1, size))\n",
        "    img = np.sin(5*x) * np.cos(5*y) + 0.5*np.sin(10*x + 5*y)\n",
        "    img = (img - img.min()) / (img.max() - img.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_10"
      },
      "outputs": [],
      "source": [
        "# Singular value spectrum\n",
        "fig5, axes5 = plt.subplots(1, 2, figsize=(14, 4))\n",
        "ax1_svd = axes5[0]\n",
        "ax1_svd.semilogy(sigma_img, 'b.-')\n",
        "ax1_svd.set_xlabel('Index')\n",
        "ax1_svd.set_ylabel('Singular Value (log scale)')\n",
        "ax1_svd.set_title('Singular Value Spectrum')\n",
        "ax1_svd.grid(True, alpha=0.3)\n",
        "ax2_svd = axes5[1]\n",
        "cumulative_energy = np.cumsum(sigma_img**2) / np.sum(sigma_img**2)\n",
        "ax2_svd.plot(cumulative_energy, 'g.-')\n",
        "ax2_svd.axhline(0.9, color='r', linestyle='--', label='90% energy')\n",
        "ax2_svd.axhline(0.99, color='orange', linestyle='--', label='99% energy')\n",
        "k_90 = np.searchsorted(cumulative_energy, 0.9) + 1\n",
        "k_99 = np.searchsorted(cumulative_energy, 0.99) + 1\n",
        "ax2_svd.axvline(k_90, color='r', linestyle=':', alpha=0.5)\n",
        "ax2_svd.axvline(k_99, color='orange', linestyle=':', alpha=0.5)\n",
        "ax2_svd.set_xlabel('Number of Singular Values (k)')\n",
        "ax2_svd.set_ylabel('Cumulative Energy')\n",
        "ax2_svd.set_title(f'Energy Captured: 90% at k={k_90}, 99% at k={k_99}')\n",
        "ax2_svd.legend()\n",
        "ax2_svd.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "## 1.5 Norms and Regularization\n",
        "| Norm | Definition | Deep Learning Use |\n",
        "|------|------------|-------------------|\n",
        "| L1 | $\\|\\mathbf{x}\\|_1 = \\sum_i |x_i|$ | Sparsity (Lasso) |\n",
        "| L2 | $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}$ | Weight decay |\n",
        "| Frobenius | $\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{ij} a_{ij}^2}$ | Matrix regularization |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_12"
      },
      "outputs": [],
      "source": [
        "# Visualize norm balls\n",
        "fig6, axes6 = plt.subplots(1, 3, figsize=(15, 4))\n",
        "theta_norm = np.linspace(0, 2*np.pi, 1000)\n",
        "# L1 ball\n",
        "ax1_norm = axes6[0]\n",
        "l1_ball = np.vstack([np.sign(np.cos(theta_norm)) * np.abs(np.cos(theta_norm)),\n",
        "                     np.sign(np.sin(theta_norm)) * np.abs(np.sin(theta_norm))])\n",
        "# Diamond shape\n",
        "t = np.linspace(0, 2*np.pi, 5)\n",
        "l1_x = np.cos(t)\n",
        "l1_y = np.sin(t)\n",
        "l1_points = np.array([[1, 0], [0, 1], [-1, 0], [0, -1], [1, 0]])\n",
        "ax1_norm.plot(l1_points[:, 0], l1_points[:, 1], 'b-', linewidth=2)\n",
        "ax1_norm.fill(l1_points[:, 0], l1_points[:, 1], alpha=0.3)\n",
        "ax1_norm.set_title('L1 Ball: $\\|x\\|_1 \\leq 1$\\nPromotes Sparsity')\n",
        "ax1_norm.set_xlim(-1.5, 1.5)\n",
        "ax1_norm.set_ylim(-1.5, 1.5)\n",
        "ax1_norm.set_aspect('equal')\n",
        "ax1_norm.grid(True, alpha=0.3)\n",
        "# L2 ball\n",
        "ax2_norm = axes6[1]\n",
        "l2_x = np.cos(theta_norm)\n",
        "l2_y = np.sin(theta_norm)\n",
        "ax2_norm.plot(l2_x, l2_y, 'g-', linewidth=2)\n",
        "ax2_norm.fill(l2_x, l2_y, alpha=0.3, color='green')\n",
        "ax2_norm.set_title('L2 Ball: $\\|x\\|_2 \\leq 1$\\nWeight Decay')\n",
        "ax2_norm.set_xlim(-1.5, 1.5)\n",
        "ax2_norm.set_ylim(-1.5, 1.5)\n",
        "ax2_norm.set_aspect('equal')\n",
        "ax2_norm.grid(True, alpha=0.3)\n",
        "# L-inf ball\n",
        "ax3_norm = axes6[2]\n",
        "linf_points = np.array([[1, 1], [-1, 1], [-1, -1], [1, -1], [1, 1]])\n",
        "ax3_norm.plot(linf_points[:, 0], linf_points[:, 1], 'r-', linewidth=2)\n",
        "ax3_norm.fill(linf_points[:, 0], linf_points[:, 1], alpha=0.3, color='red')\n",
        "ax3_norm.set_title('L‚àû Ball: $\\|x\\|_‚àû \\leq 1$\\nAdversarial Robustness')\n",
        "ax3_norm.set_xlim(-1.5, 1.5)\n",
        "ax3_norm.set_ylim(-1.5, 1.5)\n",
        "ax3_norm.set_aspect('equal')\n",
        "ax3_norm.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_13"
      },
      "source": [
        "## Summary\n",
        "| Concept | Definition | Neural Network Role |\n",
        "|---------|------------|---------------------|\n",
        "| **Dot Product** | $\\mathbf{w}^T\\mathbf{x}$ | Core neuron computation |\n",
        "| **Matrix Mult.** | $\\mathbf{Y} = \\mathbf{WX}$ | Layer transformation |\n",
        "| **Eigendecomp.** | $\\mathbf{A} = \\mathbf{Q\\Lambda Q}^T$ | PCA, Hessian analysis |\n",
        "| **SVD** | $\\mathbf{A} = \\mathbf{U\\Sigma V}^T$ | Compression, init. |\n",
        "| **Norms** | $\\|\\cdot\\|_p$ | Regularization |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Krishnendu Chaudhury. *Math and Architectures of Deep Learning*, Chapters 1-2.\n",
        "- **Supplementary**: Goodfellow, I., et al. (2016). *Deep Learning*, Chapter 2.\n",
        "## Connection to ML Refined Curriculum\n",
        "This linear algebra foundation supports:\n",
        "- Week 1: Understanding feature representations\n",
        "- Weeks 2-3: Gradient computations\n",
        "- Week 8: PCA as eigendecomposition\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}