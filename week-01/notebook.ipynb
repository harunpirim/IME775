{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# Week 1: Introduction to Machine Learning\n",
        "**IME775: Data Driven Modeling and Optimization**\n",
        "ðŸ“– **Reference**: Watt, Borhani, & Katsaggelos (2020). *Machine Learning Refined* (2nd ed.), **Chapter 1**\n",
        "---\n",
        "## Learning Objectives\n",
        "By the end of this session, you will be able to:\n",
        "- Define machine learning and its role in data-driven decision making\n",
        "- Understand the basic taxonomy of ML problems (supervised, unsupervised)\n",
        "- Connect machine learning to mathematical optimization\n",
        "- Identify regression vs classification problems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_2"
      },
      "source": [
        "## What is Machine Learning?\n",
        "> \"Machine learning is a term used to describe a broad collection of pattern-finding \n",
        "> algorithms designed to properly identify system rules empirically by leveraging \n",
        "> enormous amounts of data and computing power.\" â€” *ML Refined, Ch. 1*\n",
        "### The Core Idea\n",
        "Given **data** about a system, find a **rule** (model) that:\n",
        "- Accurately describes the relationship between inputs and outputs\n",
        "- Generalizes to new, unseen data\n",
        "- Enables prediction and decision-making\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_3"
      },
      "source": [
        "## Example: Distinguishing Cats from Dogs (Section 1.2)\n",
        "The classic ML problem from Chapter 1:\n",
        "1. **Collect data**: Images labeled as \"cat\" or \"dog\"\n",
        "2. **Extract features**: Pixel values, edge patterns, etc.\n",
        "3. **Learn a rule**: Find decision boundary separating cats from dogs\n",
        "4. **Make predictions**: Classify new images\n",
        "This is a **supervised learning** problem with **binary classification**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_4"
      },
      "outputs": [],
      "source": [
        "# Visualize a simple classification problem\n",
        "np.random.seed(42)\n",
        "# Generate two classes of data\n",
        "n = 50\n",
        "class_0 = np.random.randn(n, 2) + np.array([2, 2])\n",
        "class_1 = np.random.randn(n, 2) + np.array([-2, -2])\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.scatter(class_0[:, 0], class_0[:, 1], c='blue', s=80, \n",
        "           label='Class 0 (e.g., Cats)', alpha=0.7, edgecolors='black')\n",
        "ax.scatter(class_1[:, 0], class_1[:, 1], c='red', s=80, \n",
        "           label='Class 1 (e.g., Dogs)', alpha=0.7, edgecolors='black')\n",
        "# Decision boundary\n",
        "x_line = np.linspace(-5, 5, 100)\n",
        "ax.plot(x_line, x_line, 'k--', linewidth=2, label='Decision Boundary')\n",
        "ax.set_xlabel('Feature 1', fontsize=12)\n",
        "ax.set_ylabel('Feature 2', fontsize=12)\n",
        "ax.set_title('Binary Classification Problem (ML Refined, Section 1.2)', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_aspect('equal')\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_5"
      },
      "source": [
        "## The Basic Taxonomy of ML Problems (Section 1.3)\n",
        "### Supervised Learning\n",
        "**Given**: Input-output pairs $(x_i, y_i)$\n",
        "**Goal**: Learn mapping $f: x \\rightarrow y$\n",
        "| Problem Type | Output $y$ | Examples |\n",
        "|--------------|------------|----------|\n",
        "| **Regression** | Continuous | House prices, temperature |\n",
        "| **Classification** | Discrete | Spam/not spam, disease diagnosis |\n",
        "### Unsupervised Learning\n",
        "**Given**: Only inputs $x_i$ (no labels)\n",
        "**Goal**: Find structure or patterns\n",
        "| Problem Type | Goal | Examples |\n",
        "|--------------|------|----------|\n",
        "| **Clustering** | Group similar data | Customer segmentation |\n",
        "| **Dimensionality Reduction** | Compress data | PCA, autoencoders |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_6"
      },
      "outputs": [],
      "source": [
        "# Regression vs Classification visualization\n",
        "fig2, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "np.random.seed(42)\n",
        "# Regression example\n",
        "ax1 = axes[0]\n",
        "x_reg = np.linspace(0, 10, 50)\n",
        "y_reg = 2 * x_reg + 3 + np.random.randn(50) * 2\n",
        "ax1.scatter(x_reg, y_reg, alpha=0.7, s=50)\n",
        "ax1.plot(x_reg, 2 * x_reg + 3, 'r-', linewidth=2, label='Learned rule')\n",
        "ax1.set_xlabel('Input x')\n",
        "ax1.set_ylabel('Output y (continuous)')\n",
        "ax1.set_title('Regression: Continuous Output')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "# Classification example\n",
        "ax2 = axes[1]\n",
        "x1 = np.random.randn(30) + 2\n",
        "y1 = np.random.randn(30) + 2\n",
        "x2 = np.random.randn(30) - 2\n",
        "y2 = np.random.randn(30) - 2\n",
        "ax2.scatter(x1, y1, c='blue', s=50, label='Class 0', alpha=0.7)\n",
        "ax2.scatter(x2, y2, c='red', s=50, label='Class 1', alpha=0.7)\n",
        "ax2.set_xlabel('Feature 1')\n",
        "ax2.set_ylabel('Feature 2')\n",
        "ax2.set_title('Classification: Discrete Output')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "fig2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_7"
      },
      "source": [
        "## Mathematical Optimization in ML (Section 1.4)\n",
        "Machine learning is fundamentally an **optimization problem**:\n",
        "$$\\min_{\\theta} \\text{Cost}(\\theta; \\text{data})$$\n",
        "Where:\n",
        "- $\\theta$: Model parameters (weights, coefficients)\n",
        "- Cost: Measures how well the model fits the data\n",
        "- data: Training examples\n",
        "### The Learning Problem\n",
        "1. **Choose a model**: Linear, polynomial, neural network, etc.\n",
        "2. **Define a cost function**: MSE, cross-entropy, etc.\n",
        "3. **Optimize**: Find $\\theta^*$ that minimizes the cost\n",
        "4. **Predict**: Use learned $\\theta^*$ on new data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_8"
      },
      "source": [
        "## Interactive: Explore Linear Regression\n",
        "Adjust the parameters to see how the line fits the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_9"
      },
      "outputs": [],
      "source": [
        "slope_slider = mo.ui.slider(-5, 5, value=2, step=0.1, label=\"Slope\")\n",
        "intercept_slider = mo.ui.slider(-10, 10, value=3, step=0.5, label=\"Intercept\")\n",
        "mo.vstack([slope_slider, intercept_slider])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_10"
      },
      "outputs": [],
      "source": [
        "# Interactive regression\n",
        "np.random.seed(42)\n",
        "x_data = np.linspace(0, 10, 30)\n",
        "y_true = 2 * x_data + 3\n",
        "y_data = y_true + np.random.randn(30) * 2\n",
        "slope = slope_slider.value\n",
        "intercept = intercept_slider.value\n",
        "y_pred = slope * x_data + intercept\n",
        "# Calculate MSE\n",
        "mse = np.mean((y_data - y_pred)**2)\n",
        "fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
        "ax3.scatter(x_data, y_data, alpha=0.7, s=50, label='Data')\n",
        "ax3.plot(x_data, y_pred, 'r-', linewidth=2, \n",
        "         label=f'Model: y = {slope:.1f}x + {intercept:.1f}')\n",
        "ax3.plot(x_data, y_true, 'g--', linewidth=1, alpha=0.5, label='True: y = 2x + 3')\n",
        "ax3.set_xlabel('x')\n",
        "ax3.set_ylabel('y')\n",
        "ax3.set_title(f'Cost (MSE) = {mse:.2f}')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "## Summary\n",
        "| Concept | Key Points |\n",
        "|---------|------------|\n",
        "| **Machine Learning** | Finding rules/patterns from data |\n",
        "| **Supervised Learning** | Learn from labeled input-output pairs |\n",
        "| **Unsupervised Learning** | Find structure in unlabeled data |\n",
        "| **Regression** | Predict continuous outputs |\n",
        "| **Classification** | Predict discrete classes |\n",
        "| **Optimization** | ML = minimizing a cost function |\n",
        "---\n",
        "## References\n",
        "- **Primary**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). *Machine Learning Refined* (2nd ed.), Chapter 1.\n",
        "- **Supplementary**: James, G. et al. (2023). *An Introduction to Statistical Learning*, Chapter 1.\n",
        "## Next Week\n",
        "**Zero-Order Optimization Techniques** (Chapter 2): Global and local optimization methods without derivatives.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}